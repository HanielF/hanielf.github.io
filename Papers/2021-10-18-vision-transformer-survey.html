<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="《Transformers in Vision: A Survey》部分笔记"><meta property="og:type" content="article"><meta property="og:title" content="论文笔记 | VisionTransformer综述笔记"><meta property="og:url" content="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="《Transformers in Vision: A Survey》部分笔记"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZocEwW.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nPPxxQ.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YA24kS.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hDCjN7.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lOoAXc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BwN5wv.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JLLHSc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnI87r.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hdEAyx.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Iyekbi.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eiEQ2E.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tLAIWk.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qIqIq9.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/P6VBTu.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nsW1Xi.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/exhvXj.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JfLpdV.png"><meta property="article:published_time" content="2021-10-18T02:56:51.000Z"><meta property="article:modified_time" content="2021-10-25T02:32:54.218Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="MachineLearning"><meta property="article:tag" content="Paper"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="VisionTransformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png"><link rel="canonical" href="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>论文笔记 | VisionTransformer综述笔记 | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION"><span class="nav-number">1.</span> <span class="nav-text">SELF-ATTENTION &amp; T RANSFORMERS IN VISION</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Single-head-Self-Attention"><span class="nav-number">1.1.</span> <span class="nav-text">Single-head Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention-in-CNNs"><span class="nav-number">1.1.1.</span> <span class="nav-text">Self-Attention in CNNs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Self-Attention-as-Stand-alone-Primitive"><span class="nav-number">1.1.2.</span> <span class="nav-text">Self-Attention as Stand-alone Primitive</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multi-head-Self-Attention-Transformers"><span class="nav-number">1.2.</span> <span class="nav-text">Multi-head Self-Attention (Transformers)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Uniform-scale-Vision-Transformers"><span class="nav-number">1.2.1.</span> <span class="nav-text">Uniform-scale Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#IPT-Pre-trained-image-processing-transformer"><span class="nav-number">1.2.1.1.</span> <span class="nav-text">IPT: Pre-trained image processing transformer</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#DeiT-Training-data-efﬁcient-image-transformers-amp-distillation-through-attention"><span class="nav-number">1.2.1.2.</span> <span class="nav-text">DeiT: Training data-efﬁcient image transformers &amp; distillation through attention</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#T2T-Tokens-to-Token-ViT-Training-Vision-Transformers-from-Scratch-on-ImageNet"><span class="nav-number">1.2.1.3.</span> <span class="nav-text">T2T: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multi-scale-Vision-Transformers"><span class="nav-number">1.2.2.</span> <span class="nav-text">Multi-scale Vision Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#DETR-End-to-End-Object-Detection-with-Transformers"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">DETR: End-to-End Object Detection with Transformers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CoAT-Co-Scale-Conv-Attentional-Image-Transformers"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">CoAT: Co-Scale Conv-Attentional Image Transformers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Swin-Transformer-Swin-transformer-Hierarchical-vision-transformer-using-shifted-windows"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">Swin-Transformer: Swin transformer: Hierarchical vision transformer using shifted windows</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Focal-Trans-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers"><span class="nav-number">1.2.2.4.</span> <span class="nav-text">Focal Trans: Focal Self-attention for Local-Global Interactions in Vision Transformers</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Hybrid-ViTs-with-Convolutions"><span class="nav-number">1.2.3.</span> <span class="nav-text">Hybrid ViTs with Convolutions</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#CvT-CvT-Introducing-Convolutions-to-Vision-Transformers"><span class="nav-number">1.2.3.1.</span> <span class="nav-text">CvT: CvT: Introducing Convolutions to Vision Transformers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#CoaT：Co-Scale-Conv-Attentional-Image-Transformers"><span class="nav-number">1.2.3.2.</span> <span class="nav-text">CoaT：Co-Scale Conv-Attentional Image Transformers</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Twins-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers"><span class="nav-number">1.2.3.3.</span> <span class="nav-text">Twins: Twins: Revisiting the Design of Spatial Attention in Vision Transformers</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Shuflle-Transformer"><span class="nav-number">1.2.4.</span> <span class="nav-text">Shuflle Transformer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CrossFormer-A-Versatile-Visiont-Ransformer-Hinging-on-Cross-Scale-Attention-Versatile"><span class="nav-number">1.2.5.</span> <span class="nav-text">CrossFormer: A Versatile Visiont Ransformer Hinging on Cross-Scale Attention Versatile</span></a></li></ol></li></ol></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">142</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">206</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文笔记 | VisionTransformer综述笔记</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-10-18 10:56:51" itemprop="dateCreated datePublished" datetime="2021-10-18T10:56:51+08:00">2021-10-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-10-25 10:32:54" itemprop="dateModified" datetime="2021-10-25T10:32:54+08:00">2021-10-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/Papers/2021-10-18-vision-transformer-survey#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="Papers/2021-10-18-vision-transformer-survey.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>4.4k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>11 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>《Transformers in Vision: A Survey》部分笔记</p></div><a id="more"></a><h2 id="SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION"><a href="#SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION" class="headerlink" title="SELF-ATTENTION & T RANSFORMERS IN VISION"></a>SELF-ATTENTION &amp; T RANSFORMERS IN VISION</h2><h3 id="Single-head-Self-Attention"><a href="#Single-head-Self-Attention" class="headerlink" title="Single-head Self-Attention"></a>Single-head Self-Attention</h3><h4 id="Self-Attention-in-CNNs"><a href="#Self-Attention-in-CNNs" class="headerlink" title="Self-Attention in CNNs"></a>Self-Attention in CNNs</h4><h4 id="Self-Attention-as-Stand-alone-Primitive"><a href="#Self-Attention-as-Stand-alone-Primitive" class="headerlink" title="Self-Attention as Stand-alone Primitive"></a>Self-Attention as Stand-alone Primitive</h4><h3 id="Multi-head-Self-Attention-Transformers"><a href="#Multi-head-Self-Attention-Transformers" class="headerlink" title="Multi-head Self-Attention (Transformers)"></a>Multi-head Self-Attention (Transformers)</h3><h4 id="Uniform-scale-Vision-Transformers"><a href="#Uniform-scale-Vision-Transformers" class="headerlink" title="Uniform-scale Vision Transformers"></a>Uniform-scale Vision Transformers</h4><p>vit数属于这一类，就是输入的时候用MHA，后面的stage就维持空间尺度不变。不同的stage串联起来就像是一个柱子一样…</p><h5 id="IPT-Pre-trained-image-processing-transformer"><a href="#IPT-Pre-trained-image-processing-transformer" class="headerlink" title="IPT: Pre-trained image processing transformer"></a>IPT: Pre-trained image processing transformer</h5><ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png" width="600"></li><li>用各种head提取特征，然后用transformer的encoder-decoder结构进行编码和对图片的重建，最后接多个head进行图片的生成重建。</li><li>切分成patch，位置编码用learnable，结构和transformer的encoder-decoder一样</li><li>应用到的是全尺寸的image上做预训练，然后用到下游的denoise, derain,等等，都是生成任务</li></ol><h5 id="DeiT-Training-data-efﬁcient-image-transformers-amp-distillation-through-attention"><a href="#DeiT-Training-data-efﬁcient-image-transformers-amp-distillation-through-attention" class="headerlink" title="DeiT: Training data-efﬁcient image transformers & distillation through attention"></a>DeiT: Training data-efﬁcient image transformers &amp; distillation through attention</h5><ol><li>第一个证明了Transformer可以用在中等大小数据集上的，一百二十万的imagenet，对比的是ViT的JFT数据集，3亿</li><li>用蒸馏的方式来学习Transformer，有一个teacher模型，还有一个student模型。目标是让student模型从teacher模型中学习到相同的知识。</li><li>这里的teacher模型用的是CNN结构的RegNetY-16GF，imagenet top1 acc=82.9。student模型就是纯的transformer。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZocEwW.png" width="500"></li><li>有两种蒸馏方式<ol><li>soft distillation: 最小化teacher模型和student模型的softmax结果的KL散度。<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nPPxxQ.png" width="500"></li><li>hard distilation: 就是默认teacher模型的结果就是ground truth，让student模型去拟合那个label，就是用交叉熵。<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YA24kS.png" width="500"></li></ol></li></ol><h5 id="T2T-Tokens-to-Token-ViT-Training-Vision-Transformers-from-Scratch-on-ImageNet"><a href="#T2T-Tokens-to-Token-ViT-Training-Vision-Transformers-from-Scratch-on-ImageNet" class="headerlink" title="T2T: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet"></a>T2T: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet</h5><ol><li>hypothesize that such performance gap roots in two main limitations of ViT: 1) the straightforward tokenization of input images by hard split makes ViT unable to model the image local structure like edges and lines, and thus it requires signiﬁcantly more training samples (like JFT-300M for pretraining) than CNNs for achieving similar performance; 2) the attention backbone of ViT is not welldesigned as CNNs for vision tasks, which contains redundancy and leads to limited feature richness and difﬁculties in model training.</li><li>We are then motivated to design a new full-transformer vision model to overcome above limitations.<ol><li>propose a progressive tokenization module to aggregate neighboring Tokens to one Token (named Tokens-to-Token module),which can model the local structure information of surrounding tokens and reduce the length of tokens iteratively.</li><li>in each Token-to-Token (T2T) step, the tokens output by a transformer layer are reconstructed as an image (restructurization) which is then split into tokens with overlapping (soft split) and ﬁnally the surrounding tokens are aggregated together by ﬂattening the split patches.</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hDCjN7.png" width="500"></li></ol></li></ol><h4 id="Multi-scale-Vision-Transformers"><a href="#Multi-scale-Vision-Transformers" class="headerlink" title="Multi-scale Vision Transformers"></a>Multi-scale Vision Transformers</h4><h5 id="DETR-End-to-End-Object-Detection-with-Transformers"><a href="#DETR-End-to-End-Object-Detection-with-Transformers" class="headerlink" title="DETR: End-to-End Object Detection with Transformers"></a>DETR: End-to-End Object Detection with Transformers</h5><ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lOoAXc.png" width="500"></li></ol><h5 id="CoAT-Co-Scale-Conv-Attentional-Image-Transformers"><a href="#CoAT-Co-Scale-Conv-Attentional-Image-Transformers" class="headerlink" title="CoAT: Co-Scale Conv-Attentional Image Transformers"></a>CoAT: Co-Scale Conv-Attentional Image Transformers</h5><ol><li>用多尺度的transformer，并提出了conv-attention，就是用 depthwise convolution 做relative position embedding，这个被用在一种分解的attention module里面。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BwN5wv.png" width="500"></li><li>后面看着好复杂。。直接贴个博客链接吧<a href="https://cloud.tencent.com/developer/article/1816902" target="_blank" rel="external nofollow noopener noreferrer">CoAT</a></li></ol><h5 id="Swin-Transformer-Swin-transformer-Hierarchical-vision-transformer-using-shifted-windows"><a href="#Swin-Transformer-Swin-transformer-Hierarchical-vision-transformer-using-shifted-windows" class="headerlink" title="Swin-Transformer: Swin transformer: Hierarchical vision transformer using shifted windows"></a>Swin-Transformer: Swin transformer: Hierarchical vision transformer using shifted windows</h5><ol><li>太出名了，ICCV马尔奖，就不多记了</li><li>用shift window实现了全局和局部的注意力结合,</li></ol><h5 id="Focal-Trans-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers"><a href="#Focal-Trans-Focal-Self-attention-for-Local-Global-Interactions-in-Vision-Transformers" class="headerlink" title="Focal Trans: Focal Self-attention for Local-Global Interactions in Vision Transformers"></a>Focal Trans: Focal Self-attention for Local-Global Interactions in Vision Transformers</h5><ol><li>就是先patch，然后让每个window作为中心，在它周围的window叫focal region<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.357ex" xmlns="http://www.w3.org/2000/svg" width="1.896ex" height="1.357ex" role="img" focusable="false" viewbox="0 -442 837.9 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container>，然后focal region中的每个位置是由<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.357ex" xmlns="http://www.w3.org/2000/svg" width="2.32ex" height="1.357ex" role="img" focusable="false" viewbox="0 -442 1025.3 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g></g></g></g></svg></mjx-container>大小的subwindow进行pooling得到的。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.65ex" xmlns="http://www.w3.org/2000/svg" width="6.127ex" height="2.181ex" role="img" focusable="false" viewbox="0 -677 2708.2 964.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"/></g></g><g data-mml-node="mo" transform="translate(1152.5, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(2208.2, 0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"/></g></g></g></svg></mjx-container>是window partition的大小，中间的焦点也就是一个window大小。所以最后得到的大小是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.357ex" xmlns="http://www.w3.org/2000/svg" width="5.086ex" height="1.357ex" role="img" focusable="false" viewbox="0 -442 2247.8 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mi" transform="translate(837.9, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="msub" transform="translate(1409.9, 0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"/></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container>， 对应原本的大小是 $s_r<em>s_w<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.452ex" xmlns="http://www.w3.org/2000/svg" width="18.326ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 8100 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">，</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">然</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">后</text><text data-variant="normal" transform="translate(2700, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">再</text><text data-variant="normal" transform="translate(3600, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">去</text><text data-variant="normal" transform="translate(4500, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">掉</text><text data-variant="normal" transform="translate(5400, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">中</text><text data-variant="normal" transform="translate(6300, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">间</text><text data-variant="normal" transform="translate(7200, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">的</text></g></g></g></svg></mjx-container>s_w</em>s_w<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.452ex" xmlns="http://www.w3.org/2000/svg" width="18.326ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 8100 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">大</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">小</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">，</text><text data-variant="normal" transform="translate(2700, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">就</text><text data-variant="normal" transform="translate(3600, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">两</text><text data-variant="normal" transform="translate(4500, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">边</text><text data-variant="normal" transform="translate(5400, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">分</text><text data-variant="normal" transform="translate(6300, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">别</text><text data-variant="normal" transform="translate(7200, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">为</text></g></g></g></svg></mjx-container>(s_r*s_w - s_w)/2$。</li><li>它是通过中间window计算query，然后对周围sub window pooling得到的区域计算key和value。进行window-wise的multihead attention</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JLLHSc.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnI87r.png" width="500"></li></ol><h4 id="Hybrid-ViTs-with-Convolutions"><a href="#Hybrid-ViTs-with-Convolutions" class="headerlink" title="Hybrid ViTs with Convolutions"></a>Hybrid ViTs with Convolutions</h4><h5 id="CvT-CvT-Introducing-Convolutions-to-Vision-Transformers"><a href="#CvT-CvT-Introducing-Convolutions-to-Vision-Transformers" class="headerlink" title="CvT: CvT: Introducing Convolutions to Vision Transformers"></a>CvT: CvT: Introducing Convolutions to Vision Transformers</h5><ol><li>就是对token使用2d卷积来编码，就是论文里的convolutional token embedding，然后在做attention操作的时候，把计算qkv的linear层替换成空间可分离深度卷积（depth-wise separable convolution）,就是 Convolutional projection.</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hdEAyx.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Iyekbi.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eiEQ2E.png" width="500"></li></ol><h5 id="CoaT：Co-Scale-Conv-Attentional-Image-Transformers"><a href="#CoaT：Co-Scale-Conv-Attentional-Image-Transformers" class="headerlink" title="CoaT：Co-Scale Conv-Attentional Image Transformers"></a>CoaT：Co-Scale Conv-Attentional Image Transformers</h5><ol><li>就是用conv做一个relative position embedding，然后提出了两种Block的形式，一种是Serial Block，就是平常用的，另一种是Parallel Block，多个Block之间是并行的，然后在多个block之间进行cross layer attention。</li><li>Parallel Block有两种实现方式，一种是Direct cross-layer attention，另一种是Attention with feature interpolation。</li><li>Direct cross-layer attention中，不同尺度的特征都是从输入得到的，然后对相同的layer，用conv和MHA。对不同的层，因为尺度不一样，所以对key和value进行上采样或者下采样的方式进行尺度的统一。用当前层的query和其他层的key，value。最后对当前层的conv attention和不同层cross-layer attention进行求和。</li><li>Attention with feature interpolation中，没有直接跨层注意力，而是对输入用独立的conv attention得到不同尺度的特征，然后用上采样和下采样让不同尺度之间进行统一。通尺度的特征在parallel group里面直接求和，然后用FFN。然后再用conv-attention module对现在这个feature interpolation做跨尺度的融合。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tLAIWk.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qIqIq9.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/P6VBTu.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nsW1Xi.png" width="500"></li></ol><h5 id="Twins-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers"><a href="#Twins-Twins-Revisiting-the-Design-of-Spatial-Attention-in-Vision-Transformers" class="headerlink" title="Twins: Twins: Revisiting the Design of Spatial Attention in Vision Transformers"></a>Twins: Twins: Revisiting the Design of Spatial Attention in Vision Transformers</h5><ol><li>做了两个工作，一个是提出了Twins-PCPVT，其实就是结合了PVT和CAPT，把PVT中的绝对位置编码换成了CAPT中的动态位置编码。位置编码的位置在每个stage的第一个encoder block后面。所以结构和PVT基本一样。</li><li>另一个工作才是重点。提出了Twins-SVT<ol><li>借鉴了空间可分离深度卷积的思想，就是在transformer的block里面，先用类似deep-wise的分组，做一个local grouped attention(LSA)，然后再类似point-wise的cnn，做一个global sub-sampled attention(GSA)。总结就是先在部分空间做MHA，然后再做一个全局的MHA。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/exhvXj.png" width="500"></li><li>GSA是在sub-window里面用卷积的方式弄一个代表key，然后用这些key来做全局的MHA</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JfLpdV.png" width="500"></li></ol></li></ol><h4 id="Shuflle-Transformer"><a href="#Shuflle-Transformer" class="headerlink" title="Shuflle Transformer"></a>Shuflle Transformer</h4><h4 id="CrossFormer-A-Versatile-Visiont-Ransformer-Hinging-on-Cross-Scale-Attention-Versatile"><a href="#CrossFormer-A-Versatile-Visiont-Ransformer-Hinging-on-Cross-Scale-Attention-Versatile" class="headerlink" title="CrossFormer: A Versatile Visiont Ransformer Hinging on Cross-Scale Attention Versatile"></a>CrossFormer: A Versatile Visiont Ransformer Hinging on Cross-Scale Attention Versatile</h4><ol><li>做了两个事，在embed那边做了多尺度的embedding，然后降低了计算量</li><li>提出了CEL：Cross-scale Embedding Layer，以及LSDA：Long Short Distance Attention</li><li>CEL：在上一个stage的输出基础上，用不同大小的kernel得到不同尺度的embedding，然后拼接或者projection得到patch embedding</li><li>LSDA：</li></ol></div><footer class="post-footer"><div class="post-tags"><a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a> <a href="/tags/VisionTransformer/" rel="tag"><i class="fa fa-tag"></i> VisionTransformer</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Papers/2021-10-16-paper-wirtting" rel="prev" title="论文写作"><i class="fa fa-chevron-left"></i> 论文写作</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">570k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">23:44</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey";
    this.page.identifier = "Papers/2021-10-18-vision-transformer-survey.html";
    this.page.title = "论文笔记 | VisionTransformer综述笔记";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : '7c360eb243560298cc4d35580774f862',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>