<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="《Transformers in Vision: A Survey》部分笔记"><meta property="og:type" content="article"><meta property="og:title" content="论文笔记 | VisionTransformer综述笔记"><meta property="og:url" content="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="《Transformers in Vision: A Survey》部分笔记"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://hanielxx.com/Papers/![QnA2Ak](https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png)"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZocEwW.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nPPxxQ.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YA24kS.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hDCjN7.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lOoAXc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/exhvXj.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JfLpdV.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BwN5wv.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hdEAyx.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Iyekbi.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eiEQ2E.png"><meta property="article:published_time" content="2021-10-18T02:56:51.000Z"><meta property="article:modified_time" content="2021-10-24T11:06:54.250Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="MachineLearning"><meta property="article:tag" content="Paper"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="VisionTransformer"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://hanielxx.com/Papers/![QnA2Ak](https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png)"><link rel="canonical" href="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>论文笔记 | VisionTransformer综述笔记 | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION"><span class="nav-number">1.</span> <span class="nav-text">SELF-ATTENTION &amp; T RANSFORMERS IN VISION</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Single-head-Self-Attention"><span class="nav-number">1.1.</span> <span class="nav-text">3.1 Single-head Self-Attention</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-1-Self-Attention-in-CNNs"><span class="nav-number">1.1.1.</span> <span class="nav-text">3.1.1 Self-Attention in CNNs</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-1-2-Self-Attention-as-Stand-alone-Primitive"><span class="nav-number">1.1.2.</span> <span class="nav-text">3.1.2 Self-Attention as Stand-alone Primitive</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Multi-head-Self-Attention-Transformers"><span class="nav-number">1.2.</span> <span class="nav-text">3.2 Multi-head Self-Attention (Transformers)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-Uniform-scale-Vision-Transformers"><span class="nav-number">1.2.1.</span> <span class="nav-text">3.2.1 Uniform-scale Vision Transformers</span></a></li></ol></li></ol></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">143</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">207</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文笔记 | VisionTransformer综述笔记</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-10-18 10:56:51" itemprop="dateCreated datePublished" datetime="2021-10-18T10:56:51+08:00">2021-10-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-10-24 19:06:54" itemprop="dateModified" datetime="2021-10-24T19:06:54+08:00">2021-10-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/Papers/2021-10-18-vision-transformer-survey#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="Papers/2021-10-18-vision-transformer-survey.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>3k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>7 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>《Transformers in Vision: A Survey》部分笔记</p></div><a id="more"></a><h2 id="SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION"><a href="#SELF-ATTENTION-amp-T-RANSFORMERS-IN-VISION" class="headerlink" title="SELF-ATTENTION & T RANSFORMERS IN VISION"></a>SELF-ATTENTION &amp; T RANSFORMERS IN VISION</h2><h3 id="3-1-Single-head-Self-Attention"><a href="#3-1-Single-head-Self-Attention" class="headerlink" title="3.1 Single-head Self-Attention"></a>3.1 Single-head Self-Attention</h3><h4 id="3-1-1-Self-Attention-in-CNNs"><a href="#3-1-1-Self-Attention-in-CNNs" class="headerlink" title="3.1.1 Self-Attention in CNNs"></a>3.1.1 Self-Attention in CNNs</h4><h4 id="3-1-2-Self-Attention-as-Stand-alone-Primitive"><a href="#3-1-2-Self-Attention-as-Stand-alone-Primitive" class="headerlink" title="3.1.2 Self-Attention as Stand-alone Primitive"></a>3.1.2 Self-Attention as Stand-alone Primitive</h4><h3 id="3-2-Multi-head-Self-Attention-Transformers"><a href="#3-2-Multi-head-Self-Attention-Transformers" class="headerlink" title="3.2 Multi-head Self-Attention (Transformers)"></a>3.2 Multi-head Self-Attention (Transformers)</h3><h4 id="3-2-1-Uniform-scale-Vision-Transformers"><a href="#3-2-1-Uniform-scale-Vision-Transformers" class="headerlink" title="3.2.1 Uniform-scale Vision Transformers"></a>3.2.1 Uniform-scale Vision Transformers</h4><p>vit数属于这一类，就是输入的时候用MHA，后面的stage就维持空间尺度不变。不同的stage串联起来就像是一个柱子一样…</p><ol><li>IPT: Pre-trained image processing transformer<ol><li><img src="![QnA2Ak](https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnA2Ak.png)" width="600"></li><li>用各种head提取特征，然后用transformer的encoder-decoder结构进行编码和对图片的重建，最后接多个head进行图片的生成重建。</li><li>切分成patch，位置编码用learnable，结构和transformer的encoder-decoder一样</li><li>应用到的是全尺寸的image上做预训练，然后用到下游的denoise, derain,等等，都是生成任务</li></ol></li><li>DeiT: Training data-efﬁcient image transformers &amp; distillation through attention<ol><li>第一个证明了Transformer可以用在中等大小数据集上的，一百二十万的imagenet，对比的是ViT的JFT数据集，3亿</li><li>用蒸馏的方式来学习Transformer，有一个teacher模型，还有一个student模型。目标是让student模型从teacher模型中学习到相同的知识。</li><li>这里的teacher模型用的是CNN结构的RegNetY-16GF，imagenet top1 acc=82.9。student模型就是纯的transformer。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZocEwW.png" width="500"></li><li>有两种蒸馏方式<ol><li>soft distillation: 最小化teacher模型和student模型的softmax结果的KL散度。<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nPPxxQ.png" width="500"></li><li>hard distilation: 就是默认teacher模型的结果就是ground truth，让student模型去拟合那个label，就是用交叉熵。<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YA24kS.png" width="500"></li></ol></li></ol></li><li>T2T: Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet<ol><li>hypothesize that such performance gap roots in two main limitations of ViT: 1) the straightforward tokenization of input images by hard split makes ViT unable to model the image local structure like edges and lines, and thus it requires signiﬁcantly more training samples (like JFT-300M for pretraining) than CNNs for achieving similar performance; 2) the attention backbone of ViT is not welldesigned as CNNs for vision tasks, which contains redundancy and leads to limited feature richness and difﬁculties in model training.</li><li>We are then motivated to design a new full-transformer vision model to overcome above limitations.<ol><li>propose a progressive tokenization module to aggregate neighboring Tokens to one Token (named Tokens-to-Token module),which can model the local structure information of surrounding tokens and reduce the length of tokens iteratively.</li><li>in each Token-to-Token (T2T) step, the tokens output by a transformer layer are reconstructed as an image (restructurization) which is then split into tokens with overlapping (soft split) and ﬁnally the surrounding tokens are aggregated together by ﬂattening the split patches.</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hDCjN7.png" width="500"></li></ol></li></ol></li><li>DETR: End-to-End Object Detection with Transformers<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lOoAXc.png" width="500"></li></ol></li><li>Twins: Twins: Revisiting the Design of Spatial Attention in Vision Transformers<ol><li>做了两个工作，一个是提出了Twins-PCPVT，其实就是结合了PVT和CAPT，把PVT中的绝对位置编码换成了CAPT中的动态位置编码。位置编码的位置在每个stage的第一个encoder block后面。所以结构和PVT基本一样。</li><li>另一个工作才是重点。提出了Twins-SVT<ol><li>借鉴了空间可分离深度卷积的思想，就是在transformer的block里面，先用类似deep-wise的分组，做一个local grouped attention(LSA)，然后再类似point-wise的cnn，做一个global sub-sampled attention(GSA)。总结就是先在部分空间做MHA，然后再做一个全局的MHA。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/exhvXj.png" width="500"></li><li>GSA是在sub-window里面用卷积的方式弄一个代表key，然后用这些key来做全局的MHA</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JfLpdV.png" width="500"></li></ol></li></ol></li><li>CoAT: Co-Scale Conv-Attentional Image Transformers<ol><li>用多尺度的transformer，并提出了conv-attention，就是用 depthwise convolution 做relative position embedding，这个被用在一种分解的attention module里面。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BwN5wv.png" width="500"></li><li>后面看着好复杂。。直接贴个博客链接吧<a href="https://cloud.tencent.com/developer/article/1816902" target="_blank" rel="external nofollow noopener noreferrer">CoAT</a></li></ol></li><li>Swin-Transformer: Swin transformer: Hierarchical vision transformer using shifted windows<ol><li>太出名了，ICCV马尔奖，就不多记了</li><li>用shift window实现了全局和局部的注意力结合,</li></ol></li><li>CvT: CvT: Introducing Convolutions to Vision Transformers<ol><li>就是对token使用2d卷积来编码，就是论文里的convolutional token embedding，然后在做attention操作的时候，把计算qkv的linear层替换成空间可分离深度卷积（depth-wise separable convolution）,就是 Convolutional projection.</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hdEAyx.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Iyekbi.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eiEQ2E.png" width="500"></li></ol></li></ol></div><footer class="post-footer"><div class="post-tags"><a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a> <a href="/tags/Paper/" rel="tag"><i class="fa fa-tag"></i> Paper</a> <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a> <a href="/tags/VisionTransformer/" rel="tag"><i class="fa fa-tag"></i> VisionTransformer</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Papers/2021-10-16-paper-wirtting" rel="prev" title="论文写作"><i class="fa fa-chevron-left"></i> 论文写作</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">568k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">23:41</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/Papers/2021-10-18-vision-transformer-survey";
    this.page.identifier = "Papers/2021-10-18-vision-transformer-survey.html";
    this.page.title = "论文笔记 | VisionTransformer综述笔记";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : '7c360eb243560298cc4d35580774f862',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>