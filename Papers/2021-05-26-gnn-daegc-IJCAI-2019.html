<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="之前研究了DAEGC模型的源码和论文，补个笔记。论文《Attributed Graph Clustering: A Deep Attentional Embedding Approach》，模型结果在 Node Clustering on Cora 上，Acc、NMI、ARI排第5， F1排第4。"><meta property="og:type" content="article"><meta property="og:title" content="论文笔记 | Attributed Graph Clustering A Deep Attentional Embedding Approach"><meta property="og:url" content="https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="之前研究了DAEGC模型的源码和论文，补个笔记。论文《Attributed Graph Clustering: A Deep Attentional Embedding Approach》，模型结果在 Node Clustering on Cora 上，Acc、NMI、ARI排第5， F1排第4。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dePxtv.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AKc7H7.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ih9MBs.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F6QEol.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EqGkNT.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LCleco.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mWV0oc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VVpH3P.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TgzYqR.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5LTggJ.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tESTdL.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tlKLqz.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8BF7AI.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n02Knc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PmeTG5.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JbFPVC.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RdR0WV.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kMzoKK.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xlw5Xl.png"><meta property="article:published_time" content="2021-05-26T11:34:18.000Z"><meta property="article:modified_time" content="2021-05-28T18:46:50.330Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="GNN"><meta property="article:tag" content="DAEGC"><meta property="article:tag" content="IJCAI-2019"><meta property="article:tag" content="NodeClustering"><meta property="article:tag" content="GraphClustering"><meta property="article:tag" content="GAE"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dePxtv.png"><link rel="canonical" href="https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>论文笔记 | Attributed Graph Clustering A Deep Attentional Embedding Approach | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract-amp-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Abstract &amp; Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Proposed-Method"><span class="nav-number">2.</span> <span class="nav-text">Proposed Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#GAE"><span class="nav-number">2.1.</span> <span class="nav-text">GAE</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#代码"><span class="nav-number">2.1.1.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#GAE主函数"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">GAE主函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#GAT"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">GAT</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Evaluate"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">Evaluate</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Utils和Dataset"><span class="nav-number">2.1.1.4.</span> <span class="nav-text">Utils和Dataset</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Self-optimizing-Embedding"><span class="nav-number">2.2.</span> <span class="nav-text">Self-optimizing Embedding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#代码-1"><span class="nav-number">2.2.1.</span> <span class="nav-text">代码</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#自监督模块"><span class="nav-number">2.2.1.1.</span> <span class="nav-text">自监督模块</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#调用主函数"><span class="nav-number">2.2.1.2.</span> <span class="nav-text">调用主函数</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">2.3.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验结果"><span class="nav-number">3.</span> <span class="nav-text">实验结果</span></a></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">136</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">198</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">论文笔记 | Attributed Graph Clustering A Deep Attentional Embedding Approach</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-05-26 19:34:18" itemprop="dateCreated datePublished" datetime="2021-05-26T19:34:18+08:00">2021-05-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-05-29 02:46:50" itemprop="dateModified" datetime="2021-05-29T02:46:50+08:00">2021-05-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Papers/" itemprop="url" rel="index"><span itemprop="name">Papers</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/Papers/2021-05-26-gnn-daegc-IJCAI-2019#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="Papers/2021-05-26-gnn-daegc-IJCAI-2019.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>15k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>37 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>之前研究了DAEGC模型的源码和论文，补个笔记。</p><p>论文《Attributed Graph Clustering: A Deep Attentional Embedding Approach》，模型结果在 Node Clustering on Cora 上，Acc、NMI、ARI排第5， F1排第4。</p></div><a id="more"></a><h2 id="Abstract-amp-Introduction"><a href="#Abstract-amp-Introduction" class="headerlink" title="Abstract & Introduction"></a>Abstract &amp; Introduction</h2><ol><li>Graph clustering<ol><li>在网络中挖掘communities和groups</li><li>目标是将节点划分成不想交的group</li><li>Attributed graph cluster关键问题是如何捕获结构关系和节点信息</li><li>输入是一个图，输出是 ${G_1, G_2,…,G_k$，同一个cluster的节点可能离得比较近，或者有相似的attribute values</li></ol></li><li>近期大多数工作都是学一个graph embedding，然后用传统的聚类方法，比如k-means或者谱聚类，谱聚类在之前文章<a href="https://hanielxx.com/MachineLearning/2021-05-09-gnn-graph-clustering">《GNN和图聚类》</a>中有。</li><li>之前的工作主要是two-step框架下的，文章认为这种方式不是goal-directed，比如面向一些特殊的clustering任务，所以提出了一个goal-directed的方法，</li><li>使用了GAT来捕获邻居特征的重要性，同时encode网络的拓扑结构和节点信息，后面用简单的inner product decoder来重建图信息。用这个GAE生成预测的邻接矩阵A_pred和graph中每个node embedding，作为后面的初始的soft label，用生成的soft label来supervise后面的self-training。</li><li>主要是分两个节点，一个是GAE阶段，得到一个初始的soft label，就是一个k-menas的结果，然后用self-training的一个算法进行迭代更新聚类中心</li><li>这篇文章主要针对大图的复杂度问题和计算量，在对邻居aggregate的时候是sample</li></ol><h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><h3 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h3><ol><li>用的是GAE的变种作为graph autoencoder，目标是学习每个节点的embedding，每个节点的权重是通过将相邻节点的embedding拼接在一起，然后做一个全连接+softmax就得到了。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dePxtv.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AKc7H7.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ih9MBs.png" width="500"></li></ol></li><li>GAT其实在拓扑信息上只考虑了1-hop邻居节点，他们为了获得更强的关系信息，用了t-orer邻居节点信息。上面的权重加入结构信息之后就成了下面这样<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F6QEol.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EqGkNT.png" width="500"></li></ol></li><li>Decoder只是简单的Inner Product<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LCleco.png" width="500"></li></ol></li><li>这部分的损失其实就是binary_cross_entropy</li></ol><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>整体的代码如下，分成了几个部分方便看</p><h5 id="GAE主函数"><a href="#GAE主函数" class="headerlink" title="GAE主函数"></a>GAE主函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GAT</span><br><span class="line"><span class="keyword">from</span> evaluation <span class="keyword">import</span> eva</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrain</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># 这部分只是Graph Attentional Encoder的过程，只算了重构损失</span></span><br><span class="line">    model = GAT(</span><br><span class="line">        num_features=args.input_dim,</span><br><span class="line">        hidden_size=args.hidden_size,</span><br><span class="line">        embedding_size=args.embedding_size,</span><br><span class="line">        alpha=args.alpha,</span><br><span class="line">    ).to(device)</span><br><span class="line">    print(model)</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data process</span></span><br><span class="line">    dataset = utils.data_preprocessing(dataset)</span><br><span class="line">    adj = dataset.adj.to(device)</span><br><span class="line">    adj_label = dataset.adj_label.to(device)</span><br><span class="line">    M = utils.get_M(adj).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data and label</span></span><br><span class="line">    x = torch.Tensor(dataset.x).to(device)</span><br><span class="line">    print(dataset)</span><br><span class="line">    print(<span class="string">"GAT:"</span>,model)</span><br><span class="line">    print(<span class="string">"M.shape:"</span>, M.shape)</span><br><span class="line">    y = dataset.y.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(args.max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># model return reconstructed structure matrix A(N, N) and encoded z(N, output_feat)</span></span><br><span class="line">        A_pred, z = model(x, adj, M)</span><br><span class="line">        loss = F.binary_cross_entropy(A_pred.view(<span class="number">-1</span>), adj_label.view(<span class="number">-1</span>))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            _, z = model(x, adj, M)</span><br><span class="line">            <span class="comment"># n_init: Number of time the k-means algorithm will be run with different centroid seeds. </span></span><br><span class="line">            <span class="comment"># The final results will be the best output of n_init consecutive runs in terms of inertia.</span></span><br><span class="line">            kmeans = KMeans(n_clusters=args.n_clusters, n_init=<span class="number">20</span>).fit(</span><br><span class="line">                z.data.cpu().numpy()</span><br><span class="line">            )</span><br><span class="line">            acc, nmi, ari, f1 = eva(y, kmeans.labels_, epoch)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(</span><br><span class="line">                model.state_dict(), <span class="string">f"./pretrain/predaegc_<span class="subst">{args.name}</span>_<span class="subst">{epoch}</span>.pkl"</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=<span class="string">"train"</span>, formatter_class=argparse.ArgumentDefaultsHelpFormatter</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">"--name"</span>, type=str, default=<span class="string">"Citeseer"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--max_epoch"</span>, type=int, default=<span class="number">100</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">0.001</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_clusters"</span>, default=<span class="number">6</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--hidden_size"</span>, default=<span class="number">256</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--embedding_size"</span>, default=<span class="number">16</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--weight_decay"</span>, type=int, default=<span class="number">5e-3</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">"--alpha"</span>, type=float, default=<span class="number">0.2</span>, help=<span class="string">"Alpha for the leaky_relu."</span></span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.cuda = torch.cuda.is_available()</span><br><span class="line">    print(<span class="string">"use cuda: {}"</span>.format(args.cuda))</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> args.cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">    datasets = utils.get_dataset(args.name)</span><br><span class="line">    dataset = datasets[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name == <span class="string">"Citeseer"</span>:</span><br><span class="line">        args.lr = <span class="number">0.005</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Cora"</span>:</span><br><span class="line">        args.lr = <span class="number">0.005</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Pubmed"</span>:</span><br><span class="line">        args.lr = <span class="number">0.001</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.input_dim = dataset.num_features</span><br><span class="line"></span><br><span class="line">    print(args)</span><br><span class="line">    pretrain(dataset)</span><br></pre></td></tr></table></figure><h5 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GATLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(GATLayer, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class="line">        <span class="comment"># 均匀分布初始化输入 Tensor，gain是缩放因子，https://pytorch.apachecn.org/docs/1.0/nn_init.html?h=nn.init.xavier_uniform_</span></span><br><span class="line">        nn.init.xavier_uniform_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.a_self = nn.Parameter(torch.zeros(size=(out_features, <span class="number">1</span>)))</span><br><span class="line">        nn.init.xavier_uniform_(self.a_self.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.a_neighs = nn.Parameter(torch.zeros(size=(out_features, <span class="number">1</span>)))</span><br><span class="line">        nn.init.xavier_uniform_(self.a_neighs.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, adj, M, concat=True)</span>:</span></span><br><span class="line">        <span class="comment"># x: [samples_cnt=N, input_feat]</span></span><br><span class="line">        <span class="comment"># w: [input_feat, output_feat]</span></span><br><span class="line">        <span class="comment"># h: [N, output_feat]</span></span><br><span class="line">        h = torch.mm(input, self.W) </span><br><span class="line"></span><br><span class="line">        attn_for_self = torch.mm(h, self.a_self)  <span class="comment"># (N,1)</span></span><br><span class="line">        attn_for_neighs = torch.mm(h, self.a_neighs)  <span class="comment"># (N,1)</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; a</span></span><br><span class="line">        <span class="comment"># tensor([[1],</span></span><br><span class="line">        <span class="comment">#         [2],</span></span><br><span class="line">        <span class="comment">#         [3]])</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; torch.transpose(a, 0, 1)</span></span><br><span class="line">        <span class="comment"># tensor([[1, 2, 3]])</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; a+torch.transpose(a, 0, 1)</span></span><br><span class="line">        <span class="comment"># tensor([[2, 3, 4],</span></span><br><span class="line">        <span class="comment">#         [3, 4, 5],</span></span><br><span class="line">        <span class="comment">#         [4, 5, 6]])</span></span><br><span class="line">        attn_dense = attn_for_self + torch.transpose(attn_for_neighs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [N, N]*[N, N]=&gt;[N, N]</span></span><br><span class="line">        attn_dense = torch.mul(attn_dense, M) </span><br><span class="line">        attn_dense = self.leakyrelu(attn_dense)  <span class="comment"># (N,N)</span></span><br><span class="line"></span><br><span class="line">        zero_vec = <span class="number">-9e15</span> * torch.ones_like(adj) <span class="comment"># [N, N]</span></span><br><span class="line">        <span class="comment"># torch.where: Return a tensor of elements selected from either x or y, depending on condition</span></span><br><span class="line">        <span class="comment"># torch.where(condition, x, y) → Tensor, xi if condition else yi</span></span><br><span class="line">        adj = torch.where(adj &gt; <span class="number">0</span>, attn_dense, zero_vec) <span class="comment"># [N, N]</span></span><br><span class="line">        <span class="comment"># 对每一行的样本所有邻居softmax</span></span><br><span class="line">        attention = F.softmax(adj, dim=<span class="number">1</span>) <span class="comment"># N, N</span></span><br><span class="line">        h_prime = torch.matmul(attention, h) <span class="comment"># N, output_feat</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> concat:</span><br><span class="line">            <span class="comment"># torch.nn.function.elu: Applies element-wise, ELU(x)=max(0,x)+min(0,α∗(exp(x)−1)) .</span></span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.__class__.__name__</span><br><span class="line">            + <span class="string">" ("</span></span><br><span class="line">            + str(self.in_features)</span><br><span class="line">            + <span class="string">" -&gt; "</span></span><br><span class="line">            + str(self.out_features)</span><br><span class="line">            + <span class="string">")"</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, hidden_size, embedding_size, alpha)</span>:</span></span><br><span class="line">        super(GAT, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.conv1 = GATLayer(num_features, hidden_size, alpha)</span><br><span class="line">        self.conv2 = GATLayer(hidden_size, embedding_size, alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, adj, M)</span>:</span></span><br><span class="line">        h = self.conv1(x, adj, M)</span><br><span class="line">        h = self.conv2(h, adj, M)</span><br><span class="line">        <span class="comment"># p是Lp normalize中的p，dim是the dimension to reduce. Default: 1</span></span><br><span class="line">        <span class="comment"># z: [N, output_feat]</span></span><br><span class="line">        z = F.normalize(h, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># decoder, A: [N, N]</span></span><br><span class="line">        A_pred = self.dot_product_decode(z)</span><br><span class="line">        <span class="keyword">return</span> A_pred, z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dot_product_decode</span><span class="params">(self, Z)</span>:</span></span><br><span class="line">        A_pred = torch.sigmoid(torch.matmul(Z, Z.t()))</span><br><span class="line">        <span class="keyword">return</span> A_pred</span><br></pre></td></tr></table></figure><h5 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> munkres <span class="keyword">import</span> Munkres</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> normalized_mutual_info_score <span class="keyword">as</span> nmi_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> adjusted_rand_score <span class="keyword">as</span> ari_score</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linear_sum_assignment <span class="keyword">as</span> linear</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># similar to https://github.com/karenlatong/AGC-master/blob/master/metrics.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cluster_acc</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># 对y_pred标签重新分配，然后计算acc等指标</span></span><br><span class="line">    <span class="comment"># 可以用匈牙利算法 (Kuhn-Munkres or Hungarian Algorithm) 实现</span></span><br><span class="line">    y_true = y_true - np.min(y_true)</span><br><span class="line"></span><br><span class="line">    l1 = list(set(y_true))</span><br><span class="line">    numclass1 = len(l1)</span><br><span class="line"></span><br><span class="line">    l2 = list(set(y_pred))</span><br><span class="line">    numclass2 = len(l2)</span><br><span class="line"></span><br><span class="line">    ind = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> numclass1 != numclass2:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> l1:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> l2:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_pred[ind] = i</span><br><span class="line">                ind += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    l2 = list(set(y_pred))</span><br><span class="line">    numclass2 = len(l2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> numclass1 != numclass2:</span><br><span class="line">        print(<span class="string">"error"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    cost = np.zeros((numclass1, numclass2), dtype=int)</span><br><span class="line">    <span class="keyword">for</span> i, c1 <span class="keyword">in</span> enumerate(l1):</span><br><span class="line">        mps = [i1 <span class="keyword">for</span> i1, e1 <span class="keyword">in</span> enumerate(y_true) <span class="keyword">if</span> e1 == c1]</span><br><span class="line">        <span class="keyword">for</span> j, c2 <span class="keyword">in</span> enumerate(l2):</span><br><span class="line">            mps_d = [i1 <span class="keyword">for</span> i1 <span class="keyword">in</span> mps <span class="keyword">if</span> y_pred[i1] == c2]</span><br><span class="line">            cost[i][j] = len(mps_d)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># match two clustering results by Munkres algorithm</span></span><br><span class="line">    m = Munkres()</span><br><span class="line">    cost = cost.__neg__().tolist()</span><br><span class="line">    indexes = m.compute(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the match results</span></span><br><span class="line">    new_predict = np.zeros(len(y_pred))</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(l1):</span><br><span class="line">        <span class="comment"># correponding label in l2:</span></span><br><span class="line">        c2 = l2[indexes[i][<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ai is the index with label==c2 in the pred_label list</span></span><br><span class="line">        ai = [ind <span class="keyword">for</span> ind, elm <span class="keyword">in</span> enumerate(y_pred) <span class="keyword">if</span> elm == c2]</span><br><span class="line">        new_predict[ai] = c</span><br><span class="line"></span><br><span class="line">    acc = metrics.accuracy_score(y_true, new_predict)</span><br><span class="line">    f1_macro = metrics.f1_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    precision_macro = metrics.precision_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    recall_macro = metrics.recall_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    f1_micro = metrics.f1_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    precision_micro = metrics.precision_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    recall_micro = metrics.recall_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    <span class="keyword">return</span> acc, f1_macro</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eva</span><span class="params">(y_true, y_pred, epoch=<span class="number">0</span>)</span>:</span></span><br><span class="line">    acc, f1 = cluster_acc(y_true, y_pred)</span><br><span class="line">    nmi = nmi_score(y_true, y_pred, average_method=<span class="string">"arithmetic"</span>)</span><br><span class="line">    ari = ari_score(y_true, y_pred)</span><br><span class="line">    print(<span class="string">f"epoch <span class="subst">{epoch}</span>:acc <span class="subst">{acc:<span class="number">.4</span>f}</span>, nmi <span class="subst">{nmi:<span class="number">.4</span>f}</span>, ari <span class="subst">{ari:<span class="number">.4</span>f}</span>, f1 <span class="subst">{f1:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    <span class="keyword">return</span> acc, nmi, ari, f1</span><br></pre></td></tr></table></figure><h5 id="Utils和Dataset"><a href="#Utils和Dataset" class="headerlink" title="Utils和Dataset"></a>Utils和Dataset</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># Planetoid参考 https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/planetoid.html#Planetoid</span></span><br><span class="line">    datasets = Planetoid(<span class="string">'./dataset'</span>, dataset)</span><br><span class="line">    <span class="keyword">return</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_preprocessing</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># 其实就是用边构建邻接矩阵，参考 https://pytorch.apachecn.org/docs/1.0/torch_tensors.html</span></span><br><span class="line">    dataset.adj = torch.sparse_coo_tensor(</span><br><span class="line">        dataset.edge_index, torch.ones(dataset.edge_index.shape[<span class="number">1</span>]), torch.Size([dataset.x.shape[<span class="number">0</span>], dataset.x.shape[<span class="number">0</span>]])</span><br><span class="line">    ).to_dense()</span><br><span class="line">    dataset.adj_label = dataset.adj</span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.eye: 返回二维张量，对角线上是1，其它地方是0.</span></span><br><span class="line">    <span class="comment"># 给邻接矩阵加上节点到自己的边</span></span><br><span class="line">    dataset.adj += torch.eye(dataset.x.shape[<span class="number">0</span>]) <span class="comment"># (x.shape[0], x.shape[0])</span></span><br><span class="line">    <span class="comment"># 每个元素除以每行的l1范数，即每行元素和，如果是l2就是除以每行样本的l2范数</span></span><br><span class="line">    <span class="comment"># 这里的adj就是论文中的 transition matrix B_{ij}=1/d_i if e_{ij} \in E</span></span><br><span class="line">    dataset.adj = normalize(dataset.adj, norm=<span class="string">"l1"</span>)</span><br><span class="line">    dataset.adj = torch.from_numpy(dataset.adj).to(dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_M</span><span class="params">(adj)</span>:</span></span><br><span class="line">    adj_numpy = adj.cpu().numpy()</span><br><span class="line">    <span class="comment"># t_order</span></span><br><span class="line">    t=<span class="number">2</span></span><br><span class="line">    tran_prob = normalize(adj_numpy, norm=<span class="string">"l1"</span>, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># M就是论文中的proximity matrix M</span></span><br><span class="line">    M_numpy = sum([np.linalg.matrix_power(tran_prob, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, t + <span class="number">1</span>)]) / t</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(M_numpy)</span><br></pre></td></tr></table></figure><h3 id="Self-optimizing-Embedding"><a href="#Self-optimizing-Embedding" class="headerlink" title="Self-optimizing Embedding"></a>Self-optimizing Embedding</h3><ol><li>这部分刚开始其实一直没怎么看懂，直到看完源码，然后看了之前那篇综述，对整个套路有了大概的了解之后才懂…</li><li>主要是学习两个分部，一个P分布一个Q分布</li><li>用上面GAE跑出来的node embedding作为初始化，然后跑一次k-means得到初始的簇头，然后在后面的训练过程中不断更新簇头</li><li>Q分布是通过node embedding和簇头embedding得到的。簇头初始化通过上面的GAE+k-means得到，node embedding在GAE的基础上更新。所以后面的训练的每个epoch，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewbox="0 -442 603 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"/></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewbox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g></g></g></svg></mjx-container>都会更新。</li><li>Q分布中的每个值可以衡量每个节点和簇头有多接近，每个epoch中Q分布被认为trustworthy，被当成了soft label，相当于是当前epoch的node embedding和簇头embedding算距离，够近就认为你就是这个簇的，一个假标签。这是模型预测出来的y_pred。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mWV0oc.png" width="500"></li></ol></li><li>每个节点的soft label可以通过下面的argmax得到<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VVpH3P.png" width="500"></li></ol></li><li>而目标分布P才是真的”ground-truth label”，它是通过近期的Q算出来的，所以也依赖Q。P分布按照阶段更新，被当做是这个阶段内的ground-truth，真标签。</li><li>它不能像Q那样每个epoch都更新，不然目标也太不稳定了，Q都不知道朝哪里梯度下降，没法收敛。P代表了在这一个阶段内（论文中写的是5个epoch），Q应该是朝哪里更新，起的作用就是监督学习里的真标签<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:0" xmlns="http://www.w3.org/2000/svg" width="1.726ex" height="1.545ex" role="img" focusable="false" viewbox="0 -683 763 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"/></g></g></g></svg></mjx-container>。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TgzYqR.png" width="500"></li></ol></li><li>最后，这部分的损失就是P和Q的KL散度。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5LTggJ.png" width="500"></li></ol></li><li>总的损失是两个部分的加权和<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tESTdL.png" width="500"></li></ol></li></ol><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p>这里的evaluate和上面的一样</p><h5 id="自监督模块"><a href="#自监督模块" class="headerlink" title="自监督模块"></a>自监督模块</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DAEGC</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, hidden_size, embedding_size, alpha, num_clusters, v=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(DAEGC, self).__init__()</span><br><span class="line">        self.num_clusters = num_clusters</span><br><span class="line">        self.v = v</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pretrain model</span></span><br><span class="line">        self.gat = GAT(num_features, hidden_size, embedding_size, alpha)</span><br><span class="line">        <span class="comment"># 初始化的时候加载pretrain的GAT模型</span></span><br><span class="line">        self.gat.load_state_dict(torch.load(args.pretrain_path, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cluster layer，簇头embedding</span></span><br><span class="line">        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))</span><br><span class="line">        torch.nn.init.xavier_normal_(self.cluster_layer.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, adj, M)</span>:</span></span><br><span class="line">        <span class="comment"># 得到reconstruct的邻接矩阵和[N, feat_size]的节点embedding Z</span></span><br><span class="line">        A_pred, z = self.gat(x, adj, M)</span><br><span class="line">        q = self.get_Q(z)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> A_pred, z, q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Q</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        q = <span class="number">1.0</span> / (<span class="number">1.0</span> + torch.sum(torch.pow(z.unsqueeze(<span class="number">1</span>) - self.cluster_layer, <span class="number">2</span>), <span class="number">2</span>) / self.v)</span><br><span class="line">        q = q.pow((self.v + <span class="number">1.0</span>) / <span class="number">2.0</span>)</span><br><span class="line">        q = (q.t() / torch.sum(q, <span class="number">1</span>)).t()</span><br><span class="line">        <span class="keyword">return</span> q</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target_distribution</span><span class="params">(q)</span>:</span></span><br><span class="line">    weight = q**<span class="number">2</span> / q.sum(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (weight.t() / weight.sum(<span class="number">1</span>)).t()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainer</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    model = DAEGC(num_features=args.input_dim, hidden_size=args.hidden_size,</span><br><span class="line">                  embedding_size=args.embedding_size, alpha=args.alpha, num_clusters=args.n_clusters).to(device)</span><br><span class="line">    print(model)</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data process</span></span><br><span class="line">    dataset = utils.data_preprocessing(dataset)</span><br><span class="line">    adj = dataset.adj.to(device)</span><br><span class="line">    adj_label = dataset.adj_label.to(device)</span><br><span class="line">    M = utils.get_M(adj).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data and label</span></span><br><span class="line">    data = torch.Tensor(dataset.x).to(device)</span><br><span class="line">    y = dataset.y.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 这里的GAT已经load了pretrain的模型</span></span><br><span class="line">        <span class="comment"># 相当于用那个epoch的模型做一次eval</span></span><br><span class="line">        _, z = model.gat(data, adj, M)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get kmeans and pretrain cluster result</span></span><br><span class="line">    <span class="comment"># 这里是用pretrain的结果来初始化kmeans的中心</span></span><br><span class="line">    kmeans = KMeans(n_clusters=args.n_clusters, n_init=<span class="number">20</span>)</span><br><span class="line">    y_pred = kmeans.fit_predict(z.data.cpu().numpy())</span><br><span class="line">    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)</span><br><span class="line">    eva(y, y_pred, <span class="string">'pretrain'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(args.max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">if</span> epoch % args.update_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># update_interval</span></span><br><span class="line">            A_pred, z, Q = model(data, adj, M)</span><br><span class="line"></span><br><span class="line">            q = Q.detach().data.cpu().numpy().argmax(<span class="number">1</span>)  <span class="comment"># Q</span></span><br><span class="line">            eva(y, q, epoch)</span><br><span class="line"></span><br><span class="line">        A_pred, z, q = model(data, adj, M)</span><br><span class="line">        p = target_distribution(Q.detach())</span><br><span class="line"></span><br><span class="line">        kl_loss = F.kl_div(q.log(), p, reduction=<span class="string">'batchmean'</span>)</span><br><span class="line">        re_loss = F.binary_cross_entropy(A_pred.view(<span class="number">-1</span>), adj_label.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        loss = <span class="number">10</span> * kl_loss + re_loss</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><h5 id="调用主函数"><a href="#调用主函数" class="headerlink" title="调用主函数"></a>调用主函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GAT</span><br><span class="line"><span class="keyword">from</span> evaluation <span class="keyword">import</span> eva</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># !python3 daegc.py --update_interval 5 --name Cora --epoch 45 --max_epoch 200</span></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=<span class="string">'train'</span>,</span><br><span class="line">        formatter_class=argparse.ArgumentDefaultsHelpFormatter)</span><br><span class="line">    parser.add_argument(<span class="string">'--name'</span>, type=str, default=<span class="string">'Citeseer'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, type=int, default=<span class="number">30</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--max_epoch'</span>, type=int, default=<span class="number">100</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--lr'</span>, type=float, default=<span class="number">0.0001</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--n_clusters'</span>, default=<span class="number">6</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--update_interval'</span>, default=<span class="number">1</span>, type=int)  <span class="comment"># [1,3,5]</span></span><br><span class="line">    parser.add_argument(<span class="string">'--hidden_size'</span>, default=<span class="number">256</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--embedding_size'</span>, default=<span class="number">16</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--weight_decay'</span>, type=int, default=<span class="number">5e-3</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--alpha'</span>, type=float, default=<span class="number">0.2</span>, help=<span class="string">'Alpha for the leaky_relu.'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.cuda = torch.cuda.is_available()</span><br><span class="line">    print(<span class="string">"use cuda: {}"</span>.format(args.cuda))</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> args.cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">    datasets = utils.get_dataset(args.name)</span><br><span class="line">    dataset = datasets[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name == <span class="string">'Citeseer'</span>:</span><br><span class="line">      args.lr = <span class="number">0.0001</span></span><br><span class="line">      args.k = <span class="literal">None</span></span><br><span class="line">      args.n_clusters = <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">'Cora'</span>:</span><br><span class="line">      args.lr = <span class="number">0.0001</span></span><br><span class="line">      args.k = <span class="literal">None</span></span><br><span class="line">      args.n_clusters = <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Pubmed"</span>:</span><br><span class="line">        args.lr = <span class="number">0.001</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.pretrain_path = <span class="string">f'./pretrain/predaegc_<span class="subst">{args.name}</span>_<span class="subst">{args.epoch}</span>.pkl'</span></span><br><span class="line">    args.input_dim = dataset.num_features</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>仔细一想，确实是自监督的，毕竟soft label和target label都是自己的embedding算出来的，然后再更新自己的embedding。</li><li>主要是依赖GAE算出来的那个embedding吧，初始化的影响可能比较大…核心还是GAT，Attention 牛批</li><li>虽然感觉同时更新这两个分布，还用自己算出来的分布来拟合另一个自己算出来的分布，这两个分布还有依赖关系，是有点扯…但是人家的效果还就是好emmmm 玄学</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tlKLqz.png" width="500"></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ul><li>贴一下实验对比的baseline</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8BF7AI.png" width="500"></li><li>贴一下在Cora，Citeseer和Pubmed上的结果对比，这里其实都是他们自己跑出来的结果，我看和AGC那篇论文，同样的baseline，效果都不一样…</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n02Knc.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PmeTG5.png" width="500"></li><li>贴一下分类的效果图，t-SNE可视化的node embedding</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JbFPVC.png" alt="figure3"></li><li>提到了超参数的设置，还有不同参数的效果对比，实验做的还是挺多的</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RdR0WV.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kMzoKK.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xlw5Xl.png" width="500"></li></ul></div><footer class="post-footer"><div class="post-tags"><a href="/tags/GNN/" rel="tag"><i class="fa fa-tag"></i> GNN</a> <a href="/tags/DAEGC/" rel="tag"><i class="fa fa-tag"></i> DAEGC</a> <a href="/tags/IJCAI-2019/" rel="tag"><i class="fa fa-tag"></i> IJCAI-2019</a> <a href="/tags/NodeClustering/" rel="tag"><i class="fa fa-tag"></i> NodeClustering</a> <a href="/tags/GraphClustering/" rel="tag"><i class="fa fa-tag"></i> GraphClustering</a> <a href="/tags/GAE/" rel="tag"><i class="fa fa-tag"></i> GAE</a></div><div class="post-nav"><div class="post-nav-item"><a href="/Papers/2021-05-20-gnn-survey-IEEE-2021" rel="prev" title="论文笔记 | A Comprehensive Survey on Graph Neural Networks"><i class="fa fa-chevron-left"></i> 论文笔记 | A Comprehensive Survey on Graph Neural Networks</a></div><div class="post-nav-item"><a href="/Papers/2021-05-29-gnn-agc-IJCAI-2019" rel="next" title="论文笔记 | Attributed Graph Clustering via Adaptive Graph Convolution">论文笔记 | Attributed Graph Clustering via Adaptive Graph Convolution <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">509k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">21:12</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019";
    this.page.identifier = "Papers/2021-05-26-gnn-daegc-IJCAI-2019.html";
    this.page.title = "论文笔记 | Attributed Graph Clustering A Deep Attentional Embedding Approach";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : 'bcee5f9b985ff9fe85b14feee0b18222',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>