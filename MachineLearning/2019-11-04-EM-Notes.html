<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="主要介绍EM的整个推导过程。"><meta property="og:type" content="article"><meta property="og:title" content="EM算法"><meta property="og:url" content="https://hanielxx.com/MachineLearning/2019-11-04-EM-Notes"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="主要介绍EM的整个推导过程。"><meta property="og:locale" content="en_US"><meta property="article:published_time" content="2019-11-04T05:42:24.000Z"><meta property="article:modified_time" content="2021-05-17T15:34:23.025Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="MachineLearning"><meta property="article:tag" content="Algorithm"><meta property="article:tag" content="EM"><meta property="article:tag" content="Notes"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://hanielxx.com/MachineLearning/2019-11-04-EM-Notes.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>EM算法 | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Jensen不等式"><span class="nav-number">1.</span> <span class="nav-text">1. Jensen不等式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概念"><span class="nav-number">1.1.</span> <span class="nav-text">概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Jensen的表述"><span class="nav-number">1.2.</span> <span class="nav-text">Jensen的表述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图解"><span class="nav-number">1.3.</span> <span class="nav-text">图解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-EM算法"><span class="nav-number">2.</span> <span class="nav-text">2. EM算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#隐藏变量"><span class="nav-number">2.1.</span> <span class="nav-text">隐藏变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#建立、优化下界"><span class="nav-number">2.2.</span> <span class="nav-text">建立、优化下界</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法步骤"><span class="nav-number">2.3.</span> <span class="nav-text">EM算法步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法收敛"><span class="nav-number">2.4.</span> <span class="nav-text">EM算法收敛</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#高斯混合模型"><span class="nav-number">3.</span> <span class="nav-text">高斯混合模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#E步"><span class="nav-number">3.1.</span> <span class="nav-text">E步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#M步"><span class="nav-number">3.2.</span> <span class="nav-text">M步</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#构造拉格郎日乘子优化"><span class="nav-number">3.3.</span> <span class="nav-text">构造拉格郎日乘子优化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#总结"><span class="nav-number">4.</span> <span class="nav-text">总结</span></a></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">142</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">207</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/MachineLearning/2019-11-04-EM-Notes"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">EM算法</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2019-11-04 13:42:24" itemprop="dateCreated datePublished" datetime="2019-11-04T13:42:24+08:00">2019-11-04</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-05-17 23:34:23" itemprop="dateModified" datetime="2021-05-17T23:34:23+08:00">2021-05-17</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/MachineLearning/2019-11-04-EM-Notes#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="MachineLearning/2019-11-04-EM-Notes.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>7.7k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>19 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>主要介绍EM的整个推导过程。</p></div><a id="more"></a><h2 id="1-Jensen不等式"><a href="#1-Jensen不等式" class="headerlink" title="1. Jensen不等式"></a>1. Jensen不等式</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>回顾优化理论中的一些概念:</p><ul><li>设f是定义域为实数的函数</li><li>如果对于所有的实数x，$$ f’’(x) \ge 0 $$，那么f是凸函数。</li><li>当x是向量时，如果其hessian矩阵H是半正定的（$$ H \ge 0 $$），那么f是凸函数。</li><li>如果$$ f’’(x) \gt 0 $$或者$$ H \gt 0 $$，那么称f是严格凸函数。</li></ul><h3 id="Jensen的表述"><a href="#Jensen的表述" class="headerlink" title="Jensen的表述"></a>Jensen的表述</h3><div class="note"><p>如果f是凸函数，X是随机变量，那么$$ E[f(X)] \ge f(EX) $$<br>特别地，如果f是严格凸函数，那么$$ E[f(X)] = f(EX)$$, 当且仅当$$ p(x=E[x]=1) $$，也就是说X是常量。</p><p>这里我们将$$ f(E[x]) $$简写为$$ f(EX) $$。</p></div><h3 id="图解"><a href="#图解" class="headerlink" title="图解"></a>图解</h3><p>如果用图表示会很清晰：</p><center></center><p>图中，实线f是凸函数，X是随机变量，有0.5的概率是a，有0.5的概率是b。（就像掷硬币一样）。<br>X的期望值就是a和b的中值了，图中可以看到$$ E[f(X)] \ge f(EX)$$成立。</p><p>当$$ f $$是（严格）凹函数即当且仅当$$ -f $$是（严格）凸函数。</p><p>Jensen不等式应用于凹函数时，不等号方向反向，也就是$$ E[f(X)] \le f(EX) $$。</p><h2 id="2-EM算法"><a href="#2-EM算法" class="headerlink" title="2. EM算法"></a>2. EM算法</h2><h3 id="隐藏变量"><a href="#隐藏变量" class="headerlink" title="隐藏变量"></a>隐藏变量</h3><p>给定的训练样本是$$ { x^{(1)},…,x^{(m)}} $$，样例间独立，我们想找到每个样例隐含的类别z，能使得$$p(x,z)$$最大。$$ p(x,z) $$的最大似然估计如下：</p><p>$$<br>\begin{equation}\begin{split}<br>{\ell}(\theta) &amp;= \sum_{i=1}^m log p (x;\theta) \<br>&amp;= \sum_{i=1}^m log \sum_z p(x,z;\theta)<br>\end{split}\end{equation}<br>$$</p><ul><li>第一步是对极大似然取对数</li><li>第二步是对每个样例的每个可能类别z求联合分布概率和</li><li>直接求$$ \theta $$一般比较困难，因为有隐藏变量z存在，但是一般确定了z后，求解就容易了。</li></ul><h3 id="建立、优化下界"><a href="#建立、优化下界" class="headerlink" title="建立、优化下界"></a>建立、优化下界</h3><p>EM是一种解决存在隐含变量优化问题的有效方法。既然不能直接最大化$$ \ell(\theta) $$，我们可以不断地建立$$ \ell $$的下界（E步），然后优化下界（M步）。</p><p>这句话比较抽象，看下面的。</p><ul><li>对于每一个样例i，让$$ Q_i $$表示该样例隐含变量z的某种分布，$$ Q(i) $$满足的条件是$$ \sum_z Q_i(z)=1, Q_i(z) \ge 0 $$。</li><li>如果z是连续性的，那么$$ Q_i $$是概率密度函数，需要将求和符号换做积分符号。</li><li>比如要将班上学生聚类，假设隐藏变量z是身高，那么就是连续的高斯分布。如果按照隐藏变量是男女，那么就是伯努利分布了。</li></ul><p>可以由前面阐述的内容得到下面的公式：</p><p>\begin{equation}\begin{split}<br>\sum_i logp(x^{(i)}; \theta) &amp;= \sum_i log \sum_{z^{(i)}} p(x^{(i)},z^{(i)}; \theta) \<br>&amp;= \sum_i log \sum_{z^{(i)}} Q_i(z^{(i)}) {p(x^{(i)}, z^{(i)}; \theta) \over Q_i(z^{(i)})} \<br>&amp; \ge \sum_i \sum_{z^{(i)}} Q_i(z^{(i)}) log {p(x^{(i)},z^{(i)}; \theta) \over Q_i(z^{(i)})}<br>\end{split}\end{equation}</p><ul><li>（1）到（2）比较直接，就是分子分母同乘以一个相等的函数。</li><li>（2）到（3）利用了Jensen不等式，考虑到$$ log(x) $$是凹函数（二阶导数小于0），而且$$ \sum_{z^{(i)}} Q_i(z^{(i)})[{ p(x^{(i)}), z^{(i)}; \theta] \over Q_i(z^{(i)}) }] $$就是$$ [{ {p(x^{(i)}, z^{(i)}; \theta)} \over Q_i(z^{(i)}) }]$$的期望（回想期望公式中的Lazy Statistician规则）</li></ul><p>设Y是随机变量X的函数$$Y=g(X) $$（g是连续函数），那么</p><ul><li>（1） X是离散型随机变量，它的分布律为$$ P(X=x_k)=p_k, k=1,2,.. $$。若$$ \sum_{k=1}^{\infty}g(x_k)p_k $$绝对收敛，则有<br>$$ E(Y) = E[g(X)] = \sum_{k=1}^{\infty}g(x_k)p_k $$</li><li>（2） X是连续型随机变量，它的概率密度为$$ f(x) $$，若$${\int}<em>{-\infty}^{\infty}g(x)f(x)dx $$绝对收敛，则有<br>$$ E(Y)=E[g(X)]={\int}</em>{-\infty}^{\infty}g(x)f(x)d(x) $$</li></ul><p>对应于上述问题，Y是$$ [{ p(x^{(i)}, z^{(i)}; \theta) \over Q_i(z^{(i)}) }] $$，X是$$ z^{(i)} $$，$$ Q_i(z^{(i)}) $$是$$ p_k $$，$$ g $$是$$ z^{(i)} $$到$$ [ { { p(x^{(i)}, z^{(i)}; \theta) } \over Q_i(z^{(i)}) }] $$的映射。这样解释了式子（2）中的期望，再根据凹函数时的Jensen不等式：<br>$$ f(E_{z^{(i)}{\sim}Q_i}[{ {p(x^{(i)}, z^{(i)}; \theta)} \over Q_i(z^{(i)}) }]) \ge E_{z^{(i)}{\sim}Q_i}[{ f({ p(x^{(i)},z^{(i)};\theta) } \over Q_i(z^{(i)})) }] $$</p><p>可以得到（3）。</p><p>这个过程可以看作是对 $${\ell} $$求了下界。对于$$ Q_i $$的选择，有多种可能，那种更好的？假设$$ \theta $$已经给定，那么$$ {\ell}(\theta) $$的值就决定于$$ Q_i(z^{(i)}) $$和$$ p(x^{(i)},z^{(i)}) $$了。我们可以通过调整这两个概率使下界不断上升，以逼近$$ {\ell} $$的真实值，那么什么时候算是调整好了呢？当不等式变成等式时，说明我们调整后的概率能够等价于$$ {\ell} $$了。按照这个思路，我们要找到等式成立的条件。</p><p>根据Jensen不等式，要想让等式成立，需要让随机变量变成常数值，这里得到：<br>$$ { p(x^{(i)},z^{(i)}; \theta) \over Q_i(z^{(i)})} =c $$</p><p>c为常数，不依赖于$$ z^{(i)} $$。对此式子做进一步推导，我们知道$$ \sum_z Q(z^{(i)})=q $$，那么也就有$$ \sum_z p(x^{(i)},z^{(i)};\theta) =c $$，（多个等式分子分母相加不变，这个认为每个样例的两个概率比值都是c），那么有下式：</p><p>$$<br>\begin{equation}\begin{split}<br>Q_i(Z^{(i)}) &amp;= {p(x^{(i)},z^{(i)};\theta) \over \sum_z p(x^{(i)},z;\theta)} \<br>&amp;= {p(x^{(i)},z^{(i)};\theta) \over p(x^{(i)};\theta) }\<br>&amp;= p(z^{(i)}|x^{(i)};\theta)<br>\end{split}\end{equation}<br>$$</p><p>至此，我们推出了在固定其他参数$$ \theta $$后，$$ Q_i(z^{(i)}) $$的计算公式就是后验概率，解决$$Q_i(z^{(i)}) $$如何选择的问题。这一步就是E步，建立$$ {\ell}(\theta) $$的下界。接下来的M步，就是在给定i$$ Q_i(z^{(i)}) $$后，调整$$ \theta $$，去极大化$$ {\ell}(\theta) $$的下界（在固定$$ Q_i(z^{(i)}) $$后，下界还可以调整的更大）。</p><h3 id="EM算法步骤"><a href="#EM算法步骤" class="headerlink" title="EM算法步骤"></a>EM算法步骤</h3><p><strong>一般的EM算法的步骤如下：</strong></p><p>循环重复直到收敛</p><ol><li><p>（E步）对于每一个i，计算：<br>$$ Q_i(z^{(i)}) := p(z^{(i)}|x^{(i)};\theta) $$</p></li><li><p>（M步）计算：<br>$$ \theta := arg max_\theta \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log{p(z^{(i)},z^{(i)};\theta) \over Q_i(z^{(i)}) } $$</p></li></ol><h3 id="EM算法收敛"><a href="#EM算法收敛" class="headerlink" title="EM算法收敛"></a>EM算法收敛</h3><p>那么究竟怎么确保EM收敛？</p><ul><li>假定$$ \theta^{(t)} 和 \theta^{(t+1)} $$是EM第t次和t+1次迭代后的结果。</li><li>如果我们证明了$$ {\ell}(\theta^{(t)}) \le {\ell}(\theta^{(t+1)}) $$，也就是说极大似然估计单调增加，那么最终我们会到达最大似然估计的最大值。</li></ul><p>下面来证明，选定$$ \theta^{(t)} $$后，我们得到E步<br>$$ Q_i^{(t)} := p(z^{(i)} | x^{(i)} ; \theta^{(t)} ) $$</p><p>这一步保证了在给定$$ \theta^{(t)} $$时，Jensen不等式中的等式成立，也就是<br>$$ {\ell}(\theta^{(t)}) = \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)}) log{ p(x^{(i)},z^{(i)};\theta^{(t)}) \over Q_i^{(i)}(z^{(i)})} $$</p><p>然后进行M步，固定$$ Q_i^{(t)} $$，并将$$ \theta^{(t)} $$视作变量，对上面的$$ {\ell}(\theta^{(t)}) $$求导后，得到$$ \theta^{(t+1)} $$，这样经过一些推导会有以下式子成立：<br>$$<br>\begin{equation}\begin{split}<br>{\ell}(\theta^{(t+1)}) &amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(t)}(z^{(i)})log{ p(x^{(i)},z^{(i)});\theta^{(t+1)} \over Q_i^{(t)}(z^{(i)})} \<br>&amp;\ge \sum_i \sum_{z^{(i)}} Q_i^{(i)}(z^{(i)})log{ p(x^{(i)},z^{(i)};\theta^{(t)}) \over Q_i^{(t)(z^{(i)})}} \<br>&amp;= {\ell}(\theta^{(t)})<br>\end{split}\end{equation}<br>$$</p><p>解释第（4）步，得到$$ \theta^{(t+1)} $$，只是最大化$$ {\ell}(\theta^{(t)}) $$，也就是$$ {\ell}(\theta^{(t+1)}) $$的下界，而没有使等式成立，等式成立只有是在固定$$ \theta $$，并按E步得到$$ Q_i $$时才能成立。</p><p>况且根据我们前面得到的下式，对于所有的$$ Q_i 和 \theta $$都成立<br>$$ {\ell} \ge \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log{ p(x^{(i)},z^{(i)};\theta) \over Q_i(z^{(i)})} $$</p><p>第（5）步利用了M步的定义，M步就是将$$ \theta^{(t)} $$调整到$$ |the^{(t+1)} $$，使得下界最大化。因此（5）成立，（6）是之前的等式结果。</p><p>这样就证明了$$ {\ell}(\theta) $$会单调增加。一种收敛方法是$$ {\ell}(\theta) $$不再变化，还有一种就是变化幅度很小。</p><p>再次解释一下（4）、（5）、（6）。首先（4）对所有的参数都满足，而其等式成立条件只是在固定$$ \theta $$，并调整好Q时成立，而第（4）步只是固定Q，调整$$ \theta $$，不能保证等式一定成立。（4）到（5）就是M步的定义，（5）到（6）是前面E步所保证等式成立条件。也就是说E步会将下界拉到与$$ {\ell}(\theta) $$一个特定值（这里$$ \theta^{(t)} $$）一样的高度，而此时发现下界仍然可以上升，因此经过M步后，下界又被拉升，但达不到与$$ {\ell}(\theta) $$另外一个特定值一样的高度，之后E步又将下界拉到与这个特定值一样的高度，重复下去，直到最大值。</p><p>如果我们定义:</p><p>$$ J(Q,\theta) = \sum_i \sum_{z^{(i)}} Q_i(z^{(i)})log{ p(x^{(i)},z^{(i)};\theta) \over Q_i(z^{(i)})} $$</p><p>从前面的推导中我们知道$$ {\ell}(\theta) \ge J(Q,\theta) $$，EM可以看作是J的坐标上升法，E步固定$$ \theta $$，优化$$ Q $$，M步固定$$ Q $$优化$$ \theta $$。</p><h2 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h2><p>我们已经知道了EM的精髓和推导过程，再次审视一下混合高斯模型。之前提到的混合高斯模型的参数$$ \phi , \mu 和 \sum $$计算公式都是根据很多假定得出的，有些没有说明来由。为了简单，这里在M步只给出$$ \phi 和 \mu $$的推导方法。</p><h3 id="E步"><a href="#E步" class="headerlink" title="E步"></a>E步</h3><p>E步很简单，按照一般EM公式得到：</p><p>$$ \omega_j^{(i)} = Q_i(z^{(i)}=j) = P(z^{(i)}=j|x^{(i)}; \phi,\mu,\sum) $$</p><p>简单解释就是每个样例i的隐含类别$$ z^{(i)} $$为j的概率可以通过后验概率计算得到。</p><h3 id="M步"><a href="#M步" class="headerlink" title="M步"></a>M步</h3><p>在M步中，我们需要在固定$$ Q_i(z^{(i)}) $$后最大化最大似然估计，也就是</p><p>$$<br>\begin{equation}\begin{split}<br>\sum_{i=1}^m \sum_{z^{(i)}} &amp; Q_i(z^{(i)})log{ p(x^{(i)},z^{(i)};\phi, \mu, \sum) \over Q_i(z^{(i)})} \<br>&amp;= \sum_{i=1}^m \sum_{j=1}^k Q_i(z^{(i)}=j)log{ p(x^{(i)}| z^{(i)}=j;\mu,\sum)p(z^{(i)}=j;\phi) \over Q_i(z^{(i)}=j)} \<br>&amp;= \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} log { {1 \over (2\pi)^{n/2}|\sum_j|^{1/2} } exp(-{1 \over 2}(x^{(i)}-\mu_j)^T \sum_j^{-1}(x^{(i)}-\mu_j)) \cdot \phi_j \over \omega_j^{(i)} }<br>\end{split}\end{equation}<br>$$</p><p>这是将$$ z^{(i)} $$的k种情况展开后的样子，未知参数$$ \phi_j , \mu_j 和 \sum_j$$。</p><p>固定$$ \phi_j和\sum_j $$对$$ \mu_j $$求导得</p><center></center><p>等于0时，得到<br>$$ \mu_l := {\sum_{i=1}^m \omega_l^{(i)}x^{(i)} \over \sum_{i=1}^m \omega_l^{(i)}} $$</p><p>这就是我们之前模型中的$$ \mu $$的更新公式。</p><p>然后推导$$ \phi_j $$的更新公式。看之前得到的</p><p>$$ \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} log { {1 \over (2\pi)^{n/2}|\sum_j|^{1/2}} exp(-{1 \over 2}(x^{(i)}-\mu_j)^T \sum_j^{-1}(x^{(i)}-\mu_j)) \cdot \phi_j \over \omega_j^{(i)}} $$</p><p>在$$ \phi和\mu $$确定后，分子上面的一串都是常数了，实际上需要优化的公式是：</p><p>$$ \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} log \phi_j $$</p><p>需要知道的是，$$ \phi_j $$还需要满足一定的约束条件就是$$ \sum_{j=1}^k \phi_j =1 $$。</p><h3 id="构造拉格郎日乘子优化"><a href="#构造拉格郎日乘子优化" class="headerlink" title="构造拉格郎日乘子优化"></a>构造拉格郎日乘子优化</h3><p>这个优化问题我们很熟悉了，直接构造拉格朗日乘子。<br>$$ L(\phi) = \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} log \phi_j + \beta(\sum_{j=1}^k \phi_j-1)$$</p><p>还有一点就是$$ \phi_j \ge 0 $$，但这一点会在得到的公式里自动满足。</p><p>求导得：<br>$$ {\partial \over \partial \phi_j} L( \phi ) = \sum_{i=1}^m {\omega_j^{(i)} \over \phi_j}+\beta $$</p><p>等于0时，得到:</p><p>$$ \phi_j = {\sum_{i=1}^m \omega_j^{(i)} \over -\beta} $$</p><p>也就是说$$ \phi_j \propto \sum_{i=1}^m \omega_j^{(i)} $$。再次使用$$ \sum_{j=1}^k \phi_j = 1 $$，得到</p><p>$$ -\beta = \sum_{i=1}^m \sum_{j=1}^k \omega_j^{(i)} = \sum_{i=1}^m 1 = m $$</p><p>这样就神奇地得到了$$ \beta $$。</p><p>那么就顺势得到M步中$$ \phi_j $$的更新公式：</p><p>$$ \phi_j := {1 \over m} \sum_{i=1}^m \omega_j^{(i)} $$</p><p>$$\sum$$的推导也类似，不过稍微复杂一些，毕竟是矩阵。结果在之前的混合高斯模型中已经给出。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>如果将样本看作观察值，潜在类别看作是隐藏变量，那么聚类问题也就是参数估计问题。</p><p>只不过聚类问题中参数分为隐含类别变量和其他参数，这犹如在x-y坐标系中找一个曲线的极值，然而曲线函数不能直接求导，因此什么梯度下降方法就不适用了。<br>但固定一个变量后，另外一个可以通过求导得到，因此可以使用坐标上升法，一次固定一个变量，对另外的求极值，最后逐步逼近极值。</p><p>对应到EM上，E步估计隐含变量，M步估计其他参数，交替将极值推向最大。EM中还有&ldquo;硬&rdquo;指定和&ldquo;软&rdquo;指定的概念，&ldquo;软&rdquo;指定看似更为合理，但计算量要大，&ldquo;硬&rdquo;指定在某些场合如K-means中更为实用（要是保持一个样本点到其他所有中心的概率，就会很麻烦）。</p><p>另外，EM的收敛性证明方法确实很牛，能够利用log的凹函数性质，还能够想到利用创造下界，拉平函数下界，优化下界的方法来逐步逼近极大值。而且每一步迭代都能保证是单调的。</p><p>最重要的是证明的数学公式非常精妙，硬是分子分母都乘以z的概率变成期望来套上Jensen不等式。</p><p>有一个EM应用的例子，明白地说就是将班上学生的身高都放在一起，要求聚成两个类。<br>这些身高可以看作是男生身高的高斯分布和女生身高的高斯分布组成。因此变成了如何估计每个样例是男生还是女生，然后在确定男女生情况下，如何估计均值和方差，里面也给出了公式，有兴趣可以参考。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a> <a href="/tags/Algorithm/" rel="tag"><i class="fa fa-tag"></i> Algorithm</a> <a href="/tags/EM/" rel="tag"><i class="fa fa-tag"></i> EM</a> <a href="/tags/Notes/" rel="tag"><i class="fa fa-tag"></i> Notes</a></div><div class="post-nav"><div class="post-nav-item"><a href="/LeetCode/2019-11-04-LeetCode-031-Next-Permutation" rel="prev" title="LeetCode-031-Next Permutation"><i class="fa fa-chevron-left"></i> LeetCode-031-Next Permutation</a></div><div class="post-nav-item"><a href="/LeetCode/2019-11-05-LeetCode-033-Search-in-Rotated-Sorted-Array" rel="next" title="LeetCode-033-Search in Rotated Sorted Array">LeetCode-033-Search in Rotated Sorted Array <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">565k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">23:32</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/MachineLearning/2019-11-04-EM-Notes";
    this.page.identifier = "MachineLearning/2019-11-04-EM-Notes.html";
    this.page.title = "EM算法";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : '71080eeba2870f7a2e208e93eb0020f5',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>