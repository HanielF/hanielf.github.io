<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="进一步理解和使用AllenNLP这个框架，续前面的AllenNLP框架学习笔记（一）"><meta property="og:type" content="article"><meta property="og:title" content="AllenNLP框架学习笔记（二）"><meta property="og:url" content="https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="进一步理解和使用AllenNLP这个框架，续前面的AllenNLP框架学习笔记（一）"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://i.loli.net/2021/04/26/cezXt9NnbB1foYd.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/I6x77E.png"><meta property="article:published_time" content="2021-04-26T12:11:01.000Z"><meta property="article:modified_time" content="2021-05-25T15:36:53.752Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="AllenNLP"><meta property="article:tag" content="NLP"><meta property="article:tag" content="OpenLibrary"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="Pytorch"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://i.loli.net/2021/04/26/cezXt9NnbB1foYd.png"><link rel="canonical" href="https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>AllenNLP框架学习笔记（二） | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#数据相关"><span class="nav-number">1.</span> <span class="nav-text">数据相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fields"><span class="nav-number">1.1.</span> <span class="nav-text">Fields</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#instances"><span class="nav-number">1.2.</span> <span class="nav-text">instances</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型相关"><span class="nav-number">2.</span> <span class="nav-text">模型相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#通用的架构"><span class="nav-number">3.</span> <span class="nav-text">通用的架构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#特征表示相关"><span class="nav-number">4.</span> <span class="nav-text">特征表示相关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#文本到特征"><span class="nav-number">4.1.</span> <span class="nav-text">文本到特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenizer"><span class="nav-number">4.2.</span> <span class="nav-text">Tokenizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextFields"><span class="nav-number">4.3.</span> <span class="nav-text">TextFields</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TokenIndexer"><span class="nav-number">4.4.</span> <span class="nav-text">TokenIndexer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TextFieldEmbedders"><span class="nav-number">4.5.</span> <span class="nav-text">TextFieldEmbedders</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#单个Indexer"><span class="nav-number">4.5.1.</span> <span class="nav-text">单个Indexer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#多个Indexer"><span class="nav-number">4.5.2.</span> <span class="nav-text">多个Indexer</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tokenizer-TokenIndexer-ToeknEmbedders的配合"><span class="nav-number">4.6.</span> <span class="nav-text">Tokenizer, TokenIndexer, ToeknEmbedders的配合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用预训练编码"><span class="nav-number">4.7.</span> <span class="nav-text">用预训练编码</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#获取text-fields"><span class="nav-number">4.7.1.</span> <span class="nav-text">获取text_fields</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#embedding"><span class="nav-number">4.7.2.</span> <span class="nav-text">embedding</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word-level模型同时使用wordpiece的transformer"><span class="nav-number">4.8.</span> <span class="nav-text">word-level模型同时使用wordpiece的transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#pooling-over-Wordpieces"><span class="nav-number">4.8.1.</span> <span class="nav-text">pooling over Wordpieces</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#扩散标签到wordpiece"><span class="nav-number">4.8.2.</span> <span class="nav-text">扩散标签到wordpiece</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#padding和mask"><span class="nav-number">4.9.</span> <span class="nav-text">padding和mask</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用TextField输出的TextFieldTensors"><span class="nav-number">4.10.</span> <span class="nav-text">使用TextField输出的TextFieldTensors</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#未完"><span class="nav-number">5.</span> <span class="nav-text">未完</span></a></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">152</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">239</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">AllenNLP框架学习笔记（二）</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-04-26 20:11:01" itemprop="dateCreated datePublished" datetime="2021-04-26T20:11:01+08:00">2021-04-26</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-05-25 23:36:53" itemprop="dateModified" datetime="2021-05-25T23:36:53+08:00">2021-05-25</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/MachineLearning/2021-04-26-allennlp-notes-2#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="MachineLearning/2021-04-26-allennlp-notes-2.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>35k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>1:28</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>进一步理解和使用AllenNLP这个框架，续前面的<a href="https://hanielxx.com/MachineLearning/2021-04-14-allennlp-notes">AllenNLP框架学习笔记（一）</a></p></div><a id="more"></a><h2 id="数据相关"><a href="#数据相关" class="headerlink" title="数据相关"></a>数据相关</h2><h3 id="Fields"><a href="#Fields" class="headerlink" title="Fields"></a>Fields</h3><p>所有的数据都被包装成<code>Fields</code>类，一个field包含一个数据样本，在模型中会被转换为tensor作为输入和输出。</p><p>有很多种类的fields类</p><ul><li>LabelField</li><li>MultiLabelField</li><li>SequenceLabelField</li><li>SpanField</li><li>ListField</li><li>ArrayField</li></ul><p><img src="https://i.loli.net/2021/04/26/cezXt9NnbB1foYd.png"></p><p>用法参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To create fields, simply pass the data to constructor.</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Don't worry about the token_indexers too much for now. We have a whole</span></span><br><span class="line"><span class="comment"># chapter on why TextFields are set up this way, and how they work.</span></span><br><span class="line">tokens = [Token(<span class="string">"The"</span>), Token(<span class="string">"best"</span>), Token(<span class="string">"movie"</span>), Token(<span class="string">"ever"</span>), Token(<span class="string">"!"</span>)]</span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers=token_indexers)</span><br><span class="line"></span><br><span class="line">label_field = LabelField(<span class="string">"pos"</span>)</span><br><span class="line"></span><br><span class="line">sequence_label_field = SequenceLabelField(</span><br><span class="line">    [<span class="string">"DET"</span>, <span class="string">"ADJ"</span>, <span class="string">"NOUN"</span>, <span class="string">"ADV"</span>, <span class="string">"PUNKT"</span>], text_field</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can use print() fields to see their content</span></span><br><span class="line">print(text_field)</span><br><span class="line">print(label_field)</span><br><span class="line">print(sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Many of the fields implement native python methods in intuitive ways</span></span><br><span class="line">print(len(sequence_label_field))</span><br><span class="line">print(label <span class="keyword">for</span> label <span class="keyword">in</span> sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to create empty fields of the same type</span></span><br><span class="line">print(text_field.empty_field())</span><br><span class="line">print(label_field.empty_field())</span><br><span class="line">print(sequence_label_field.empty_field())</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can count vocabulary items in fields</span></span><br><span class="line">counter: Dict[str, Dict[str, int]] = defaultdict(Counter)</span><br><span class="line">text_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line">label_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line">sequence_label_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Vocabulary for indexing fields</span></span><br><span class="line">vocab = Vocabulary(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to turn themselves into tensors</span></span><br><span class="line">text_field.index(vocab)</span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> in practice, we will batch together instances and use the maximum padding</span></span><br><span class="line"><span class="comment"># lengths, instead of getting them from a single instance.</span></span><br><span class="line"><span class="comment"># You can print this if you want to see what the padding_lengths dictionary looks</span></span><br><span class="line"><span class="comment"># like, but it can sometimes be a bit cryptic.</span></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line">print(text_field.as_tensor(padding_lengths))</span><br><span class="line"></span><br><span class="line">label_field.index(vocab)</span><br><span class="line">print(label_field.as_tensor(label_field.get_padding_lengths()))</span><br><span class="line"></span><br><span class="line">sequence_label_field.index(vocab)</span><br><span class="line">padding_lengths = sequence_label_field.get_padding_lengths()</span><br><span class="line">print(sequence_label_field.as_tensor(padding_lengths))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to batch tensors</span></span><br><span class="line">tensor1 = label_field.as_tensor(label_field.get_padding_lengths())</span><br><span class="line"></span><br><span class="line">label_field2 = LabelField(<span class="string">"pos"</span>)</span><br><span class="line">label_field2.index(vocab)</span><br><span class="line">tensor2 = label_field2.as_tensor(label_field2.get_padding_lengths())</span><br><span class="line"></span><br><span class="line">batched_tensors = label_field.batch_tensors([tensor1, tensor2])</span><br><span class="line">print(batched_tensors)</span><br></pre></td></tr></table></figure><h3 id="instances"><a href="#instances" class="headerlink" title="instances"></a>instances</h3><p>一个Instance是一个模型预测的原子单位，是Fields类实例的集合，也是dataset的组成单位。fields-&gt;instances-&gt;datasets。</p><p>Instances用dataset reader创建，然后可以从里面获得<code>Vocabulary</code>。vocab可以把<code>Fields</code>map到id上。然后这些instances会被转换为batch tensor输入到模型中。</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/I6x77E.png" alt="I6x77E"></p><p>通过把field name和对应fields的字典传进constructor，可以创建instances。instances可以转换为field name和对应tensors的字典，每个field name对应的tensor可以被batch组织进模型中。</p><p>而且每个Field 名字是很重要的，因为最后包含tensor的字典会和key一起传进模型，<strong>所以要和模型forward方法中的参数保持一致。</strong></p><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Fields</span></span><br><span class="line">tokens = [Token(<span class="string">"The"</span>), Token(<span class="string">"best"</span>), Token(<span class="string">"movie"</span>), Token(<span class="string">"ever"</span>), Token(<span class="string">"!"</span>)]</span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers=token_indexers)</span><br><span class="line"></span><br><span class="line">label_field = LabelField(<span class="string">"pos"</span>)</span><br><span class="line"></span><br><span class="line">sequence_label_field = SequenceLabelField(</span><br><span class="line">    [<span class="string">"DET"</span>, <span class="string">"ADJ"</span>, <span class="string">"NOUN"</span>, <span class="string">"ADV"</span>, <span class="string">"PUNKT"</span>], text_field</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an Instance</span></span><br><span class="line">fields: Dict[str, Field] = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: text_field,</span><br><span class="line">    <span class="string">"label"</span>: label_field,</span><br><span class="line">&#125;</span><br><span class="line">instance = Instance(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can add fields later</span></span><br><span class="line">instance.add_field(<span class="string">"label_seq"</span>, sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can simply use print() to see the instance's content</span></span><br><span class="line">print(instance)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Vocabulary</span></span><br><span class="line">counter: Dict[str, Dict[str, int]] = defaultdict(Counter)</span><br><span class="line">instance.count_vocab_items(counter)</span><br><span class="line">vocab = Vocabulary(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert all strings in all of the fields into integer IDs by calling index_fields()</span></span><br><span class="line">instance.index_fields(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instances know how to turn themselves into a dict of tensors.  When we call this</span></span><br><span class="line"><span class="comment"># method in our data code, we additionally give a `padding_lengths` argument.</span></span><br><span class="line"><span class="comment"># We will pass this dictionary to the model as **tensors, so be sure the keys</span></span><br><span class="line"><span class="comment"># match what the model expects.</span></span><br><span class="line">tensors = instance.as_tensor_dict()</span><br><span class="line">print(tensors)</span><br></pre></td></tr></table></figure><p>基本上是：token-&gt; token_indexers-&gt;text_field-&gt;label_field-&gt;fields-&gt;instance-&gt;counter-&gt;instance.count_vocab_items(counter)-&gt;vocab-&gt;instance.index_fields()</p><p>待补充</p><h2 id="模型相关"><a href="#模型相关" class="headerlink" title="模型相关"></a>模型相关</h2><p>中间待补充</p><h2 id="通用的架构"><a href="#通用的架构" class="headerlink" title="通用的架构"></a>通用的架构</h2><p>待补充</p><h2 id="特征表示相关"><a href="#特征表示相关" class="headerlink" title="特征表示相关"></a>特征表示相关</h2><h3 id="文本到特征"><a href="#文本到特征" class="headerlink" title="文本到特征"></a>文本到特征</h3><p>要将文本进行编码</p><ol><li><code>Tokenizer</code>将文本拆分成独立的<code>Tokens</code></li><li>用<code>TextField, TokenIndexer, and Vocabulary</code>将Tokens转换成index</li><li>用<code>TextFieldEmbedder</code>获得每个Token的编码，只有这一步的参数是learnable的</li></ol><p>即Text-&gt;Tokens-&gt;Ids-&gt;Vectors，前两步骤有<code>DatasetReader</code>负责，最后一步由<code>Model</code>负责。</p><p>编码方式很多，常见的Glove、Word2vec、字符级别的CNN、POS tag embedding、结合Glove和CNN，以及<strong>wordpieces级别的BERT</strong></p><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><p>主要有三种方式tokenize</p><ul><li>字符（包括空格），Characters (“AllenNLP is great” → [“A”, “l”, “l”, “e”, “n”, “N”, “L”, “P”, “ “, “i”, “s”, “ “, “g”, “r”, “e”, “a”, “t”])</li><li>Wordpieces (“AllenNLP is great” → [“Allen”, “##NL”, “##P”, “is”, “great”])</li><li>Words (“AllenNLP is great” → [“AllenNLP”, “is”, “great”])</li></ul><p>常用：</p><ul><li>SpacyTokenizer</li><li>PretrainedTransformerTokenizer, which uses a tokenizer from Hugging Face’s transformers library</li><li>CharacterTokenizer, which splits a string up into individual characters, including whitespace.</li></ul><p>每个tokenizer都实现了<code>tokenize()</code>方法，会返回Tokens列表。一个Token是一个轻量级的dataclass</p><h3 id="TextFields"><a href="#TextFields" class="headerlink" title="TextFields"></a>TextFields</h3><p>A TextField takes a list of Tokens from a Tokenizer and represents each of them as an array that can be converted into a vector by the model</p><p>TextFields读入Tokens列表，然后把每个token表示成一个idx。</p><p>包括这些方法，主要是给TokenIndexers负责，</p><ul><li>counting vocabulary items</li><li>converting strings to integers and then tensors</li><li>batching together several tensors with proper padding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = ...  <span class="comment"># Whatever tokenizer you want</span></span><br><span class="line">sentence = <span class="string">"We are learning about TextFields"</span></span><br><span class="line">tokens = tokenizer.tokenize(sentence)</span><br><span class="line">token_indexers = &#123;...&#125;  <span class="comment"># we'll talk about this in the next section</span></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">...</span><br><span class="line">instance = Instance(&#123;<span class="string">"sentence"</span>: text_field, ...&#125;)</span><br></pre></td></tr></table></figure><h3 id="TokenIndexer"><a href="#TokenIndexer" class="headerlink" title="TokenIndexer"></a>TokenIndexer</h3><p>所有的Token idx都从2开始，0是padding，1是unk</p><p>可以组合使用不同的TokenIndexer，然后进行embedding的融合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splits text into words (instead of wordpieces or characters).</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with both an id from a vocabulary and a sequence of</span></span><br><span class="line"><span class="comment"># characters.</span></span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text ."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The setup here is the same as what we saw above.</span></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line"><span class="comment"># Note now that we have two entries in this output dictionary,</span></span><br><span class="line"><span class="comment"># one for each indexer that we specified.</span></span><br><span class="line">print(<span class="string">"Combined tensor dictionary:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we split text into words with part-of-speech tags, using Spacy's POS tagger.</span></span><br><span class="line"><span class="comment"># This will result in the `tag_` variable being set on each `Token` object, which</span></span><br><span class="line"><span class="comment"># we will read in the indexer.</span></span><br><span class="line">tokenizer = SpacyTokenizer(pos_tags=<span class="literal">True</span>)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with (1) an id from a vocabulary, (2) a sequence of</span></span><br><span class="line"><span class="comment"># characters, and (3) part of speech tag ids.</span></span><br><span class="line">token_indexers = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">    <span class="string">"pos_tags"</span>: SingleIdTokenIndexer(namespace=<span class="string">"pos_tag_vocab"</span>, feature_name=<span class="string">"tag_"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Spacy tokens:"</span>, tokens)</span><br><span class="line">print(<span class="string">"POS tags:"</span>, [token.tag_ <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"Tensor dict with POS tags:"</span>, tensor_dict)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Spacy models <span class="string">'en_core_web_sm'</span> <span class="keyword">not</span> found.  Downloading <span class="keyword">and</span> installing.</span><br><span class="line">Tokens: [This, <span class="keyword">is</span>, some, text, .]</span><br><span class="line">Combined tensor <span class="built_in">dictionary</span>: &#123;<span class="string">'tokens'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])&#125;, <span class="string">'token_characters'</span>: &#123;<span class="string">'token_characters'</span>: tensor([[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">12</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>]])&#125;&#125;</span><br><span class="line">✔ Download <span class="keyword">and</span> installation successful</span><br><span class="line">You can now load the package via spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">Spacy tokens: [This, <span class="keyword">is</span>, some, text, .]</span><br><span class="line">POS tags: [<span class="string">'DT'</span>, <span class="string">'VBZ'</span>, <span class="string">'DT'</span>, <span class="string">'NN'</span>, <span class="string">'.'</span>]</span><br><span class="line">Tensor dict with POS tags: &#123;<span class="string">'tokens'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])&#125;, <span class="string">'token_characters'</span>: &#123;<span class="string">'token_characters'</span>: tensor([[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">12</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>]])&#125;, <span class="string">'pos_tags'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="TextFieldEmbedders"><a href="#TextFieldEmbedders" class="headerlink" title="TextFieldEmbedders"></a>TextFieldEmbedders</h3><h4 id="单个Indexer"><a href="#单个Indexer" class="headerlink" title="单个Indexer"></a>单个Indexer</h4><p>allennlp数据处理的时候，会把instances中的TextFiled转换成TextFiledTensors数据结构，即：<code>Dict[str, Dict[str, torch.Tensor]]</code>，外围str对应每个TokenIndexers，里面对应TokenIndexer生成的idx。这个会被输入到TextFieldEmbedder中，用TokenEmbedder来embeds or encodes。</p><p>下面的例子是两个单独的embedding，分别是普通的embedding和cnn encoder。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer;</span></span><br><span class="line"><span class="comment"># Note that we added the batch dimension at the front.  You choose the 'indexer1'</span></span><br><span class="line"><span class="comment"># name when you configure your data processing code.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"indexer1"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor(</span><br><span class="line">            [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">3</span>]]</span><br><span class="line">         )</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># You would typically get the number of embeddings here from the vocabulary;</span></span><br><span class="line"><span class="comment"># if you use `allennlp train`, there is a separate process for instantiating the</span></span><br><span class="line"><span class="comment"># Embedding object using the vocabulary that you don't need to worry about for</span></span><br><span class="line"><span class="comment"># now.</span></span><br><span class="line">embedding = Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This 'indexer1' key must match the 'indexer1' key in the `token_tensor` above.</span></span><br><span class="line"><span class="comment"># We use these names to align the TokenIndexers used in the data code with the</span></span><br><span class="line"><span class="comment"># TokenEmbedders that do the work on the model side.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"indexer1"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"Using the TextFieldEmbedder:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># As we've said a few times, what's going on inside is that we match keys between</span></span><br><span class="line"><span class="comment"># the token tensor and the token embedders, then pass the inner dictionary to the</span></span><br><span class="line"><span class="comment"># token embedder.  The above lines perform the following logic:</span></span><br><span class="line">embedded_tokens = embedding(**token_tensor[<span class="string">"indexer1"</span>])</span><br><span class="line">print(<span class="string">"Using the Embedding directly:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a TokenCharactersIndexer</span></span><br><span class="line"><span class="comment"># Note that we added the batch dimension at the front. Don't worry too much</span></span><br><span class="line"><span class="comment"># about the magic 'token_characters' key - that is hard-coded to be produced</span></span><br><span class="line"><span class="comment"># by the TokenCharactersIndexer, and accepted by TokenCharactersEncoder;</span></span><br><span class="line"><span class="comment"># you don't have to produce those yourself in normal settings, it's done for you.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"indexer2"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">1</span>, <span class="number">3</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">character_embedding = Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">3</span>, num_filters=<span class="number">4</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Again here, the 'indexer2' key is arbitrary. It just has to match whatever key</span></span><br><span class="line"><span class="comment"># you gave to the corresponding TokenIndexer in your data code, which ends up</span></span><br><span class="line"><span class="comment"># as the top-level key in the token_tensor dictionary.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"indexer2"</span>: token_encoder&#125;)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"With a character CNN:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Using the TextFieldEmbedder: tensor([[[ <span class="number">0.6184</span>,  <span class="number">0.3636</span>, <span class="number">-0.6774</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>],</span><br><span class="line">         [ <span class="number">0.2992</span>, <span class="number">-0.2631</span>, <span class="number">-0.4046</span>],</span><br><span class="line">         [ <span class="number">0.4240</span>,  <span class="number">0.2915</span>,  <span class="number">0.6677</span>],</span><br><span class="line">         [<span class="number">-0.6025</span>,  <span class="number">0.2038</span>, <span class="number">-0.0412</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br><span class="line">Using the Embedding directly: tensor([[[ <span class="number">0.6184</span>,  <span class="number">0.3636</span>, <span class="number">-0.6774</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>],</span><br><span class="line">         [ <span class="number">0.2992</span>, <span class="number">-0.2631</span>, <span class="number">-0.4046</span>],</span><br><span class="line">         [ <span class="number">0.4240</span>,  <span class="number">0.2915</span>,  <span class="number">0.6677</span>],</span><br><span class="line">         [<span class="number">-0.6025</span>,  <span class="number">0.2038</span>, <span class="number">-0.0412</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>]]], grad_fn=&lt;EmbeddingBackward&gt;)</span><br><span class="line">With a character CNN: tensor([[[<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.0000</span>, <span class="number">0.3550</span>, <span class="number">0.5636</span>, <span class="number">0.3409</span>],</span><br><span class="line">         [<span class="number">0.1199</span>, <span class="number">0.0000</span>, <span class="number">0.2336</span>, <span class="number">0.2741</span>],</span><br><span class="line">         [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><p>注意到上面的都是用<code>BasicTextFieldEmbedder(token_embedders={&quot;indexer_name&quot;: embedding or encoder})</code>。</p><h4 id="多个Indexer"><a href="#多个Indexer" class="headerlink" title="多个Indexer"></a>多个Indexer</h4><p>下面的例子是多个Indexer，其中一个是单个word，另一个是sequence of characters per token，最后一个是different single ID,对应不同的 speech tags。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer</span></span><br><span class="line"><span class="comment"># and a TokenCharactersIndexer; see the code snippet above. This time we're using</span></span><br><span class="line"><span class="comment"># more intuitive names for the indexers and embedders.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"token_characters"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding each token.</span></span><br><span class="line">embedding = Embedding(num_embeddings=<span class="number">6</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for encoding the characters in each token.</span></span><br><span class="line">character_embedding = Embedding(num_embeddings=<span class="number">6</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">3</span>, num_filters=<span class="number">4</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用名字来表示不同的namespace</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(</span><br><span class="line">    token_embedders=&#123;<span class="string">"tokens"</span>: embedding, <span class="string">"token_characters"</span>: token_encoder&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"没有 speech tag"</span>)</span><br><span class="line">print(embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer,</span></span><br><span class="line"><span class="comment"># a TokenCharactersIndexer, and another SingleIdTokenIndexer for PoS tags;</span></span><br><span class="line"><span class="comment"># see the code above.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"token_characters"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"pos_tag_tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice below how the 'vocab_namespace' parameter matches the name used above.</span></span><br><span class="line"><span class="comment"># We're showing here how the code works when we're constructing the Embedding from</span></span><br><span class="line"><span class="comment"># a configuration file, where the vocabulary object gets passed in behind the</span></span><br><span class="line"><span class="comment"># scenes (but the vocab_namespace parameter must be set in the config). If you are</span></span><br><span class="line"><span class="comment"># using a `build_model` method (see the quick start chapter) or instantiating the</span></span><br><span class="line"><span class="comment"># Embedding yourself directly, you can just grab the vocab size yourself and pass</span></span><br><span class="line"><span class="comment"># in num_embeddings, as we do above.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding each token.</span></span><br><span class="line"><span class="comment"># 这里用vocab_namespace的名字和对应的vocab匹配</span></span><br><span class="line">embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">3</span>, vocab_namespace=<span class="string">"token_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for encoding the characters in each token.</span></span><br><span class="line">character_embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">4</span>, vocab_namespace=<span class="string">"character_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">4</span>, num_filters=<span class="number">5</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding the part of speech tag of each token.</span></span><br><span class="line">pos_tag_embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">6</span>, vocab_namespace=<span class="string">"pos_tag_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice how these keys match the keys in the token_tensor dictionary above;</span></span><br><span class="line"><span class="comment"># these are the keys that you give to your TokenIndexers when constructing</span></span><br><span class="line"><span class="comment"># your TextFields in the DatasetReader.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(</span><br><span class="line">    token_embedders=&#123;</span><br><span class="line">        <span class="string">"tokens"</span>: embedding,</span><br><span class="line">        <span class="string">"token_characters"</span>: token_encoder,</span><br><span class="line">        <span class="string">"pos_tag_tokens"</span>: pos_tag_embedding,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"有 speech tag"</span>)</span><br><span class="line">print(embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">没有 speech tag</span><br><span class="line">tensor([[[ <span class="number">0.3090</span>,  <span class="number">0.0000</span>,  <span class="number">0.4446</span>,  <span class="number">0.0000</span>, <span class="number">-0.0025</span>,  <span class="number">0.4985</span>,  <span class="number">0.6270</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.3697</span>,  <span class="number">0.7020</span>, <span class="number">-0.6689</span>],</span><br><span class="line">         [ <span class="number">0.2016</span>,  <span class="number">0.0000</span>,  <span class="number">0.7770</span>,  <span class="number">0.0000</span>, <span class="number">-0.4487</span>, <span class="number">-0.6927</span>,  <span class="number">0.1282</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0455</span>,  <span class="number">0.0369</span>,  <span class="number">0.0783</span>]]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;)</span><br><span class="line">有 speech tag</span><br><span class="line">tensor([[[<span class="number">-0.5869</span>,  <span class="number">0.1731</span>,  <span class="number">0.4276</span>, <span class="number">-0.6160</span>,  <span class="number">0.5848</span>,  <span class="number">0.6462</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.2893</span>,  <span class="number">0.0415</span>,  <span class="number">0.0000</span>,  <span class="number">0.1478</span>,  <span class="number">0.1721</span>, <span class="number">-0.2290</span>],</span><br><span class="line">         [<span class="number">-0.2442</span>, <span class="number">-0.0461</span>, <span class="number">-0.3557</span>, <span class="number">-0.1157</span>, <span class="number">-0.0065</span>, <span class="number">-0.1078</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0331</span>, <span class="number">-0.4360</span>,  <span class="number">0.6749</span>],</span><br><span class="line">         [<span class="number">-0.0437</span>,  <span class="number">0.7053</span>, <span class="number">-0.5893</span>, <span class="number">-0.1253</span>, <span class="number">-0.4747</span>,  <span class="number">0.0396</span>,  <span class="number">0.0131</span>,</span><br><span class="line">           <span class="number">0.2123</span>,  <span class="number">0.2087</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.6189</span>,  <span class="number">0.3971</span>, <span class="number">-0.6693</span>],</span><br><span class="line">         [<span class="number">-0.2701</span>, <span class="number">-0.3194</span>,  <span class="number">0.0756</span>,  <span class="number">0.6921</span>,  <span class="number">0.4557</span>,  <span class="number">0.5086</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.1958</span>,  <span class="number">0.4875</span>, <span class="number">-0.6018</span>]]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><p>要注意，有两个地方的key要匹配：</p><ol><li>vocab 的 namespace，在TokenIndexers和TokenEmbedders中需要匹配，即<code>embedding = Embedding(embedding_dim=3, vocab_namespace=&quot;token_vocab&quot;, vocab=vocab)</code></li><li>TextField中用于TokenIndexer词典的key需要与BasicTextFieldEmbedder中用于TokenEmbedder词典的key匹配，就是<code>embedder = BasicTextFieldEmbedder(token_embedders={&quot;token_characters&quot;: token_encoder})</code>里面的token_characters。</li></ol><h3 id="Tokenizer-TokenIndexer-ToeknEmbedders的配合"><a href="#Tokenizer-TokenIndexer-ToeknEmbedders的配合" class="headerlink" title="Tokenizer, TokenIndexer, ToeknEmbedders的配合"></a>Tokenizer, TokenIndexer, ToeknEmbedders的配合</h3><p>你需要配置代码以选择要用作具体Tokenizer，TokenIndexers和TokenEmbedders。需要确保选择适合的组件，否则代码将无法正常工作。</p><p>比如，选择一个CharacterTokenizer和一个TokenCharactersIndexer并没有任何意义，因为Indexer假定您已将其标记为单词。</p><p>并且，TokenIndexer的输出会输入到TokenEmbedder中，通过使用key值。通常会有一对一的关系在 TokenIndexer 和 TokenEmbedder，并且一种 TokenIndexer 可能只对一个 Tokenizer 起作用</p><blockquote><p>Using a word-level tokenizer (such as SpacyTokenizer or WhitespaceTokenizer):</p><ul><li>SingleIdTokenIndexer → Embedding (for things like GloVe or other simple embeddings, including learned POS tag embeddings)</li><li>TokenCharactersIndexer → TokenCharactersEncoder (for things like a character CNN)</li><li>ElmoTokenIndexer → ElmoTokenEmbedder (for ELMo)</li><li>PretrainedTransformerMismatchedIndexer → PretrainedTransformerMismatchedEmbedder (for using a transformer like BERT when you really want to do modeling at the word level, e.g., for a tagging task; more on what this does below)</li></ul><p>Using a character-level tokenizer (such as CharacterTokenizer):</p><ul><li>SingleIdTokenIndexer → Embedding</li></ul><p>Using a wordpiece tokenizer (such as PretrainedTransformerTokenizer):</p><ul><li>PretrainedTransformerIndexer → PretrainedTransformerEmbedder</li><li>SingleIdTokenIndexer → Embedding (if you don’t want contextualized wordpieces for some reason)</li></ul></blockquote><h3 id="用预训练编码"><a href="#用预训练编码" class="headerlink" title="用预训练编码"></a>用预训练编码</h3><p>这里主要讲如何使用类似 ELMo, BERT 的预训练上下文编码，只要选择不同的Tokenizer、Indexer、Embedder.</p><h4 id="获取text-fields"><a href="#获取text-fields" class="headerlink" title="获取text_fields"></a>获取text_fields</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splits text into words (instead of wordpieces or characters).  For ELMo, you can</span></span><br><span class="line"><span class="comment"># just use any word-level tokenizer that you like, though for best results you</span></span><br><span class="line"><span class="comment"># should use the same tokenizer that was used with ELMo, which is an older version</span></span><br><span class="line"><span class="comment"># of spacy.  We're using a whitespace tokenizer here for ease of demonstration</span></span><br><span class="line"><span class="comment"># with binder.</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with an array of characters in a way that ELMo expects.</span></span><br><span class="line">token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Both ELMo and BERT do their own thing with vocabularies, so we don't need to add</span></span><br><span class="line"><span class="comment"># anything, but we do need to construct the vocab object so we can use it below.</span></span><br><span class="line"><span class="comment"># (And if you have any labels in your data that need indexing, you'll still need</span></span><br><span class="line"><span class="comment"># this.)</span></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text ."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"ELMo tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"elmo_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We typically batch things together when making tensors, which requires some</span></span><br><span class="line"><span class="comment"># padding computation.  Don't worry too much about the padding for now.</span></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"ELMo tensors:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any transformer model name that huggingface's transformers library supports will</span></span><br><span class="line"><span class="comment"># work here.  Under the hood, we're grabbing pieces from huggingface for this</span></span><br><span class="line"><span class="comment"># part.</span></span><br><span class="line">transformer_model = <span class="string">"bert-base-cased"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To do modeling with BERT correctly, we can't use just any tokenizer; we need to</span></span><br><span class="line"><span class="comment"># use BERT's tokenizer.</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each wordpiece with an id from BERT's vocabulary.</span></span><br><span class="line">token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Some text with an extraordinarily long identifier."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"BERT tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"BERT tensors:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we'll do an example with paired text, to show the right way to handle [SEP]</span></span><br><span class="line"><span class="comment"># tokens in AllenNLP.  We have built-in ways of handling this for two text pieces.</span></span><br><span class="line"><span class="comment"># If you have more than two text pieces, you'll have to manually add the special</span></span><br><span class="line"><span class="comment"># tokens.  The way we're doing this requires that you use a</span></span><br><span class="line"><span class="comment"># PretrainedTransformerTokenizer, not the abstract Tokenizer class.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Splits text into wordpieces, but without adding special tokens.</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(</span><br><span class="line">    model_name=transformer_model,</span><br><span class="line">    add_special_tokens=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">context_text = <span class="string">"This context is frandibulous."</span></span><br><span class="line">question_text = <span class="string">"What is the context like?"</span></span><br><span class="line">context_tokens = tokenizer.tokenize(context_text)</span><br><span class="line">question_tokens = tokenizer.tokenize(question_text)</span><br><span class="line">print(<span class="string">"Context tokens:"</span>, context_tokens)</span><br><span class="line">print(<span class="string">"Question tokens:"</span>, question_tokens)</span><br><span class="line"></span><br><span class="line">combined_tokens = tokenizer.add_special_tokens(context_tokens, question_tokens)</span><br><span class="line">print(<span class="string">"Combined tokens:"</span>, combined_tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(combined_tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"Combined BERT tensors:"</span>, tensor_dict)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ELMo tokens:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">ELMo tensors:</span> <span class="string">&#123;'elmo_tokens':</span> <span class="string">&#123;'elmo_tokens':</span> <span class="string">tensor([[259,</span>  <span class="number">85</span><span class="string">,</span> <span class="number">105</span><span class="string">,</span> <span class="number">106</span><span class="string">,</span> <span class="number">116</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">106</span><span class="string">,</span> <span class="number">116</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">116</span><span class="string">,</span> <span class="number">112</span><span class="string">,</span> <span class="number">110</span><span class="string">,</span> <span class="number">102</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">117</span><span class="string">,</span> <span class="number">102</span><span class="string">,</span> <span class="number">121</span><span class="string">,</span> <span class="number">117</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span>  <span class="number">47</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="number">570</span><span class="string">/570</span> <span class="string">[00:00&lt;00:00,</span> <span class="string">164kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="string">213k/213k</span> <span class="string">[00:01&lt;00:00,</span> <span class="string">203kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="string">436k/436k</span> <span class="string">[00:01&lt;00:00,</span> <span class="string">300kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="number">29.0</span><span class="string">/29.0</span> <span class="string">[00:00&lt;00:00,</span> <span class="number">10.</span><span class="string">5kB/s]</span></span><br><span class="line"><span class="attr">BERT tokens:</span> <span class="string">[[CLS],</span> <span class="string">Some,</span> <span class="string">text,</span> <span class="string">with,</span> <span class="string">an,</span> <span class="string">extra,</span> <span class="comment">##ord, ##ina, ##rily, long, id, ##ent, ##ifier, ., [SEP]]</span></span><br><span class="line"><span class="attr">BERT tensors:</span> <span class="string">&#123;'bert_tokens':</span> <span class="string">&#123;'token_ids':</span> <span class="string">tensor([</span>  <span class="number">101</span><span class="string">,</span>  <span class="number">1789</span><span class="string">,</span>  <span class="number">3087</span><span class="string">,</span>  <span class="number">1114</span><span class="string">,</span>  <span class="number">1126</span><span class="string">,</span>  <span class="number">3908</span><span class="string">,</span>  <span class="number">6944</span><span class="string">,</span>  <span class="number">2983</span><span class="string">,</span> <span class="number">11486</span><span class="string">,</span>  <span class="number">1263</span><span class="string">,</span></span><br><span class="line">        <span class="number">25021</span><span class="string">,</span>  <span class="number">3452</span><span class="string">,</span> <span class="number">17792</span><span class="string">,</span>   <span class="number">119</span><span class="string">,</span>   <span class="number">102</span><span class="string">]),</span> <span class="attr">'mask':</span> <span class="string">tensor([True,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line">        <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">]),</span> <span class="attr">'type_ids':</span> <span class="string">tensor([0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Context tokens:</span> <span class="string">[This,</span> <span class="string">context,</span> <span class="string">is,</span> <span class="string">f,</span> <span class="comment">##rand, ##ib, ##ulous, .]</span></span><br><span class="line"><span class="attr">Question tokens:</span> <span class="string">[What,</span> <span class="string">is,</span> <span class="string">the,</span> <span class="string">context,</span> <span class="string">like,</span> <span class="string">?]</span></span><br><span class="line"><span class="attr">Combined tokens:</span> <span class="string">[[CLS],</span> <span class="string">This,</span> <span class="string">context,</span> <span class="string">is,</span> <span class="string">f,</span> <span class="comment">##rand, ##ib, ##ulous, ., [SEP], What, is, the, context, like, ?, [SEP]]</span></span><br><span class="line"><span class="attr">Combined BERT tensors:</span> <span class="string">&#123;'bert_tokens':</span> <span class="string">&#123;'token_ids':</span> <span class="string">tensor([</span>  <span class="number">101</span><span class="string">,</span>  <span class="number">1188</span><span class="string">,</span>  <span class="number">5618</span><span class="string">,</span>  <span class="number">1110</span><span class="string">,</span>   <span class="number">175</span><span class="string">,</span> <span class="number">13141</span><span class="string">,</span> <span class="number">13292</span><span class="string">,</span> <span class="number">14762</span><span class="string">,</span>   <span class="number">119</span><span class="string">,</span>   <span class="number">102</span><span class="string">,</span></span><br><span class="line">         <span class="number">1327</span><span class="string">,</span>  <span class="number">1110</span><span class="string">,</span>  <span class="number">1103</span><span class="string">,</span>  <span class="number">5618</span><span class="string">,</span>  <span class="number">1176</span><span class="string">,</span>   <span class="number">136</span><span class="string">,</span>   <span class="number">102</span><span class="string">]),</span> <span class="attr">'mask':</span> <span class="string">tensor([True,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line">        <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">]),</span> <span class="attr">'type_ids':</span> <span class="string">tensor([0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">])&#125;&#125;</span></span><br></pre></td></tr></table></figure><p>注意，<code>tokenizer.add_special_tokens()</code>这个方法，只能用于<code>PretrainedTransformerTokenizer</code>。并且，<code>TokenEmbedder</code>在这种情况下是作用于整个tokens序列的，而不是单个token，是在<code>TextFieldEmbedder</code>里面做self-attention算embedding，但是在代码上没啥影响。</p><h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># It's easiest to get ELMo input by just running the data code.  See the</span></span><br><span class="line"><span class="comment"># exercise above for an explanation of this code.</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line">token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()</span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">text = <span class="string">"This is some text."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"ELMo tokens:"</span>, tokens)</span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"elmo_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"ELMo tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using a tiny, toy version of ELMo to demonstrate this.</span></span><br><span class="line">elmo_options_file = (</span><br><span class="line">    <span class="string">"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/options.json"</span></span><br><span class="line">)</span><br><span class="line">elmo_weight_file = (</span><br><span class="line">    <span class="string">"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/lm_weights.hdf5"</span></span><br><span class="line">)</span><br><span class="line">elmo_embedding = ElmoTokenEmbedder(</span><br><span class="line">    options_file=elmo_options_file, weight_file=elmo_weight_file</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"elmo_tokens"</span>: elmo_embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"ELMo embedded tokens:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Again, it's easier to just run the data code to get the right output.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using the smallest transformer model we can here, so that it runs on</span></span><br><span class="line"><span class="comment"># binder.</span></span><br><span class="line">transformer_model = <span class="string">"google/reformer-crime-and-punishment"</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)</span><br><span class="line">token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)</span><br><span class="line">text = <span class="string">"Some text with an extraordinarily long identifier."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Transformer tokens:"</span>, tokens)</span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"Transformer tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line">embedding = PretrainedTransformerEmbedder(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"bert_tokens"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"Transformer embedded tokens:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ELMo tokens: [This, <span class="keyword">is</span>, some, text.]</span><br><span class="line">ELMo tensors: &#123;<span class="string">'elmo_tokens'</span>: &#123;<span class="string">'elmo_tokens'</span>: tensor([[<span class="number">259</span>,  <span class="number">85</span>, <span class="number">105</span>, <span class="number">106</span>, <span class="number">116</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">106</span>, <span class="number">116</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">116</span>, <span class="number">112</span>, <span class="number">110</span>, <span class="number">102</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">117</span>, <span class="number">102</span>, <span class="number">121</span>, <span class="number">117</span>,  <span class="number">47</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>]])&#125;&#125;</span><br><span class="line">ELMo embedded tokens: tensor([[[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.8512</span>, <span class="number">-0.0000</span>, <span class="number">-0.4313</span>,</span><br><span class="line">          <span class="number">-1.4576</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0271</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>, <span class="number">-0.1257</span>, <span class="number">-0.4585</span>,  <span class="number">0.0000</span>, <span class="number">-1.1158</span>, <span class="number">-0.2194</span>, <span class="number">-1.5000</span>,</span><br><span class="line">          <span class="number">-1.4474</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.2561</span>, <span class="number">-0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.0740</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.8952</span>, <span class="number">-0.8725</span>,  <span class="number">0.3791</span>, <span class="number">-0.5978</span>,</span><br><span class="line">          <span class="number">-0.3816</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.1073</span>, <span class="number">-0.0000</span>,  <span class="number">0.2156</span>,</span><br><span class="line">           <span class="number">0.0582</span>,  <span class="number">0.0000</span>,  <span class="number">0.3820</span>,  <span class="number">0.5719</span>, <span class="number">-0.0000</span>, <span class="number">-0.6818</span>, <span class="number">-0.7399</span>,</span><br><span class="line">          <span class="number">-1.2560</span>, <span class="number">-1.4208</span>,  <span class="number">0.3838</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.7528</span>,  <span class="number">0.1510</span>,</span><br><span class="line">          <span class="number">-1.5196</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.6609</span>],</span><br><span class="line">         [ <span class="number">0.7935</span>,  <span class="number">1.2918</span>, <span class="number">-0.1949</span>, <span class="number">-0.7476</span>, <span class="number">-0.0000</span>, <span class="number">-0.2793</span>, <span class="number">-0.8381</span>,</span><br><span class="line">          <span class="number">-1.2474</span>,  <span class="number">0.1516</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.7108</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.4098</span>, <span class="number">-0.3095</span>,  <span class="number">0.0000</span>, <span class="number">-1.7565</span>,  <span class="number">0.4459</span>, <span class="number">-0.6707</span>,</span><br><span class="line">          <span class="number">-1.4732</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.6564</span>,  <span class="number">0.1276</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.5801</span>,  <span class="number">0.0000</span>,  <span class="number">0.2791</span>],</span><br><span class="line">         [<span class="number">-0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-6.5606</span>,  <span class="number">2.1171</span>, <span class="number">-2.1024</span>,  <span class="number">2.5919</span>, <span class="number">-4.8008</span>,</span><br><span class="line">           <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">1.9269</span>,  <span class="number">0.0000</span>, <span class="number">-0.6810</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">3.7315</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-3.3836</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,</span><br><span class="line">           <span class="number">1.8097</span>, <span class="number">-7.0459</span>,  <span class="number">0.0000</span>,  <span class="number">2.7400</span>, <span class="number">-0.0000</span>, <span class="number">-1.6098</span>,  <span class="number">2.8753</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.4789</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><h3 id="word-level模型同时使用wordpiece的transformer"><a href="#word-level模型同时使用wordpiece的transformer" class="headerlink" title="word-level模型同时使用wordpiece的transformer"></a>word-level模型同时使用wordpiece的transformer</h3><p>在part-of-speech tagging或者named entity recognition中，数据集是在word level的，因此模型的loss、output都应该是word level，但是可能会需要用transformer，这是在wordpiece level的。</p><p>有两种主要的方式：</p><ol><li>在transformer运行之后，在wordpiece-level上做一些pooling</li><li>把label扩散到wordpiece-level上</li></ol><h4 id="pooling-over-Wordpieces"><a href="#pooling-over-Wordpieces" class="headerlink" title="pooling over Wordpieces"></a>pooling over Wordpieces</h4><blockquote><p>The first step is tokenization, and here we tokenize at the word level (typically the tokenization will be already given to you, so you don’t need to run a tokenizer at all).</p><p>In the second step (indexing), we need to further tokenize each word into subword units, getting a list of wordpieces that will be indexed and passed to the transformer in the third step (embedding).</p><p>The embedding step has to run the transformer, then perform pooling to undo the subword tokenization that was done in the indexing subwordtep, so that we end up with one vector per original token.</p></blockquote><p>只需要使用<code>PretrainedTransformerMismatchedIndexer</code>和<code>PretrainedTransformerMismatchedEmbedder</code>就可以使用任何 Hugging Face 的 transformers。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This pattern is typically used in cases where your input data is already</span></span><br><span class="line"><span class="comment"># tokenized, so we're showing that here.</span></span><br><span class="line">text_tokens = [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"frandibulous"</span>, <span class="string">"text"</span>, <span class="string">"."</span>]</span><br><span class="line">tokens = [Token(x) <span class="keyword">for</span> x <span class="keyword">in</span> text_tokens]</span><br><span class="line">print(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using a very small transformer here so that it runs quickly in binder. You</span></span><br><span class="line"><span class="comment"># can change this to any transformer model name supported by Hugging Face.</span></span><br><span class="line">transformer_model = <span class="string">"google/reformer-crime-and-punishment"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents the list of word tokens with a sequences of wordpieces as determined</span></span><br><span class="line"><span class="comment"># by the transformer's tokenizer.  This actually results in a pretty complex data</span></span><br><span class="line"><span class="comment"># type, which you can see by running this.  It's complicated because we need to</span></span><br><span class="line"><span class="comment"># know how to combine the wordpieces back into words after running the</span></span><br><span class="line"><span class="comment"># transformer.</span></span><br><span class="line">indexer = PretrainedTransformerMismatchedIndexer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"transformer"</span>: indexer&#125;)</span><br><span class="line">text_field.index(Vocabulary())</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line"></span><br><span class="line"><span class="comment"># There are two key things to notice in this output.  First, there are two masks:</span></span><br><span class="line"><span class="comment"># `mask` is a word-level mask that gets used in the utility functions described in</span></span><br><span class="line"><span class="comment"># the last section of this chapter.  `wordpiece_mask` gets used by the `Embedder`</span></span><br><span class="line"><span class="comment"># itself.  Second, there is an `offsets` tensor that gives start and end wordpiece</span></span><br><span class="line"><span class="comment"># indices for the original tokens.  In the embedder, we grab these, average all of</span></span><br><span class="line"><span class="comment"># the wordpieces for each token, and return the result.</span></span><br><span class="line">print(<span class="string">"Indexed tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line">embedding = PretrainedTransformerMismatchedEmbedder(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"transformer"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"Embedded tokens size:"</span>, embedded_tokens.size())</span><br><span class="line">print(<span class="string">"Embedded tokens:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><h4 id="扩散标签到wordpiece"><a href="#扩散标签到wordpiece" class="headerlink" title="扩散标签到wordpiece"></a>扩散标签到wordpiece</h4><p>没有直接的函数可以实现这个功能，但是直接自己写个方法，把word的label转换为wordpiece的labels也比较简单。做完之后，就可以用<code>PretrainedTransformerMismatchedIndexer</code>和<code>PretrainedTransformerMismatchedEmbedder</code>来处理word-piece level的数据。</p><p>另一种选择是为非初始单词提供空标签，并为任何带有空标签的单词掩盖损失计算。但是，从建模角度来看，这是有问题的，因为它打破了CRF局部性假设（您不会从CRF转换概率中获得任何用处），并且通常会使建模更加困难。我们不推荐这种方法。</p><h3 id="padding和mask"><a href="#padding和mask" class="headerlink" title="padding和mask"></a>padding和mask</h3><p>由于要batch computation，所以不一样长的序列需要padding，allennlp里面用的是<code>text_field.get_padding_lengths()</code>。<code>collate_function</code>方法会在batch中找到最长的dimension，然后将这个最大值传给<code>text_field.as_tensor()</code>，因此每个相同维度的tensor在被创建之前就会被batched。</p><p>mask的过程在<code>TextFieldEmbedder</code>中，但是也要确保模型代码做了对应的masking computation。</p><p><code>allennlp.nn.util</code>中提供了很多masked版本的pytorch的工具类方法，比如<code>masked_softmax</code>和<code>masked_log_softmax</code>和<code>masked_topk</code></p><h3 id="使用TextField输出的TextFieldTensors"><a href="#使用TextField输出的TextFieldTensors" class="headerlink" title="使用TextField输出的TextFieldTensors"></a>使用TextField输出的TextFieldTensors</h3><p>TextField会返回一个TextFieldTensors对象，是一个复杂的字典结构。</p><p>不要直接编写访问TextfieldTensors对象内部的代码。allennlp中有几个方法可以访问textfieldtensors。TextFieldEmbedder对象会把TextFieldTensors对象转换为每个输入token对应一个embedding。一般会通过mask，并获得token id来把他们转换为字符串。allennlp.nn.util中提供了get_text_field_mask和get_token_ids_from_text_field_tesors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We're following the logic from the "Combining multiple TokenIndexers" example</span></span><br><span class="line"><span class="comment"># above.</span></span><br><span class="line">tokenizer = SpacyTokenizer(pos_tags=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text."</span></span><br><span class="line">text2 = <span class="string">"This is some text with more tokens."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">tokens2 = tokenizer.tokenize(text2)</span><br><span class="line">print(<span class="string">"Tokens:"</span>, tokens)</span><br><span class="line">print(<span class="string">"Tokens 2:"</span>, tokens2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with (1) an id from a vocabulary, (2) a sequence of</span></span><br><span class="line"><span class="comment"># characters, and (3) part of speech tag ids.</span></span><br><span class="line">token_indexers = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">    <span class="string">"pos_tags"</span>: SingleIdTokenIndexer(namespace=<span class="string">"pos_tag_vocab"</span>, feature_name=<span class="string">"tag_"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">text_field2 = TextField(tokens2, token_indexers)</span><br><span class="line">text_field2.index(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using the longer padding lengths here; we'd typically be relying on our</span></span><br><span class="line"><span class="comment"># collate function to figure out what the longest values are to use.</span></span><br><span class="line">padding_lengths = text_field2.get_padding_lengths()</span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">tensor_dict2 = text_field2.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"Combined tensor dictionary:"</span>, tensor_dict)</span><br><span class="line">print(<span class="string">"Combined tensor dictionary 2:"</span>, tensor_dict2)</span><br><span class="line"></span><br><span class="line">text_field_tensors = text_field.batch_tensors([tensor_dict, tensor_dict2])</span><br><span class="line">print(<span class="string">"Batched tensor dictionary:"</span>, text_field_tensors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We've seen plenty of examples of using a TextFieldEmbedder, so we'll just show</span></span><br><span class="line"><span class="comment"># the utility methods here.</span></span><br><span class="line">mask = nn_util.get_text_field_mask(text_field_tensors)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask)</span><br><span class="line">print(<span class="string">"Mask size:"</span>, mask.size())</span><br><span class="line">token_ids = nn_util.get_token_ids_from_text_field_tensors(text_field_tensors)</span><br><span class="line">print(<span class="string">"Token ids:"</span>, token_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can also handle getting masks when you have lists of TextFields, but there's</span></span><br><span class="line"><span class="comment"># an important parameter that you need to pass, which we'll show here.  The</span></span><br><span class="line"><span class="comment"># difference in output that you see between here and above is just that there's an</span></span><br><span class="line"><span class="comment"># extra dimension in this output.  Where shapes used to be (batch_size=2, ...),</span></span><br><span class="line"><span class="comment"># now they are (batch_size=1, list_length=2, ...).</span></span><br><span class="line">list_field = ListField([text_field, text_field2])</span><br><span class="line">tensor_dict = list_field.as_tensor(list_field.get_padding_lengths())</span><br><span class="line">text_field_tensors = list_field.batch_tensors([tensor_dict])</span><br><span class="line">print(<span class="string">"Batched tensors for ListField[TextField]:"</span>, text_field_tensors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The num_wrapping_dims argument tells get_text_field_mask how many nested lists</span></span><br><span class="line"><span class="comment"># there are around the TextField, which we need for our heuristics that guess</span></span><br><span class="line"><span class="comment"># which tensor to use when computing a mask.</span></span><br><span class="line">mask = nn_util.get_text_field_mask(text_field_tensors, num_wrapping_dims=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Tokens:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">Tokens 2:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">with,</span> <span class="string">more,</span> <span class="string">tokens,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">Combined tensor dictionary:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Combined tensor dictionary 2:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">        <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Batched tensor dictionary:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]],</span></span><br><span class="line"></span><br><span class="line">        <span class="string">[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">         <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">tensor([[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">]])</span></span><br><span class="line"><span class="attr">Mask size:</span> <span class="string">torch.Size([2,</span> <span class="number">8</span><span class="string">])</span></span><br><span class="line"><span class="attr">Token ids:</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]])</span></span><br><span class="line"><span class="string">Batched</span> <span class="string">tensors</span> <span class="string">for</span> <span class="string">ListField[TextField]:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]]])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]],</span></span><br><span class="line"></span><br><span class="line">         <span class="string">[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">          <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]]]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">]]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">tensor([[[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">]]])</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">torch.Size([1,</span> <span class="number">2</span><span class="string">,</span> <span class="number">8</span><span class="string">])</span></span><br></pre></td></tr></table></figure><h2 id="未完"><a href="#未完" class="headerlink" title="未完"></a>未完</h2><p>剩下的懒得记了，赶时间，直接看很快地过了一遍guide剩下内容。笔记自己参考下官方文档吧，也不多了。</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/AllenNLP/" rel="tag"><i class="fa fa-tag"></i> AllenNLP</a> <a href="/tags/NLP/" rel="tag"><i class="fa fa-tag"></i> NLP</a> <a href="/tags/OpenLibrary/" rel="tag"><i class="fa fa-tag"></i> OpenLibrary</a> <a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a> <a href="/tags/Pytorch/" rel="tag"><i class="fa fa-tag"></i> Pytorch</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MachineLearning/2021-04-14-allennlp-notes" rel="prev" title="AllenNLP框架学习笔记（一）"><i class="fa fa-chevron-left"></i> AllenNLP框架学习笔记（一）</a></div><div class="post-nav-item"><a href="/MachineLearning/2021-05-09-gnn-graph-clustering" rel="next" title="GNN和图聚类">GNN和图聚类 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2022</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">583k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">24:18</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2";
    this.page.identifier = "MachineLearning/2021-04-26-allennlp-notes-2.html";
    this.page.title = "AllenNLP框架学习笔记（二）";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : 'c2c698d776148cadeedf124cc8bab1fe',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>