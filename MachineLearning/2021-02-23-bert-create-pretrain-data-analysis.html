<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="BERT 作为一个里程碑式的预训练模型，很多时候我们都是直接用训练好的 model 直接 fine-tune，对它的理解只停留在 MLM 和 NSP 上。后续的很多 SOTA 模型都是在 BERT 的基础上发展来，比如 ALBERT、RoBERTa、XLNet 之类。这里对 BERT 创建预训练数据的源码：create_pretraining_data.py和run_pretrain.py进行分析"><meta property="og:type" content="article"><meta property="og:title" content="BERT-预训练源码理解"><meta property="og:url" content="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="BERT 作为一个里程碑式的预训练模型，很多时候我们都是直接用训练好的 model 直接 fine-tune，对它的理解只停留在 MLM 和 NSP 上。后续的很多 SOTA 模型都是在 BERT 的基础上发展来，比如 ALBERT、RoBERTa、XLNet 之类。这里对 BERT 创建预训练数据的源码：create_pretraining_data.py和run_pretrain.py进行分析"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BOBnSQ.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LaGZ7B.png"><meta property="article:published_time" content="2021-02-23T08:11:01.000Z"><meta property="article:modified_time" content="2021-03-29T03:48:40.529Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="BERT"><meta property="article:tag" content="Pretrain"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BOBnSQ.png"><link rel="canonical" href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>BERT-预训练源码理解 | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#原始数据格式"><span class="nav-number">1.</span> <span class="nav-text">原始数据格式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数据生成-tfrecord"><span class="nav-number">2.</span> <span class="nav-text">数据生成 tfrecord</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#生成-tfrecord-命令"><span class="nav-number">2.1.</span> <span class="nav-text">生成 tfrecord 命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数说明"><span class="nav-number">2.2.</span> <span class="nav-text">参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-of-pretraining-data"><span class="nav-number">2.3.</span> <span class="nav-text">main of pretraining data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-training-instances"><span class="nav-number">2.4.</span> <span class="nav-text">create_training_instances</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-instances-from-document"><span class="nav-number">2.5.</span> <span class="nav-text">create_instances_from_document</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-masked-lm-predictions"><span class="nav-number">2.6.</span> <span class="nav-text">create_masked_lm_predictions</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TrainingInstance"><span class="nav-number">2.7.</span> <span class="nav-text">TrainingInstance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#write-instance-to-example-files"><span class="nav-number">2.8.</span> <span class="nav-text">write_instance_to_example_files</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预训练-run-pretrain"><span class="nav-number">3.</span> <span class="nav-text">预训练 run_pretrain</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#预训练命令"><span class="nav-number">3.1.</span> <span class="nav-text">预训练命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#参数说明-1"><span class="nav-number">3.2.</span> <span class="nav-text">参数说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#main-of-pretrain"><span class="nav-number">3.3.</span> <span class="nav-text">main of pretrain</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#input-fn-builder"><span class="nav-number">3.4.</span> <span class="nav-text">input_fn_builder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#model-fn-builder"><span class="nav-number">3.5.</span> <span class="nav-text">model_fn_builder</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#文件说明"><span class="nav-number">3.6.</span> <span class="nav-text">文件说明</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BertModel"><span class="nav-number">3.7.</span> <span class="nav-text">BertModel</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-lookup"><span class="nav-number">3.8.</span> <span class="nav-text">embedding_lookup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#embedding-postprocessor"><span class="nav-number">3.9.</span> <span class="nav-text">embedding_postprocessor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformer-model"><span class="nav-number">3.10.</span> <span class="nav-text">transformer_model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#attention-layer"><span class="nav-number">3.11.</span> <span class="nav-text">attention_layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#get-masked-lm-output"><span class="nav-number">3.12.</span> <span class="nav-text">get_masked_lm_output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#get-next-sentence-output"><span class="nav-number">3.13.</span> <span class="nav-text">get_next_sentence_output</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#create-optimizer"><span class="nav-number">3.14.</span> <span class="nav-text">create_optimizer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimator-类"><span class="nav-number">3.15.</span> <span class="nav-text">Estimator 类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#介绍"><span class="nav-number">3.15.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化"><span class="nav-number">3.15.2.</span> <span class="nav-text">初始化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#train"><span class="nav-number">3.15.3.</span> <span class="nav-text">train</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#evaluate"><span class="nav-number">3.15.4.</span> <span class="nav-text">evaluate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#predict"><span class="nav-number">3.15.5.</span> <span class="nav-text">predict</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#estimator-train-evaluate"><span class="nav-number">3.16.</span> <span class="nav-text">estimator.train&#x2F;evaluate</span></a></li></ol></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">120</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">BERT-预训练源码理解</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-02-23 16:11:01" itemprop="dateCreated datePublished" datetime="2021-02-23T16:11:01+08:00">2021-02-23</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-03-29 11:48:40" itemprop="dateModified" datetime="2021-03-29T11:48:40+08:00">2021-03-29</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="MachineLearning/2021-02-23-bert-create-pretrain-data-analysis.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>73k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>3:02</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>BERT 作为一个里程碑式的预训练模型，很多时候我们都是直接用训练好的 model 直接 fine-tune，对它的理解只停留在 MLM 和 NSP 上。后续的很多 SOTA 模型都是在 BERT 的基础上发展来，比如 ALBERT、RoBERTa、XLNet 之类。</p><p>这里对 BERT 创建预训练数据的源码：<code>create_pretraining_data.py</code>和<code>run_pretrain.py</code>进行分析和理解。</p><p><a href="https://github.com/google-research/bert/tree/eedf5716ce1268e56f0a50264a88cafad334ac61" target="_blank" rel="external nofollow noopener noreferrer">当前 BERT 对应的 commit</a></p><p>部分参考<a href="https://carlos9310.github.io/2019/09/30/pre-trained-bert/" target="_blank" rel="external nofollow noopener noreferrer">预训练模型-BERT预训练源码解读笔记</a></p></div><a id="more"></a><h2 id="原始数据格式"><a href="#原始数据格式" class="headerlink" title="原始数据格式"></a>原始数据格式</h2><ol><li>每行一句话，每个文档中间用空格分开。</li><li>可以输入多个文件，也可以输出多个 tfrecord 文件</li><li>参考样例可以看 bert 中附带的<code>sample_text.txt</code></li></ol><h2 id="数据生成-tfrecord"><a href="#数据生成-tfrecord" class="headerlink" title="数据生成 tfrecord"></a>数据生成 tfrecord</h2><p>主要为<code>create_pretraining_data.py</code>分析。</p><h3 id="生成-tfrecord-命令"><a href="#生成-tfrecord-命令" class="headerlink" title="生成 tfrecord 命令"></a>生成 tfrecord 命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python create_pretraining_data.py \</span><br><span class="line">  --input_file=./sample_text.txt \</span><br><span class="line">  --output_file=/tmp/tf_examples.tfrecord \</span><br><span class="line">  --vocab_file=<span class="variable">$BERT_BASE_DIR</span>/vocab.txt \</span><br><span class="line">  --do_lower_case=True \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --masked_lm_prob=0.15 \</span><br><span class="line">  --random_seed=12345 \</span><br><span class="line">  --dupe_factor=5</span><br></pre></td></tr></table></figure><p>这里并不是所有的参数，所有的参数和说明可以看<code>create_pretraining_data.py</code>中的<a href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><code>flags.DEFINE_string</code></a>。</p><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">flags.DEFINE_string(<span class="string">"input_file"</span>, <span class="literal">None</span>,</span><br><span class="line">                    <span class="string">"Input raw text file (or comma-separated list of files)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"output_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Output TF example file (or comma-separated list of files)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(<span class="string">"vocab_file"</span>, <span class="literal">None</span>,</span><br><span class="line">                    <span class="string">"The vocabulary file that the BERT model was trained on."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">"do_lower_case"</span>, <span class="literal">True</span>,</span><br><span class="line">    <span class="string">"Whether to lower case the input text. Should be True for uncased "</span></span><br><span class="line">    <span class="string">"models and False for cased models."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">"do_whole_word_mask"</span>, <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"Whether to use whole word masking rather than per-WordPiece masking."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_seq_length"</span>, <span class="number">128</span>, <span class="string">"Maximum sequence length."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_predictions_per_seq"</span>, <span class="number">20</span>,</span><br><span class="line">                     <span class="string">"Maximum number of masked LM predictions per sequence."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"random_seed"</span>, <span class="number">12345</span>, <span class="string">"Random seed for data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"dupe_factor"</span>, <span class="number">10</span>,</span><br><span class="line">    <span class="string">"Number of times to duplicate the input data (with different masks)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(<span class="string">"masked_lm_prob"</span>, <span class="number">0.15</span>, <span class="string">"Masked LM probability."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(</span><br><span class="line">    <span class="string">"short_seq_prob"</span>, <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"Probability of creating sequences which are shorter than the "</span></span><br><span class="line">    <span class="string">"maximum length."</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>input_file：是输入文件，就按照上面说的格式。如果有多个文件可以用逗号分开</li><li>output_file：是输出的 tfrecord 文件，多个可以用逗号分开</li><li>vocab_file：是词表，可以直接用 bert 模型里面的 vocab。如果重新训练也可以用自己的词表</li><li>do_lower_case：是表示是否把输入小写</li><li>do_whole_word_mask：表示是否要进行整个单词的 mask，而不是 word piece 的 mask。word piece 会把单词拆分，非单词首部的用##开头</li><li>max_seq_length：表示拼接后的句子对组成的序列中包含 Wordpiece 级别的 token 数的上限，超过部分，需将较长的句子进行首尾截断</li><li>max_predictions_per_seq：表示每个序列中需要预测的 token 的上限</li><li>masked_lm_prob：表示生成的序列中被 masked 的 token 占总 token 数的比例。(这里的 masked 是广义的 mask，即将选中的 token 替换成[mask]或保持原词汇或随机替换成词表中的另一个词)，且有如下关系<code>max_predictions_per_seq = max_seq_length * masked_lm_prob</code></li><li>random_seed：用于复现结果，每次保持一样</li><li>dupe_factor：对输入使用不同的 mask 的次数，会重复创建 TrainingInstance</li><li>masked_lm_prob：mask LM 的概率，一般按照上面的公式确定</li><li>short_seq_prob：会按照这个概率创建比最大长度短的句子</li></ul><h3 id="main-of-pretraining-data"><a href="#main-of-pretraining-data" class="headerlink" title="main of pretraining data"></a>main of pretraining data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  tokenizer = tokenization.FullTokenizer(</span><br><span class="line">      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">","</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"*** Reading from input files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span>, input_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 这个rng会一直用下去，作为参数传递</span></span><br><span class="line">  rng = random.Random(FLAGS.random_seed)</span><br><span class="line">  <span class="comment"># 得到的是一个一维instance列表，每个instance是一行预处理得到</span></span><br><span class="line">  instances = create_training_instances(</span><br><span class="line">      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,</span><br><span class="line">      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,</span><br><span class="line">      rng)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 所以输出文件也是可以为多个，用逗号分割</span></span><br><span class="line">  output_files = FLAGS.output_file.split(<span class="string">","</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"*** Writing to output files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span>, output_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 写入文件</span></span><br><span class="line">  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,</span><br><span class="line">                                  FLAGS.max_predictions_per_seq, output_files)</span><br></pre></td></tr></table></figure><p>可以看到，这个模块的流程大概是：</p><ol><li>创建 tokenizer，使用到了 vocab 和 do_lower_case 这两个参数</li><li>将 input files 整理到数组中</li><li>创建 training instances，见<a href="#create_training_instances">create_training_instances</a></li><li>将生成的每一个 TrainingInstance 对象依此转成 tf.train.Example 对象后</li><li>将上述生成的对象序列化到.tfrecord 格式的文件中。最终生成的.tfrecord 格式的文件是 BERT 预训练时的数据源，见<a href="#write_instance_to_example_files">write_instance_to_example_files</a></li></ol><p>下面先看如何<a href="#create_training_instances">创建 training instances</a></p><h3 id="create-training-instances"><a href="#create-training-instances" class="headerlink" title="create_training_instances"></a>create_training_instances</h3><p>直接在代码上写注释了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_instances</span><span class="params">(input_files, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                              dupe_factor, short_seq_prob, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                              max_predictions_per_seq, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Create `TrainingInstance`s from raw text."""</span></span><br><span class="line">  all_documents = [[]]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input file format:</span></span><br><span class="line">  <span class="comment"># (1) One sentence per line. These should ideally be actual sentences, not</span></span><br><span class="line">  <span class="comment"># entire paragraphs or arbitrary spans of text. (Because we use the</span></span><br><span class="line">  <span class="comment"># sentence boundaries for the "next sentence prediction" task).</span></span><br><span class="line">  <span class="comment"># (2) Blank lines between documents. Document boundaries are needed so</span></span><br><span class="line">  <span class="comment"># that the "next sentence prediction" task doesn't span between documents.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 读取文件并保存在all_documents二维列表中。</span></span><br><span class="line">  <span class="comment"># 每个文件按行读取，每读一行，转换成Unicode，如果没有了就break</span></span><br><span class="line">  <span class="comment"># 如果读到空行，就再all_documents中加入一个list表示下一个文件的开始。</span></span><br><span class="line">  <span class="comment"># 每行使用tokenizer进行tokenize，加到最后一个文档中。</span></span><br><span class="line">  <span class="comment"># 最终形成如下形式的all_documents:</span></span><br><span class="line">  <span class="comment"># [[[d1_s1],[d1_s2],…,[d2_sn]],…,[d2_sm]],…,[[dk_s1],…,[dk_sz]]]。</span></span><br><span class="line">  <span class="comment"># 上述表示一个语料中有k个文档，第一个文档有n句话，第二个文档有m句话，</span></span><br><span class="line">  <span class="comment"># 第k个文档有z句话，d1_s1表示第一个文档中的第一句话被分割成wordpiece级别的list。</span></span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(input_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        line = line.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Empty lines are used as document delimiters</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          all_documents.append([])</span><br><span class="line">        tokens = tokenizer.tokenize(line)</span><br><span class="line">        <span class="keyword">if</span> tokens:</span><br><span class="line">          all_documents[<span class="number">-1</span>].append(tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Remove empty documents</span></span><br><span class="line">  <span class="comment"># 过滤空文档，并且随机打乱文档顺序</span></span><br><span class="line">  all_documents = [x <span class="keyword">for</span> x <span class="keyword">in</span> all_documents <span class="keyword">if</span> x]</span><br><span class="line">  rng.shuffle(all_documents)</span><br><span class="line"></span><br><span class="line">  vocab_words = list(tokenizer.vocab.keys())</span><br><span class="line">  instances = []</span><br><span class="line">  <span class="comment"># 重复dupe_factor次，为每个文档创建创建instances列表</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(dupe_factor):</span><br><span class="line">    <span class="comment"># 对all_documents中的每一个文档生成由TrainingInstance对象组成的instances列表</span></span><br><span class="line">    <span class="comment"># 即create_instances_from_document，并拼接(extend)所有的instances到一个instances中</span></span><br><span class="line">    <span class="keyword">for</span> document_index <span class="keyword">in</span> range(len(all_documents)):</span><br><span class="line">      instances.extend(</span><br><span class="line">          <span class="comment"># 一个文档是一个instances列表，里面每个instance对象是一行预处理的结果</span></span><br><span class="line">          create_instances_from_document(</span><br><span class="line">              all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 随机打乱instances并返回，这时候所有的instance对象都在一个列表里</span></span><br><span class="line">  rng.shuffle(instances)</span><br><span class="line">  <span class="keyword">return</span> instances</span><br></pre></td></tr></table></figure><p>这里引申出一个问题，怎么从文档生成 TrainingInstance 对象组成的 instances 列表</p><p>见<a href="#create_instances_from_document">create_instances_from_document</a></p><p>debug 截图如下所示</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BOBnSQ.png" alt="BOBnSQ"></p><h3 id="create-instances-from-document"><a href="#create-instances-from-document" class="headerlink" title="create_instances_from_document"></a>create_instances_from_document</h3><p>同样是注释的形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_instances_from_document</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    all_documents, document_index, max_seq_length, short_seq_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">    masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates `TrainingInstance`s for a single document."""</span></span><br><span class="line">  <span class="comment"># 当前文档</span></span><br><span class="line">  document = all_documents[document_index]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Account for [CLS], [SEP], [SEP]</span></span><br><span class="line">  max_num_tokens = max_seq_length - <span class="number">3</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We *usually* want to fill up the entire sequence since we are padding</span></span><br><span class="line">  <span class="comment"># to `max_seq_length` anyways, so short sequences are generally wasted</span></span><br><span class="line">  <span class="comment"># computation. However, we *sometimes*</span></span><br><span class="line">  <span class="comment"># (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter</span></span><br><span class="line">  <span class="comment"># sequences to minimize the mismatch between pre-training and fine-tuning.</span></span><br><span class="line">  <span class="comment"># The `target_seq_length` is just a rough target however, whereas</span></span><br><span class="line">  <span class="comment"># `max_seq_length` is a hard limit.</span></span><br><span class="line">  <span class="comment"># 通过short_seq_prob随机生成小于max_num_tokens的短句</span></span><br><span class="line">  target_seq_length = max_num_tokens</span><br><span class="line">  <span class="keyword">if</span> rng.random() &lt; short_seq_prob:</span><br><span class="line">    target_seq_length = rng.randint(<span class="number">2</span>, max_num_tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We DON'T just concatenate all of the tokens from a document into a long</span></span><br><span class="line">  <span class="comment"># sequence and choose an arbitrary split point because this would make the</span></span><br><span class="line">  <span class="comment"># next sentence prediction task too easy. Instead, we split the input into</span></span><br><span class="line">  <span class="comment"># segments "A" and "B" based on the actual "sentences" provided by the user</span></span><br><span class="line">  <span class="comment"># input.</span></span><br><span class="line">  instances = []</span><br><span class="line">  current_chunk = []</span><br><span class="line">  current_length = <span class="number">0</span></span><br><span class="line">  i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> i &lt; len(document):</span><br><span class="line">    <span class="comment"># document是二维的，document[i]就是其中的一句话，被tokenize之后的wordpiece</span></span><br><span class="line">    segment = document[i]</span><br><span class="line">    <span class="comment"># chunk一直保存当前的segment</span></span><br><span class="line">    current_chunk.append(segment)</span><br><span class="line">    current_length += len(segment)</span><br><span class="line">    <span class="comment"># 只有当前chunk保存的token数达到了目标序列长度（不一定是max_num_tokens)，</span></span><br><span class="line">    <span class="comment"># 或者是文档的最后一行了，才进入这个if</span></span><br><span class="line">    <span class="comment"># 否则就直接i+1，继续把下一个segment加入到current_chunk中</span></span><br><span class="line">    <span class="comment"># 所以current_chunk有可能是一句话，也有可能使得多句话</span></span><br><span class="line">    <span class="comment"># 同理，segment A和segment B也是有可能一句，也可能多句</span></span><br><span class="line">    <span class="keyword">if</span> i == len(document) - <span class="number">1</span> <span class="keyword">or</span> current_length &gt;= target_seq_length:</span><br><span class="line">      <span class="comment"># 当前chunk非空就得进，防止chunk被置空之后没有新的segment加进去</span></span><br><span class="line">      <span class="keyword">if</span> current_chunk:</span><br><span class="line">        <span class="comment"># `a_end` is how many segments from `current_chunk` go into the `A`</span></span><br><span class="line">        <span class="comment"># (first) sentence.</span></span><br><span class="line">        <span class="comment"># a_end是用于确定把current_chunk中的多少token分给segment A的</span></span><br><span class="line">        a_end = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> len(current_chunk) &gt;= <span class="number">2</span>:</span><br><span class="line">          a_end = rng.randint(<span class="number">1</span>, len(current_chunk) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tokens_a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(a_end):</span><br><span class="line">          tokens_a.extend(current_chunk[j])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 现在开始确定segment B，有两种方式，一种是random_next，</span></span><br><span class="line">        <span class="comment"># 另一种是依次拼接current_chunk中剩下的token</span></span><br><span class="line">        tokens_b = []</span><br><span class="line">        <span class="comment"># Random next</span></span><br><span class="line">        is_random_next = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果current_chunk长度为1，那么就肯定被分给A，segment B只能random next</span></span><br><span class="line">        <span class="comment"># random next的概率是50%</span></span><br><span class="line">        <span class="comment"># 此时segment B的长度就被确定为：target_seq_length - len(tokens_a)，</span></span><br><span class="line">        <span class="comment"># 因为A和B最后要拼在一起训练</span></span><br><span class="line">        <span class="keyword">if</span> len(current_chunk) == <span class="number">1</span> <span class="keyword">or</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          is_random_next = <span class="literal">True</span></span><br><span class="line">          target_b_length = target_seq_length - len(tokens_a)</span><br><span class="line"></span><br><span class="line">          <span class="comment"># This should rarely go for more than one iteration for large</span></span><br><span class="line">          <span class="comment"># corpora. However, just to be careful, we try to make sure that</span></span><br><span class="line">          <span class="comment"># the random document is not the same as the document</span></span><br><span class="line">          <span class="comment"># we're processing.</span></span><br><span class="line">          <span class="comment"># 确保random next的不是当前文档</span></span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            random_document_index = rng.randint(<span class="number">0</span>, len(all_documents) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> random_document_index != document_index:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">          <span class="comment"># 随机确定segment B在random doc中的开始位置</span></span><br><span class="line">          random_document = all_documents[random_document_index]</span><br><span class="line">          random_start = rng.randint(<span class="number">0</span>, len(random_document) - <span class="number">1</span>)</span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> range(random_start, len(random_document)):</span><br><span class="line">            tokens_b.extend(random_document[j])</span><br><span class="line">            <span class="keyword">if</span> len(tokens_b) &gt;= target_b_length:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line">          <span class="comment"># We didn't actually use these segments so we "put them back" so</span></span><br><span class="line">          <span class="comment"># they don't go to waste.</span></span><br><span class="line">          <span class="comment"># 剩下的没有用上的segment，直接回退回去，不能浪费，i控制的是document[i]</span></span><br><span class="line">          num_unused_segments = len(current_chunk) - a_end</span><br><span class="line">          i -= num_unused_segments</span><br><span class="line">        <span class="comment"># Actual next，segment B直接就是current chunk中剩下的内容</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          is_random_next = <span class="literal">False</span></span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> range(a_end, len(current_chunk)):</span><br><span class="line">            tokens_b.extend(current_chunk[j])</span><br><span class="line">        <span class="comment"># 确保segment A和B拼起来不会超过num_tokens</span></span><br><span class="line">        <span class="comment"># 每次哪个长去掉哪个，并且按照50%的几率从头部去掉or从尾部去掉，直到符合要求</span></span><br><span class="line">        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(tokens_a) &gt;= <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> len(tokens_b) &gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终拼接后的两个segments形成的tokens的基础上做mask操作，</span></span><br><span class="line">        <span class="comment"># 生成masked LM任务需要的tokens形式</span></span><br><span class="line">        tokens = []</span><br><span class="line">        segment_ids = []</span><br><span class="line">        <span class="comment"># token和segment ids数量相同，segment A对应的ids是0，B对应的是1</span></span><br><span class="line">        tokens.append(<span class="string">"[CLS]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_a:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># segment A和B之间用[SEP]分开</span></span><br><span class="line">        tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_b:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后也要加一个[SEP]</span></span><br><span class="line">        tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建Masked LM</span></span><br><span class="line">        (tokens, masked_lm_positions,</span><br><span class="line">         masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一个pair(A, B)创建一个instance</span></span><br><span class="line">        instance = TrainingInstance(</span><br><span class="line">            tokens=tokens,</span><br><span class="line">            segment_ids=segment_ids,</span><br><span class="line">            is_random_next=is_random_next,</span><br><span class="line">            masked_lm_positions=masked_lm_positions,</span><br><span class="line">            masked_lm_labels=masked_lm_labels)</span><br><span class="line">        <span class="comment"># 每一行是一个instance，每个文档是instances列表，最后返回这个列表</span></span><br><span class="line">        instances.append(instance)</span><br><span class="line">      <span class="comment"># 一个current_chunk创建一个instance，用完清空</span></span><br><span class="line">      current_chunk = []</span><br><span class="line">      current_length = <span class="number">0</span></span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> instances</span><br></pre></td></tr></table></figure><p>debug 截图如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LaGZ7B.png" alt="LaGZ7B"></p><p>上面主要是解决了 BERT 中的 NSP 问题，然后又引出了两个问题：</p><ol><li>MLM 问题，就是那个<a href="#create_masked_lm_predictions">create_masked_lm_predictions()</a></li><li>生成 TrainingInstance 问题，即<a href="#traininginstance">TrainingInstance()</a></li></ol><h3 id="create-masked-lm-predictions"><a href="#create-masked-lm-predictions" class="headerlink" title="create_masked_lm_predictions"></a>create_masked_lm_predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span><span class="params">(tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates the predictions for the masked LM objective.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  输入的tokens：['[CLS]', 'ancient', 'sage', '-', 'un', '##im', '##port', '##ant', ...]</span></span><br><span class="line"><span class="string">  输入的rng: &lt;random.Random object at 0x7fbcdc9e8020&gt;</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="comment"># 这个for循环的作用是创建candidate indexes列表，如果要whole word mask，需要进行拼接处理</span></span><br><span class="line">  <span class="comment"># wordpiece 会把一些词拆成多个，单词的第一个word piece没有任何标记，</span></span><br><span class="line">  <span class="comment"># 后续的word piece会在开头加上##标记</span></span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">"[CLS]"</span> <span class="keyword">or</span> token == <span class="string">"[SEP]"</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> len(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">"##"</span>)):</span><br><span class="line">      cand_indexes[<span class="number">-1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 打乱indexs</span></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># output_tokens 是 tokens输入的copy</span></span><br><span class="line">  output_tokens = list(tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># mask掉的token数量不可以超过max_predictions_per_seq</span></span><br><span class="line">  num_to_predict = min(max_predictions_per_seq,</span><br><span class="line">                       max(<span class="number">1</span>, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = set()</span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># 没看懂下面这段什么意思index不应该都是唯一的吗，为什么可能index in covered_indexes == True的情况</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 针对whole word mask，</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">"[MASK]"</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, len(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 为这个token创建MaskedLmInstance</span></span><br><span class="line">      <span class="comment"># 声明：MaskedLmInstance = collections.namedtuple("MaskedLmInstance", ["index", "label"])</span></span><br><span class="line">      <span class="comment"># 类型namedtuple: Returns a new subclass of tuple with named fields.</span></span><br><span class="line">      <span class="comment"># 用于创建position embedding和real label</span></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> len(masked_lms) &lt;= num_to_predict</span><br><span class="line">  <span class="comment"># 因为之前对cand_indexes进行了shuffle，所以返回前要进行sort</span></span><br><span class="line">  masked_lms = sorted(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure><p>最后返回给<code>create_instances_from_document</code>中的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tokens, masked_lm_positions,</span><br><span class="line">  masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">      tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br></pre></td></tr></table></figure><h3 id="TrainingInstance"><a href="#TrainingInstance" class="headerlink" title="TrainingInstance"></a>TrainingInstance</h3><p>在结束 Masked LM 之后就可以来创建 TrainingInstance。</p><p>在<code>create_instances_from_document</code>中调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">instance = TrainingInstance(</span><br><span class="line">    tokens=tokens,</span><br><span class="line">    segment_ids=segment_ids,</span><br><span class="line">    is_random_next=is_random_next,</span><br><span class="line">    masked_lm_positions=masked_lm_positions,</span><br><span class="line">    masked_lm_labels=masked_lm_labels)</span><br><span class="line">instances.append(instance)</span><br></pre></td></tr></table></figure><p>TrainingInstance 类别源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainingInstance</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""A single training instance (sentence pair)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_random_next)</span>:</span></span><br><span class="line">    self.tokens = tokens</span><br><span class="line">    self.segment_ids = segment_ids</span><br><span class="line">    self.is_random_next = is_random_next</span><br><span class="line">    self.masked_lm_positions = masked_lm_positions</span><br><span class="line">    self.masked_lm_labels = masked_lm_labels</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 用于print函数调用的，一般都是return一个什么东西</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">    s = <span class="string">""</span></span><br><span class="line">    s += <span class="string">"tokens: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.tokens]))</span><br><span class="line">    s += <span class="string">"segment_ids: %s\n"</span> % (<span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.segment_ids]))</span><br><span class="line">    s += <span class="string">"is_random_next: %s\n"</span> % self.is_random_next</span><br><span class="line">    s += <span class="string">"masked_lm_positions: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [str(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.masked_lm_positions]))</span><br><span class="line">    s += <span class="string">"masked_lm_labels: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.masked_lm_labels]))</span><br><span class="line">    s += <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">  <span class="comment"># __str__()用于显示给用户，而__repr__()用于显示给开发人员</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.__str__()</span><br></pre></td></tr></table></figure><h3 id="write-instance-to-example-files"><a href="#write-instance-to-example-files" class="headerlink" title="write_instance_to_example_files"></a>write_instance_to_example_files</h3><p>返回到<a href="#main-of-pretraining-data">main 函数</a>中可以看到，还需要把 TrainingInstance 对象写入到输出文件中，即<br><code>write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length, FLAGS.max_predictions_per_seq, output_files)</code>那段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_instance_to_example_files</span><span class="params">(instances, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                                    max_predictions_per_seq, output_files)</span>:</span></span><br><span class="line">  <span class="string">"""Create TF example files from `TrainingInstance`s."""</span></span><br><span class="line">  writers = []</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    writers.append(tf.python_io.TFRecordWriter(output_file))</span><br><span class="line"></span><br><span class="line">  writer_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  total_written = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> (inst_index, instance) <span class="keyword">in</span> enumerate(instances):</span><br><span class="line">    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)</span><br><span class="line">    input_mask = [<span class="number">1</span>] * len(input_ids)</span><br><span class="line">    segment_ids = list(instance.segment_ids)</span><br><span class="line">    <span class="keyword">assert</span> len(input_ids) &lt;= max_seq_length</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保证各个数据的长度都是max_seq_length，不足补0</span></span><br><span class="line">    <span class="keyword">while</span> len(input_ids) &lt; max_seq_length:</span><br><span class="line">      input_ids.append(<span class="number">0</span>)</span><br><span class="line">      input_mask.append(<span class="number">0</span>)</span><br><span class="line">      segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">    <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">    <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line"></span><br><span class="line">    masked_lm_positions = list(instance.masked_lm_positions)</span><br><span class="line">    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)</span><br><span class="line">    <span class="comment"># masked_lm_weights和input_mask 的作用一样，标注masked_lm_ids</span></span><br><span class="line">    <span class="comment"># 和 masked_lm_positions 哪些是真实值，哪些是补全值</span></span><br><span class="line">    masked_lm_weights = [<span class="number">1.0</span>] * len(masked_lm_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> len(masked_lm_positions) &lt; max_predictions_per_seq:</span><br><span class="line">      masked_lm_positions.append(<span class="number">0</span>)</span><br><span class="line">      masked_lm_ids.append(<span class="number">0</span>)</span><br><span class="line">      masked_lm_weights.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    next_sentence_label = <span class="number">1</span> <span class="keyword">if</span> instance.is_random_next <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关于创建feature的函数</span></span><br><span class="line">    <span class="comment"># def create_int_feature(values):</span></span><br><span class="line">    <span class="comment">#   feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))</span></span><br><span class="line">    <span class="comment">#   return feature</span></span><br><span class="line">    features = collections.OrderedDict()</span><br><span class="line">    features[<span class="string">"input_ids"</span>] = create_int_feature(input_ids)</span><br><span class="line">    features[<span class="string">"input_mask"</span>] = create_int_feature(input_mask)</span><br><span class="line">    features[<span class="string">"segment_ids"</span>] = create_int_feature(segment_ids)</span><br><span class="line">    features[<span class="string">"masked_lm_positions"</span>] = create_int_feature(masked_lm_positions)</span><br><span class="line">    features[<span class="string">"masked_lm_ids"</span>] = create_int_feature(masked_lm_ids)</span><br><span class="line">    features[<span class="string">"masked_lm_weights"</span>] = create_float_feature(masked_lm_weights)</span><br><span class="line">    features[<span class="string">"next_sentence_labels"</span>] = create_int_feature([next_sentence_label])</span><br><span class="line"></span><br><span class="line">    tf_example = tf.train.Example(features=tf.train.Features(feature=features))</span><br><span class="line">    <span class="comment"># 保存为tf.train.Example(features字典[key: 特征, value: tf.train.Feature对象])保存</span></span><br><span class="line">    writers[writer_index].write(tf_example.SerializeToString())</span><br><span class="line">    writer_index = (writer_index + <span class="number">1</span>) % len(writers)</span><br><span class="line"></span><br><span class="line">    total_written += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印前20个样本</span></span><br><span class="line">    <span class="keyword">if</span> inst_index &lt; <span class="number">20</span>:</span><br><span class="line">      tf.logging.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">      tf.logging.info(<span class="string">"tokens: %s"</span> % <span class="string">" "</span>.join(</span><br><span class="line">          [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> instance.tokens]))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> feature_name <span class="keyword">in</span> features.keys():</span><br><span class="line">        feature = features[feature_name]</span><br><span class="line">        values = []</span><br><span class="line">        <span class="keyword">if</span> feature.int64_list.value:</span><br><span class="line">          values = feature.int64_list.value</span><br><span class="line">        <span class="keyword">elif</span> feature.float_list.value:</span><br><span class="line">          values = feature.float_list.value</span><br><span class="line">        tf.logging.info(</span><br><span class="line">            <span class="string">"%s: %s"</span> % (feature_name, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> values])))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> writer <span class="keyword">in</span> writers:</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"Wrote %d total instances"</span>, total_written)</span><br></pre></td></tr></table></figure><h2 id="预训练-run-pretrain"><a href="#预训练-run-pretrain" class="headerlink" title="预训练 run_pretrain"></a>预训练 run_pretrain</h2><h3 id="预训练命令"><a href="#预训练命令" class="headerlink" title="预训练命令"></a>预训练命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python run_pretraining.py \</span><br><span class="line">  --input_file=/tmp/tf_examples.tfrecord \</span><br><span class="line">  --output_dir=/tmp/pretraining_output \</span><br><span class="line">  --do_train=True \</span><br><span class="line">  --do_eval=True \</span><br><span class="line">  --bert_config_file=<span class="variable">$BERT_BASE_DIR</span>/bert_config.json \</span><br><span class="line">  --init_checkpoint=<span class="variable">$BERT_BASE_DIR</span>/bert_model.ckpt \</span><br><span class="line">  --train_batch_size=32 \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --num_train_steps=20 \</span><br><span class="line">  --num_warmup_steps=10 \</span><br><span class="line">  --learning_rate=2e-5</span><br></pre></td></tr></table></figure><h3 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Required parameters</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"bert_config_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The config json file corresponding to the pre-trained BERT model. "</span></span><br><span class="line">    <span class="string">"This specifies the model architecture."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"input_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Input TF example files (can be a glob or comma separated)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"output_dir"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The output directory where the model checkpoints will be written."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Other parameters</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"init_checkpoint"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Initial checkpoint (usually from a pre-trained BERT model)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"max_seq_length"</span>, <span class="number">128</span>,</span><br><span class="line">    <span class="string">"The maximum total input sequence length after WordPiece tokenization. "</span></span><br><span class="line">    <span class="string">"Sequences longer than this will be truncated, and sequences shorter "</span></span><br><span class="line">    <span class="string">"than this will be padded. Must match data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"max_predictions_per_seq"</span>, <span class="number">20</span>,</span><br><span class="line">    <span class="string">"Maximum number of masked LM predictions per sequence. "</span></span><br><span class="line">    <span class="string">"Must match data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"do_train"</span>, <span class="literal">False</span>, <span class="string">"Whether to run training."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"do_eval"</span>, <span class="literal">False</span>, <span class="string">"Whether to run eval on the dev set."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"train_batch_size"</span>, <span class="number">32</span>, <span class="string">"Total batch size for training."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"eval_batch_size"</span>, <span class="number">8</span>, <span class="string">"Total batch size for eval."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(<span class="string">"learning_rate"</span>, <span class="number">5e-5</span>, <span class="string">"The initial learning rate for Adam."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"num_train_steps"</span>, <span class="number">100000</span>, <span class="string">"Number of training steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"num_warmup_steps"</span>, <span class="number">10000</span>, <span class="string">"Number of warmup steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"save_checkpoints_steps"</span>, <span class="number">1000</span>,</span><br><span class="line">                     <span class="string">"How often to save the model checkpoint."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"iterations_per_loop"</span>, <span class="number">1000</span>,</span><br><span class="line">                     <span class="string">"How many steps to make in each estimator call."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_eval_steps"</span>, <span class="number">100</span>, <span class="string">"Maximum number of eval steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"use_tpu"</span>, <span class="literal">False</span>, <span class="string">"Whether to use TPU or GPU/CPU."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"tpu_name"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The Cloud TPU to use for training. This should be either the name "</span></span><br><span class="line">    <span class="string">"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 "</span></span><br><span class="line">    <span class="string">"url."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"tpu_zone"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"[Optional] GCE zone where the Cloud TPU is located in. If not "</span></span><br><span class="line">    <span class="string">"specified, we will attempt to automatically detect the GCE project from "</span></span><br><span class="line">    <span class="string">"metadata."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"gcp_project"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"[Optional] Project name for the Cloud TPU-enabled project. If not "</span></span><br><span class="line">    <span class="string">"specified, we will attempt to automatically detect the GCE project from "</span></span><br><span class="line">    <span class="string">"metadata."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(<span class="string">"master"</span>, <span class="literal">None</span>, <span class="string">"[Optional] TensorFlow master URL."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"num_tpu_cores"</span>, <span class="number">8</span>,</span><br><span class="line">    <span class="string">"Only used if `use_tpu` is True. Total number of TPU cores to use."</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>bert_config_file: 预训练模型的配置文件，直接使用对应大小 bert model 里的 config，或者自己调</li><li>input_file: tfrecord 格式的文件</li><li>output_dir: 预训练生成的模型文件路径，会自动创建</li><li>init_checkpoint: 预训练模型的初始检查点，从头开始训练就不需要这个参数，fine-tune 的话就加载 bert 预训练的 ckpt</li><li>max_seq_length: 最大序列长度，超过这个的会被阶段，不足的会补齐。要和数据生成 tfrecord 过程的一致。类似于 RNN 中的最大时间步，每次可动态调整。针对某一特定领域的语料，可在通用的语言模型的基础上，每次通过设置不同长度的专业领域的句子对微调语言模型，使最终生成的预训练的语言模型更适合某一特定领域</li><li>train_batch_size: 训练的 mini batch 大小。如果出现内存不够的问题，那么调小 max_seq_length 或者 batch 大小就可以。</li><li>do_train: 如果不训练只是预测 or 验证，可以设置为 false</li><li>do_eval: 是否进行 eval 验证</li><li>eval_batch_size: 验证的时候的 batch 大小</li><li>learning_rate: Adam 的学习率，有论文表明越小越好，一般是 2e-5 级别</li><li>num_train_steps: 训练的步数，如果是自己从头训练，这个步数要根据语料大小看，一般设置 w 级别。</li><li>num_warmup_steps: warmup 步数，学习率从 0 逐渐增加到初始学习率所需的步数，以后的步数保持固定学习率。参考<a href="https://github.com/google-research/bert/issues/529" target="_blank" rel="external nofollow noopener noreferrer">github-issue</a></li><li>save_checkpoints_steps: 每隔多少步保存一次模型</li><li>iterations_per_loop: 每次调用 estimator 的步数</li><li>max_eval_steps: 最大的 eval 步数</li><li>tup 相关参数看说明</li></ul><h3 id="main-of-pretrain"><a href="#main-of-pretrain" class="headerlink" title="main of pretrain"></a>main of pretrain</h3><p>在源码中写了注释说明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 可以选择不训练的，不改变模型参数</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.do_train <span class="keyword">and</span> <span class="keyword">not</span> FLAGS.do_eval:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"At least one of `do_train` or `do_eval` must be True."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 读取模型参数</span></span><br><span class="line">  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)</span><br><span class="line"></span><br><span class="line">  tf.gfile.MakeDirs(FLAGS.output_dir)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">","</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"*** Input Files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span> % input_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># TPU相关</span></span><br><span class="line">  tpu_cluster_resolver = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">if</span> FLAGS.use_tpu <span class="keyword">and</span> FLAGS.tpu_name:</span><br><span class="line">    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(</span><br><span class="line">        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br><span class="line"></span><br><span class="line">  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2</span><br><span class="line">  run_config = tf.contrib.tpu.RunConfig(</span><br><span class="line">      cluster=tpu_cluster_resolver,</span><br><span class="line">      master=FLAGS.master,</span><br><span class="line">      model_dir=FLAGS.output_dir,</span><br><span class="line">      save_checkpoints_steps=FLAGS.save_checkpoints_steps,</span><br><span class="line">      tpu_config=tf.contrib.tpu.TPUConfig(</span><br><span class="line">          iterations_per_loop=FLAGS.iterations_per_loop,</span><br><span class="line">          num_shards=FLAGS.num_tpu_cores,</span><br><span class="line">          per_host_input_for_training=is_per_host))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 正式创建模型，其实返回的是output_spec = tf.contrib.tpu.TPUEstimatorSpec()</span></span><br><span class="line">  model_fn = model_fn_builder(</span><br><span class="line">      bert_config=bert_config,</span><br><span class="line">      init_checkpoint=FLAGS.init_checkpoint,</span><br><span class="line">      learning_rate=FLAGS.learning_rate,</span><br><span class="line">      num_train_steps=FLAGS.num_train_steps,</span><br><span class="line">      num_warmup_steps=FLAGS.num_warmup_steps,</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      use_one_hot_embeddings=FLAGS.use_tpu)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># If TPU is not available, this will fall back to normal Estimator on CPU</span></span><br><span class="line">  <span class="comment"># or GPU.</span></span><br><span class="line">  estimator = tf.contrib.tpu.TPUEstimator(</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      model_fn=model_fn,</span><br><span class="line">      config=run_config,</span><br><span class="line">      train_batch_size=FLAGS.train_batch_size,</span><br><span class="line">      eval_batch_size=FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">    tf.logging.info(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">    tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.train_batch_size)</span><br><span class="line">    <span class="comment"># 从tfrecord解析出BERT的输入数据</span></span><br><span class="line">    train_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">True</span>)</span><br><span class="line">    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_eval:</span><br><span class="line">    tf.logging.info(<span class="string">"***** Running evaluation *****"</span>)</span><br><span class="line">    tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.eval_batch_size)</span><br><span class="line">    <span class="comment"># 如果不训练只是eval，输入也是一样的tfrecord</span></span><br><span class="line">    eval_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># evaluate操作得到结果</span></span><br><span class="line">    result = estimator.evaluate(</span><br><span class="line">        input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)</span><br><span class="line"></span><br><span class="line">    output_eval_file = os.path.join(FLAGS.output_dir, <span class="string">"eval_results.txt"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(output_eval_file, <span class="string">"w"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">      tf.logging.info(<span class="string">"***** Eval results *****"</span>)</span><br><span class="line">      <span class="keyword">for</span> key <span class="keyword">in</span> sorted(result.keys()):</span><br><span class="line">        tf.logging.info(<span class="string">"  %s = %s"</span>, key, str(result[key]))</span><br><span class="line">        writer.write(<span class="string">"%s = %s\n"</span> % (key, str(result[key])))</span><br></pre></td></tr></table></figure><p>这里引出几个问题：</p><ul><li>模型的创建: <a href="#model_fn_builder">model_fn_builder</a></li><li>数据的解析: <a href="#input_fn_builder">input_fn_builder</a></li><li>模型的训练：estimator.train</li><li>验证集的测试: estimator.evaluate</li></ul><h3 id="input-fn-builder"><a href="#input-fn-builder" class="headerlink" title="input_fn_builder"></a>input_fn_builder</h3><p>从 tfrecord 中解析出 bert 的输入数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn_builder</span><span class="params">(input_files,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_predictions_per_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">                     is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_cpu_threads=<span class="number">4</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Creates an `input_fn` closure to be passed to TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(params)</span>:</span></span><br><span class="line">    <span class="string">"""The actual input function."""</span></span><br><span class="line">    batch_size = params[<span class="string">"batch_size"</span>]</span><br><span class="line"></span><br><span class="line">    name_to_features = &#123;</span><br><span class="line">        <span class="string">"input_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"input_mask"</span>: <span class="comment"># input_mask=0表示是补齐的部分</span></span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"segment_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_positions"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_weights"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),</span><br><span class="line">        <span class="string">"next_sentence_labels"</span>:</span><br><span class="line">            tf.FixedLenFeature([<span class="number">1</span>], tf.int64),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For training, we want a lot of parallel reading and shuffling.</span></span><br><span class="line">    <span class="comment"># For eval, we want no shuffling and parallel reading doesn't matter.</span></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      <span class="comment"># 得到dataset对象</span></span><br><span class="line">      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))</span><br><span class="line">      <span class="comment"># 如果 repeat 转换在 shuffle 转换之前应用，则迭代次数边界将变的不确定。</span></span><br><span class="line">      <span class="comment"># 也就是说，某些元素可以在其他元素出现之前重复一次。</span></span><br><span class="line">      <span class="comment"># 另一方面，如果在 repeat 转换之前应用 shuffle 转换，则在涉及 shuffle</span></span><br><span class="line">      <span class="comment"># 转换的内部状态初始化的每个迭代次数开始时性能可能会下降。</span></span><br><span class="line">      <span class="comment"># 换句话说，前者（在 shuffle 之前 repeat）提供了更好的性能，</span></span><br><span class="line">      <span class="comment"># 而后者（在 repeat 之前 shuffle）提供了更确定性的排序。</span></span><br><span class="line">      d = d.repeat()</span><br><span class="line">      d = d.shuffle(buffer_size=len(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `cycle_length` is the number of parallel files that get read.</span></span><br><span class="line">      cycle_length = min(num_cpu_threads, len(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `sloppy` mode means that the interleaving is not exact. This adds</span></span><br><span class="line">      <span class="comment"># even more randomness to the training pipeline.</span></span><br><span class="line">      <span class="comment"># 这里的说明可以看https://tensorflow.juejin.im/performance/datasets_performance.html</span></span><br><span class="line">      <span class="comment"># sloppy设置为True，转换可能会偏离其确定性顺序，但是这样可以让训练数据更加随机化</span></span><br><span class="line">      d = d.apply(</span><br><span class="line">          tf.contrib.data.parallel_interleave(</span><br><span class="line">              tf.data.TFRecordDataset,</span><br><span class="line">              sloppy=is_training,</span><br><span class="line">              cycle_length=cycle_length))</span><br><span class="line">      d = d.shuffle(buffer_size=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># evaluate模式就无所谓顺序和并行了</span></span><br><span class="line">      d = tf.data.TFRecordDataset(input_files)</span><br><span class="line">      <span class="comment"># Since we evaluate for a fixed number of steps we don't want to encounter</span></span><br><span class="line">      <span class="comment"># out-of-range exceptions.</span></span><br><span class="line">      d = d.repeat()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We must `drop_remainder` on training because the TPU requires fixed</span></span><br><span class="line">    <span class="comment"># size dimensions. For eval, we assume we are evaluating on the CPU or GPU</span></span><br><span class="line">    <span class="comment"># and we *don't* want to drop the remainder, otherwise we wont cover</span></span><br><span class="line">    <span class="comment"># every sample.</span></span><br><span class="line">    d = d.apply(</span><br><span class="line">        <span class="comment"># 这个方法相当于先map，然后batch</span></span><br><span class="line">        tf.contrib.data.map_and_batch(</span><br><span class="line">            <span class="keyword">lambda</span> record: _decode_record(record, name_to_features),</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            num_parallel_batches=num_cpu_threads,</span><br><span class="line">            drop_remainder=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 将解析后的值分成多组batch，作为模型的输入数据(model_fn中的features)。</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> input_fn</span><br></pre></td></tr></table></figure><h3 id="model-fn-builder"><a href="#model-fn-builder" class="headerlink" title="model_fn_builder"></a>model_fn_builder</h3><p>用于构造 Estimator 使用的 model_fn。包含了特征提取、模型创建、计算损失、加载 checkpoint、计算 acc，并返回一个 EstimatorSpec。</p><p>定义好了<code>get_masked_lm_output</code>和<code>get_next_sentence_output</code>两个训练任务后，就可以写出训练过程，之后将训练集传入自动训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>]</span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>]</span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>]</span><br><span class="line">    masked_lm_positions = features[<span class="string">"masked_lm_positions"</span>]</span><br><span class="line">    masked_lm_ids = features[<span class="string">"masked_lm_ids"</span>]</span><br><span class="line">    masked_lm_weights = features[<span class="string">"masked_lm_weights"</span>]</span><br><span class="line">    next_sentence_labels = features[<span class="string">"next_sentence_labels"</span>]</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重点是这里的创建BERTModel，看下一部分的模型代码</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算MaskedLM的损失</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算NSP的损失</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MLM的损失和NSP损失相加</span></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得所有可训练的variable</span></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载checkpoint</span></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印log</span></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练过程，获得spec</span></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      <span class="comment"># 创建优化器optimizer</span></span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 下面是计算损失和acc的函数</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels)</span>:</span></span><br><span class="line">        <span class="string">"""Computes the loss and accuracy of the model."""</span></span><br><span class="line">        <span class="comment"># [batch_size*max_predictions_per_seq=640, vocab_size=30522]</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [<span class="number">-1</span>, masked_lm_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        <span class="comment"># [batch_size*max_predictions_per_seq=640, ]</span></span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># tf.metrics.accuracy返回两个值，accuracy为到上一个batch为止的准确度，</span></span><br><span class="line">        <span class="comment"># update_op为更新本批次后的准确度。</span></span><br><span class="line">        <span class="comment"># masked_lm_weights用于标记哪些是真实值哪些是补全值</span></span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        <span class="comment"># 就是per_example_loss交叉熵损失</span></span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [<span class="number">-1</span>, next_sentence_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [<span class="number">-1</span>])</span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"masked_lm_accuracy"</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">"masked_lm_loss"</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">"next_sentence_accuracy"</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">"next_sentence_loss"</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Only TRAIN and EVAL modes are supported: %s"</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure><p>基于上述搭建好的模型结构及相应的损失函数，在训练阶段，利用相应的优化器(AdamWeightDecayOptimizer)优化损失函数，使其减小，并保存不同训练步数对应的模型参数，直到跑完所有步数，从而确定最终的模型结构与参数。</p><p>从这里引出几个问题：</p><ol><li>Bert 模型的创建，见<a href="#bertmodel">model = modeling.BertModel(···)</a></li><li>计算 MaskedLM 的损失，见<a href="#get_masked_lm_output">(masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output()</a></li><li>计算 NSP 的损失，见<a href="#get_next_sentence_output">(next_sentence_loss, next_sentence_example_loss, next_sentence_log_probs) = get_next_sentence_output()</a></li><li>创建优化器，用来更新模型(权重)参数，见<a href="#create_optimizer">create_optimizer()</a></li></ol><h3 id="文件说明"><a href="#文件说明" class="headerlink" title="文件说明"></a>文件说明</h3><p>由于 BERT 在预训练中使用了 estimator 这种高级 API 形式，在训练完成后会自动生成 ckpt 格式的模型文件(结构和数据是分开的) 及可供 tensorboard 查看的事件文件。具体文件说明如下：</p><ol><li><code>checkpoint</code>: 记录了模型文件的路径信息列表，可以用来迅速查找最近一次的 ckpt 文件。(每个 ckpt 文件对应一个模型)其内容如下所示<ul><li>model_checkpoint_path: “model.ckpt-20”</li><li>all_model_checkpoint_paths: “model.ckpt-0”</li><li>all_model_checkpoint_paths: “model.ckpt-20”</li></ul></li><li><code>events.out.tfevents.1570029823.04c93f97d224</code>：事件文件，tensorboard 可加载显示</li><li><code>graph.pbtxt</code> : 以 Protobuffer 格式描述的模型结构文件(text 格式的图文件(.pbtext),二进制格式的图文件为(.pb))，记录了模型中所有的节点信息，内容大致如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">node &#123;</span><br><span class="line">  name: <span class="string">"global_step/Initializer/zeros"</span></span><br><span class="line">  op: <span class="string">"Const"</span></span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"_class"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: <span class="string">"loc:@global_step"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"_output_shapes"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"dtype"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT64</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"value"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      tensor &#123;</span><br><span class="line">        dtype: DT_INT64</span><br><span class="line">        tensor_shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        int64_val: <span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li><code>model.ckpt-20.data-00000-of-00001</code> : 模型文件中的数据(the values of all variables)部分 (二进制文件)</li><li><code>model.ckpt-20.index</code> : 模型文件中的映射表( Each key is a name of a tensor and its value is a serialized BundleEntryProto. Each BundleEntryProto describes the metadata of a tensor: which of the “data” files contains the content of a tensor, the offset into that file, checksum, some auxiliary data, etc.)部分 (二进制文件)</li><li><code>model.ckpt-20.meta</code> : 模型文件中的(图)结构(由 GraphDef, SaverDef, MateInfoDef,SignatureDef,CollectionDef 等组成的 MetaGraphDef)部分 (二进制文件，内容和 graph.pbtxt 基本一样，其是一个序列化的 MetaGraphDef protocol buffer)</li></ol><p>在评估阶段，直接加载训练好的模型结构与参数，对预测样本进行预测即可。</p><h3 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  # python</span></span><br><span class="line"><span class="string">  # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">  pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">  logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get_shape_list</span></span><br><span class="line">    <span class="comment"># Returns a list of the shape of tensor, preferring static dimensions.</span></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        <span class="comment"># embedding_output 是input_ids对应的embedding输出</span></span><br><span class="line">        <span class="comment"># embedding_table就是整个的embedding table</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        <span class="comment"># 上面得到的embedding输出，先加上token_type_embedding，</span></span><br><span class="line">        <span class="comment"># 然后加上position_embedding。shape相同，对应位置相加</span></span><br><span class="line">        <span class="comment"># token_type_embedding和position_embedding都是通过构建一个look up table得到</span></span><br><span class="line">        <span class="comment"># 最后进行layer_norm_and_dropout</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores.</span></span><br><span class="line">        <span class="comment"># input_ids, input_mask: (32, 128)</span></span><br><span class="line">        <span class="comment"># attention_mask: (32, 128, 128)</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">        <span class="comment"># encoder部分，由num_hidden_layers(12)个transformer encoder组成的</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="comment"># 两种输出，一种是最后一层transformer encoder的sequence_output，</span></span><br><span class="line">      <span class="comment"># token级别的embedding，用于masked LM任务训练</span></span><br><span class="line">      <span class="comment"># 另一种输出是取sequence_output的第一个token，然后接一个带有tanh的全连接层最为输出，</span></span><br><span class="line">      <span class="comment"># 句子级别的embedding，用于NSP任务的训练</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_pooled_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.pooled_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_sequence_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets final hidden layer of encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the final hidden of the transformer encoder.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.sequence_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_all_encoder_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets output of the embedding lookup (i.e., input to the transformer).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the output of the embedding layer, after summing the word</span></span><br><span class="line"><span class="string">      embeddings with the positional embeddings and the token type embeddings,</span></span><br><span class="line"><span class="string">      then performing layer normalization. This is the input to the transformer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_table</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_table</span><br></pre></td></tr></table></figure><p>这里引出几个部分：</p><ol><li>embedding 表的构建，即<a href="#embedding_lookup">embedding_lookup</a></li><li>在 word_embeddings 的基础上增加 segment_id 和 position 信息，最后将叠加后 embedding 分别进行 layer_norm，batch_norm 和 dropout 操作。见<a href="#embedding_postprocessor">embedding_postprocessor</a></li><li>transformer 模型的构建，即<a href="#transformer_model">transformer_model</a></li><li>attention 自注意力层的构建，即<a href="#attention_layer">attention_layer</a></li></ol><h3 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h3><p>构建一个 embedding lookup 表，用于生成每个 token 的表示，同时返回 input_ids 对应的 embedding。</p><p>这里的 embedding 只包括 word_embedding，token embedding 和 position embedding 在 embedding_postprocessor 中处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">      ids.</span></span><br><span class="line"><span class="string">    vocab_size: int. Size of the embedding vocabulary.</span></span><br><span class="line"><span class="string">    embedding_size: int. Width of the word embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Embedding initialization range.</span></span><br><span class="line"><span class="string">    word_embedding_name: string. Name of the embedding table.</span></span><br><span class="line"><span class="string">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span></span><br><span class="line"><span class="string">      embeddings. If False, use `tf.gather()`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># flat_input_ids shape: (4096, )</span></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># input_shape = [32, 128, 1]</span></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># output shape: [4096=32*128, 512] -&gt; [batch, seq_len, embedding_size=512]</span></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure><h3 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h3><p>在 word_embeddings 的基础上增加 segment_id 和 position 信息，最后将叠加后 embedding 分别进行 layer_norm(对每个样本的不同维度进行归一化操作)，batch_norm(是对不同样本的同一特征进行归一化操作)和 dropout(一个张量中某几个位置的值变成 0)操作。</p><p>token_type_table 与 full_position_embeddings 为模型待学习参数。它们和 word_embedding 是对应位置相加，不改变 shape</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># This vocab will be small so we always do one-hot here, since it is always</span></span><br><span class="line">    <span class="comment"># faster for a small vocabulary.</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h3 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      seq_length], with 1 for positions that can be attended to and 0 in</span></span><br><span class="line"><span class="string">      positions that should not be.</span></span><br><span class="line"><span class="string">    hidden_size: int. Hidden size of the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads in the Transformer.</span></span><br><span class="line"><span class="string">    intermediate_size: int. The size of the "intermediate" (a.k.a., feed</span></span><br><span class="line"><span class="string">      forward) layer.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: function. The non-linear activation function to apply</span></span><br><span class="line"><span class="string">      to the output of the intermediate/feed-forward layer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. Dropout probability of the attention</span></span><br><span class="line"><span class="string">      probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the initializer (stddev of truncated</span></span><br><span class="line"><span class="string">      normal).</span></span><br><span class="line"><span class="string">    do_return_all_layers: Whether to also return all layers or just the final</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># hidden size需要是注意力头个数的倍数</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          <span class="comment"># 这里引入attention_layer，输入是上一次的输出，输出是context Z矩阵</span></span><br><span class="line">          <span class="comment"># attention_head: [B*F, N*H] if do_return_2d_tensor=True</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 调试过程中确定每次len(attention_heads)都是1</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          <span class="comment"># 把所有的Z拼接在一起，然后使用Wo线性变换</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="comment"># 上面是self-attention部分，下面是和layer_input进行残差连接</span></span><br><span class="line">        <span class="comment"># 使用线性变换到hidden_size维，然后dropout，</span></span><br><span class="line">        <span class="comment"># 然后加上layer_input之后layer_norm（对一个样本的特征进行归一化）</span></span><br><span class="line">        <span class="comment"># [B*F, N*H] -&gt; [B*F, hidden_size]</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="comment"># 再进行一个带激活函数的线性变换，激活函数只用在这，用的是smooth版的relu：gelu</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="comment"># 再次有一个layer_output和attention_output的残差连接</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># bert的配置是True，返回所有层</span></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      <span class="comment"># [4096, 512] -&gt; [32, 128, 512]</span></span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure><h3 id="attention-layer"><a href="#attention-layer" class="headerlink" title="attention_layer"></a>attention_layer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is an implementation of multi-headed attention based on "Attention</span></span><br><span class="line"><span class="string">  is all you Need". If `from_tensor` and `to_tensor` are the same, then</span></span><br><span class="line"><span class="string">  this is self-attention. Each timestep in `from_tensor` attends to the</span></span><br><span class="line"><span class="string">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This function first projects `from_tensor` into a "query" tensor and</span></span><br><span class="line"><span class="string">  `to_tensor` into "key" and "value" tensors. These are (effectively) a list</span></span><br><span class="line"><span class="string">  of tensors of length `num_attention_heads`, where each tensor is of shape</span></span><br><span class="line"><span class="string">  [batch_size, seq_length, size_per_head].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Then, the query and key tensors are dot-producted and scaled. These are</span></span><br><span class="line"><span class="string">  softmaxed to obtain attention probabilities. The value tensors are then</span></span><br><span class="line"><span class="string">  interpolated by these probabilities, then concatenated back to a single</span></span><br><span class="line"><span class="string">  tensor and returned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In practice, the multi-headed attention are done with transposes and</span></span><br><span class="line"><span class="string">  reshapes rather than actual separate tensors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    size_per_head: int. Size of each attention head. 传入的是 int(hidden_size / num_attention_heads)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query_act: (optional) Activation function for the query transform.</span></span><br><span class="line"><span class="string">    key_act: (optional) Activation function for the key transform.</span></span><br><span class="line"><span class="string">    value_act: (optional) Activation function for the value transform.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span></span><br><span class="line"><span class="string">      attention probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span></span><br><span class="line"><span class="string">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span></span><br><span class="line"><span class="string">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span></span><br><span class="line"><span class="string">      * size_per_head].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor` and `to_tensor`.</span></span><br><span class="line"><span class="string">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor`.</span></span><br><span class="line"><span class="string">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line">  <span class="comment">#   N * H 就相当于hidden size</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># from_tensor or to tensor: rank &gt;=2 -&gt; [batch * seq_len, width]</span></span><br><span class="line">  <span class="comment"># 相当于batch * seq_len的二维按行顺序排列成了一维</span></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 下面是计算Q、K和V，实现不是原本的三维矩阵乘，而是压缩成了二维</span></span><br><span class="line">  <span class="comment"># 得到的结果相当于是每个attention-head的Q按照列拼接在一起，所以输出的unit</span></span><br><span class="line">  <span class="comment"># 是num_attention_heads*size_per_head，每个输出节点都有一组权重，相当于W^Q的一列</span></span><br><span class="line">  <span class="comment"># 返回的是一个tensor</span></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, width]-&gt;[B*F, N*H]</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      <span class="comment"># kernel_initializer用于初始化权重w</span></span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># 先reshape到[B, F, N, H]，然后transpose到[B, N, F, H]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line">  <span class="comment"># transpose_b=True表示b=key_layer在乘法之前进行转置，其实是最后两个维度进行转置</span></span><br><span class="line">  <span class="comment"># tf.matmul()在高维矩阵乘法中，其实是对高维矩阵的每个二维矩阵相乘</span></span><br><span class="line">  <span class="comment"># 所以转制后变成[B, N, F, H] * [B, N, H, T] =&gt; [B, N, F, T]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># size_per_head就是score除以的\sqrt(d_k)</span></span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    <span class="comment"># 如果mask显示为1，那么不改变score，否则会变得很小。这个mask是attention的可视域</span></span><br><span class="line">    <span class="comment"># 这个mask追溯上去得到的是全1的矩阵，对padding部分进行mask</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  <span class="comment"># base bert_config中attention_probs_dropout_prob=0.1，所以实际上dropout了0.9的token</span></span><br><span class="line">  <span class="comment"># dropout函数输出的是output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  <span class="comment"># context其实就是图解中的Z值</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 返回的是整个的Z矩阵</span></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure><h3 id="get-masked-lm-output"><a href="#get-masked-lm-output" class="headerlink" title="get_masked_lm_output"></a>get_masked_lm_output</h3><p>从 BertModel 部分返回到<a href="#model_fn_builder">model_fn_builder</a>。</p><p>搞定了<code>modeling.BertModel</code>，下面开始计算 Masked LM 和 NSP 任务。</p><p>两个任务的本质都是分类任务，一个是二分类，即两个 segment 是否是连贯的；一个是多分类，即输入序列中被 mask 的 token 为词表中某个 token 的概率。它们的损失函数都是<strong>交叉熵损失</strong>。</p><p>NSP 问题中 0 是连续的，1 是随机的。</p><p>在<code>model_fn_builder</code>中是通过下面代码进行调用的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(masked_lm_loss,</span><br><span class="line">  masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">      bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">      masked_lm_positions, masked_lm_ids, masked_lm_weights</span><br></pre></td></tr></table></figure><p>源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span><span class="params">(bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the masked LM."""</span></span><br><span class="line">  <span class="comment"># input_tensor是model.get_sequence_output()得到，是encoder layer的最后一层，shape=(32, 128, 512)</span></span><br><span class="line">  <span class="comment"># output_weights是emebdding table</span></span><br><span class="line">  <span class="comment"># position是masked_lm_positions，ids和weights都是masked_</span></span><br><span class="line">  <span class="comment"># gather_indexes是从input_tensor中取出positions位置的tensor</span></span><br><span class="line">  <span class="comment"># 此时input_tensor.shape = （4096, 512)-&gt;(640, 512)，即</span></span><br><span class="line">  <span class="comment"># [batch_size*max_predictions_per_seq=20, hidden_size]</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/predictions"</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="comment"># 对input做了一个fc+ac+norm的操作</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"transform"</span>):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment"># output_bias: [30522]</span></span><br><span class="line">    <span class="comment"># input_tensor: [640, 512]</span></span><br><span class="line">    <span class="comment"># output_weights: [30522, 512]</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># transpose_b表示转置一下</span></span><br><span class="line">    <span class="comment"># logits: [batch_size*max_predictions_per_seq=640, vocab_size=30522]</span></span><br><span class="line">    <span class="comment"># log_probs: [batch_size*max_predictions_per_seq, vocab_size]</span></span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># label_ids: [640, ]</span></span><br><span class="line">    <span class="comment"># label_weights: [640, ]</span></span><br><span class="line">    label_ids = tf.reshape(label_ids, [<span class="number">-1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_predictions_per_seq, vocab_size]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line">    <span class="comment"># [640, 30522]*[640, 30522]</span></span><br><span class="line">    <span class="comment"># reduce_sum: 调用reduce_sum(arg1, arg2)时，arg1为要求和的数据，arg2为0或1，</span></span><br><span class="line">    <span class="comment"># 通常用reduction_indices=[0]或reduction_indices=[1]来传递。</span></span><br><span class="line">    <span class="comment"># arg2 = 0时，是纵向求和，当arg2 = 1时，是横向求和；省略arg2参数时，默认对所有元素进行求和。</span></span><br><span class="line">    <span class="comment"># reduce就是“对矩阵降维”的含义，在reduce_sum()中就是按照求和的方式对矩阵降维。</span></span><br><span class="line">    <span class="comment"># 那么其他reduce前缀的函数也举一反三了，比如reduce_mean()就是按照某个维度求平均值，等等。</span></span><br><span class="line">    <span class="comment"># per_example_loss-&gt;reduce_sum-&gt;[640, ]，每个位置预测对的log概率*(-1)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># label_weights即masked_lm_weights</span></span><br><span class="line">    <span class="comment"># 和input_mask的作用一样，标注masked_lm_ids 和 masked_lm_positions 哪些是真实值，哪些是补全值</span></span><br><span class="line">    <span class="comment"># label_weights: [640, ]</span></span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    <span class="comment"># 计算非0的真实位置个数，+1e-5防止分子为0，</span></span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_qiumple_loss, log_probs)</span><br></pre></td></tr></table></figure><h3 id="get-next-sentence-output"><a href="#get-next-sentence-output" class="headerlink" title="get_next_sentence_output"></a>get_next_sentence_output</h3><p><code>model_fn_builder</code>中计算完了<code>get_masked_lm_output</code>之后，计算<code>get_next_sentence_output</code>。</p><p>调用方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">  next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">      bert_config, model.get_pooled_output(), next_sentence_labels)</span><br></pre></td></tr></table></figure><p>源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span><span class="params">(bert_config, input_tensor, labels)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the next sentence prediction."""</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is "next sentence" and 1 is</span></span><br><span class="line">  <span class="comment"># "random sentence". This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="comment"># input_tensor: [batch_size, hidden_size]</span></span><br><span class="line">  <span class="comment"># labels: [batch_size, 1]</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/seq_relationship"</span>):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">"output_weights"</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logits: [batch_size, 2]，相当于全连接层+softmax分类</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># one_hot_labels: [batch_size, 2]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 交叉熵损失：-\frac&#123;1&#125;&#123;N&#125; \sum_i (y_i log(p_i) + (1-y_i) log(1-p_i))</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure><h3 id="create-optimizer"><a href="#create-optimizer" class="headerlink" title="create_optimizer"></a>create_optimizer</h3><p>调用部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">  <span class="comment"># 创建优化器optimizer</span></span><br><span class="line">  train_op = optimization.create_optimizer(</span><br><span class="line">      total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br></pre></td></tr></table></figure><p>实现部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_optimizer</span><span class="params">(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu)</span>:</span></span><br><span class="line">  <span class="string">"""Creates an optimizer training op."""</span></span><br><span class="line">  global_step = tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear decay of the learning rate.</span></span><br><span class="line">  learning_rate = tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate=<span class="number">0.0</span>,</span><br><span class="line">      power=<span class="number">1.0</span>,</span><br><span class="line">      cycle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span></span><br><span class="line">  <span class="comment"># learning rate will be `global_step/num_warmup_steps * init_lr`.</span></span><br><span class="line">  <span class="keyword">if</span> num_warmup_steps:</span><br><span class="line">    global_steps_int = tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float = tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done = global_steps_float / warmup_steps_float</span><br><span class="line">    warmup_learning_rate = init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    <span class="comment"># 小trick，is_warmup为1时，算后面的</span></span><br><span class="line">    learning_rate = (</span><br><span class="line">        (<span class="number">1.0</span> - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># It is recommended that you use this optimizer for fine tuning, since this</span></span><br><span class="line">  <span class="comment"># is how the model was trained (note that the Adam m/v variables are NOT</span></span><br><span class="line">  <span class="comment"># loaded from init_checkpoint.)</span></span><br><span class="line">  optimizer = AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate=learning_rate,</span><br><span class="line">      weight_decay_rate=<span class="number">0.01</span>,</span><br><span class="line">      beta_1=<span class="number">0.9</span>,</span><br><span class="line">      beta_2=<span class="number">0.999</span>,</span><br><span class="line">      epsilon=<span class="number">1e-6</span>,</span><br><span class="line">      exclude_from_weight_decay=[<span class="string">"LayerNorm"</span>, <span class="string">"layer_norm"</span>, <span class="string">"bias"</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_tpu:</span><br><span class="line">    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line">  tvars = tf.trainable_variables()</span><br><span class="line">  grads = tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is how the model was pre-trained.</span></span><br><span class="line">  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">  train_op = optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step=global_step)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normally the global step update is done inside of `apply_gradients`.</span></span><br><span class="line">  <span class="comment"># However, `AdamWeightDecayOptimizer` doesn't do this. But if you use</span></span><br><span class="line">  <span class="comment"># a different optimizer, you should probably take this line out.</span></span><br><span class="line">  new_global_step = global_step + <span class="number">1</span></span><br><span class="line">  train_op = tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  <span class="keyword">return</span> train_op</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdamWeightDecayOptimizer</span><span class="params">(tf.train.Optimizer)</span>:</span></span><br><span class="line">  <span class="string">"""A basic Adam optimizer that includes "correct" L2 weight decay."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">               weight_decay_rate=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               beta_1=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               beta_2=<span class="number">0.999</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               epsilon=<span class="number">1e-6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               exclude_from_weight_decay=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               name=<span class="string">"AdamWeightDecayOptimizer"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a AdamWeightDecayOptimizer."""</span></span><br><span class="line">    super(AdamWeightDecayOptimizer, self).__init__(<span class="literal">False</span>, name)</span><br><span class="line"></span><br><span class="line">    self.learning_rate = learning_rate</span><br><span class="line">    self.weight_decay_rate = weight_decay_rate</span><br><span class="line">    self.beta_1 = beta_1</span><br><span class="line">    self.beta_2 = beta_2</span><br><span class="line">    self.epsilon = epsilon</span><br><span class="line">    self.exclude_from_weight_decay = exclude_from_weight_decay</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply_gradients</span><span class="params">(self, grads_and_vars, global_step=None, name=None)</span>:</span></span><br><span class="line">    <span class="string">"""See base class."""</span></span><br><span class="line">    assignments = []</span><br><span class="line">    <span class="keyword">for</span> (grad, param) <span class="keyword">in</span> grads_and_vars:</span><br><span class="line">      <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> param <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      param_name = self._get_variable_name(param.name)</span><br><span class="line"></span><br><span class="line">      m = tf.get_variable(</span><br><span class="line">          name=param_name + <span class="string">"/adam_m"</span>,</span><br><span class="line">          shape=param.shape.as_list(),</span><br><span class="line">          dtype=tf.float32,</span><br><span class="line">          trainable=<span class="literal">False</span>,</span><br><span class="line">          initializer=tf.zeros_initializer())</span><br><span class="line">      v = tf.get_variable(</span><br><span class="line">          name=param_name + <span class="string">"/adam_v"</span>,</span><br><span class="line">          shape=param.shape.as_list(),</span><br><span class="line">          dtype=tf.float32,</span><br><span class="line">          trainable=<span class="literal">False</span>,</span><br><span class="line">          initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Standard Adam update.</span></span><br><span class="line">      next_m = (</span><br><span class="line">          tf.multiply(self.beta_1, m) + tf.multiply(<span class="number">1.0</span> - self.beta_1, grad))</span><br><span class="line">      next_v = (</span><br><span class="line">          tf.multiply(self.beta_2, v) + tf.multiply(<span class="number">1.0</span> - self.beta_2,</span><br><span class="line">                                                    tf.square(grad)))</span><br><span class="line"></span><br><span class="line">      update = next_m / (tf.sqrt(next_v) + self.epsilon)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Just adding the square of the weights to the loss function is *not*</span></span><br><span class="line">      <span class="comment"># the correct way of using L2 regularization/weight decay with Adam,</span></span><br><span class="line">      <span class="comment"># since that will interact with the m and v parameters in strange ways.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># Instead we want ot decay the weights in a manner that doesn't interact</span></span><br><span class="line">      <span class="comment"># with the m/v parameters. This is equivalent to adding the square</span></span><br><span class="line">      <span class="comment"># of the weights to the loss with plain (non-momentum) SGD.</span></span><br><span class="line">      <span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">        update += self.weight_decay_rate * param</span><br><span class="line"></span><br><span class="line">      update_with_lr = self.learning_rate * update</span><br><span class="line"></span><br><span class="line">      next_param = param - update_with_lr</span><br><span class="line"></span><br><span class="line">      assignments.extend(</span><br><span class="line">          [param.assign(next_param),</span><br><span class="line">           m.assign(next_m),</span><br><span class="line">           v.assign(next_v)])</span><br><span class="line">    <span class="keyword">return</span> tf.group(*assignments, name=name)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_do_use_weight_decay</span><span class="params">(self, param_name)</span>:</span></span><br><span class="line">    <span class="string">"""Whether to use L2 weight decay for `param_name`."""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.weight_decay_rate:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> self.exclude_from_weight_decay:</span><br><span class="line">      <span class="keyword">for</span> r <span class="keyword">in</span> self.exclude_from_weight_decay:</span><br><span class="line">        <span class="keyword">if</span> re.search(r, param_name) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_variable_name</span><span class="params">(self, param_name)</span>:</span></span><br><span class="line">    <span class="string">"""Get the variable name from the tensor name."""</span></span><br><span class="line">    m = re.match(<span class="string">"^(.*):\\d+$"</span>, param_name)</span><br><span class="line">    <span class="keyword">if</span> m <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      param_name = m.group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> param_name</span><br></pre></td></tr></table></figure><p>首先是学习率部分，将学习率设置为线性衰减的形式，接着根据global_step是否达到num_warmup_steps，在原来线性衰减的基础上将学习率进一步分成warmup_learning_rate和learning_rate两种方式。然后是优化器的构建。</p><p>先是实例化AdamWeightDecayOptimizer(其是梯度下降法的一种变种，也由待更新参数、学习率和参数更新方向三大要素组成)，接着通过tvars = tf.trainable_variables()解析出模型中所有待训练的参数变量，并给出loss关于所有参数变量的梯度表示grads = tf.gradients(loss, tvars)，同时限制梯度的大小。最后基于上述描述的梯度与变量，进行参数更新操作。更新时，依此遍历每一个待更新的参数，根据标准的Adam更新公式(参考Adam和学习率衰减（learning rate decay）)，先确定参数更新方向，接着在方向的基础上增加衰减参数(这个操作叫纠正的L2 weight decay)，然后在纠正后的方向上移动一定距离(learning_rate * update)后，更新现有的参数。 以上更新步骤随着训练步数不断进行，直到走完所有训练步数。</p><h3 id="Estimator-类"><a href="#Estimator-类" class="headerlink" title="Estimator 类"></a>Estimator 类</h3><p>tf 提供了很多预创建的 Estimator，也可以自己定义 Estimator 类。但都是基于<code>tf.estimator.Estimator</code>。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>Estimator 类，用来训练和验证 TensorFlow 模型：</p><ul><li>Estimator 对象包含了一个模型 model_fn，这个模型给定输入和参数，会返回训练、验证或者预测等所需要的操作节点。</li><li>所有的输出（检查点、事件文件等）会写入到 model_dir，或者其子文件夹中。如果 model_dir 为空，则默认为临时目录。</li><li>config 参数为 tf.estimator.RunConfig 对象，包含了执行环境的信息。如果没有传递 config，则它会被 Estimator 实例化，使用的是默认配置。</li><li>params 包含了超参数。Estimator 只传递超参数，不会检查超参数，因此 params 的结构完全取决于开发者。</li><li>Estimator 的所有方法都不能被子类覆盖（它的构造方法强制决定的）。子类应该使用 model_fn 来配置母类，或者增添方法来实现特殊的功能。</li><li>Estimator 不支持 Eager Execution（eager execution 能够使用 Python 的 debug 工具、数据结构与控制流。并且无需使用 placeholder、session，计算结果能够立即得出）。</li></ul><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p><code>__init__(self, model_fn, model_dir=None, config=None, params=None, warm_start_from=None)</code></p><p>构造一个 Estimator 的实例。</p><p>参数：</p><ol><li>model_fn: 模型函数。<ol><li>参数：<ol><li>features: 这是 input_fn 返回的第一项（input_fn 是 train, evaluate 和 predict 的参数）。类型应该是单一的 Tensor 或者 dict。</li><li>labels: 这是 input_fn 返回的第二项。类型应该是单一的 Tensor 或者 dict。如果 mode 为 ModeKeys.PREDICT，则会默认为 labels=None。如果 model_fn 不接受 mode，model_fn 应该仍然可以处理 labels=None。</li><li>mode: 可选。指定是训练、验证还是测试。参见 ModeKeys。</li><li>params: 可选，超参数的 dict。 可以从超参数调整中配置 Estimators。</li><li>config: 可选，配置。如果没有传则为默认值。可以根据 num_ps_replicas 或 model_dir 等配置更新 model_fn。</li></ol></li><li>返回：EstimatorSpec</li></ol></li><li>model_dir:<ol><li>保存模型参数、图等的地址，也可以用来将路径中的检查点加载至 estimator 中来继续训练之前保存的模型。</li><li>如果是 PathLike， 那么路径就固定为它了。</li><li>如果是 None，那么 config 中的 model_dir 会被使用（如果设置了的话）</li><li>如果两个都设置了，那么必须相同；如果两个都是 None，则会使用临时目录。</li></ol></li><li>config: 配置类。</li><li>params: 超参数的 dict，会被传递到 model_fn。keys 是参数的名称，values 是基本 python 类型。</li><li>warm_start_from:<ol><li>可选，字符串，检查点的文件路径，用来指示从哪里开始热启动。</li><li>或者是 tf.estimator.WarmStartSettings 类来全部配置热启动。</li><li>如果是字符串路径，则所有的变量都是热启动，并且需要 Tensor 和词汇的名字都没有变。</li></ol></li><li>异常：<ol><li>RuntimeError： 开启了 eager execution</li><li>ValueError：model_fn 的参数与 params 不匹配</li><li>ValueError：这个函数被 Estimator 的子类所覆盖</li></ol></li></ol><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><p><code>train(self, input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None)</code></p><p>根据所给数据 input_fn， 对模型进行训练。</p><p>参数：</p><ol><li>input_fn：一个函数，提供由小 batches 组成的数据， 供训练使用。必须返回以下之一：<ul><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出必须是一个元组 (features, labels)，元组要求如下。</li><li>一个元组 (features, labels)：features 是一个 Tensor 或者一个字典（特征名为 Tensor），labels 是一个 Tensor 或者一个字典（特征名为 Tensor）。features 和 labels 都被 model_fn 所使用，应该符合 model_fn 输入的要求。</li></ul></li><li>hooks：SessionRunHook 子类实例的列表。用于在训练循环内部执行。</li><li>steps：模型训练的步数。<ol><li>如果是 None， 则一直训练，直到 input_fn 抛出了超过界限的异常。</li><li>steps 是递进式进行的。如果执行了两次训练（steps=10），则总共训练了 20 次。如果中途抛出了越界异常，则训练在 20 次之前就会停止。</li><li>如果你不想递进式进行，请换为设置 max_steps。如果设置了 steps，则 max_steps 必须是 None。</li></ol></li><li>max_steps：模型训练的最大步数。<ol><li>如果为 None，则一直训练，直到 input_fn 抛出了超过界限的异常。</li><li>如果设置了 max_steps， 则 steps 必须是 None。</li><li>如果中途抛出了越界异常，则训练在 max_steps 次之前就会停止。</li><li>执行两次 train(steps=100) 意味着 200 次训练；但是，执行两次 train(max_steps=100) 意味着第二次执行不会进行任何训练，因为第一次执行已经做完了所有的 100 次。</li></ol></li><li>saving_listeners：CheckpointSaverListener 对象的列表。用于在保存检查点之前或之后立即执行的回调函数。<br>返回：self：为了链接下去。</li></ol><p>异常：</p><ul><li>ValueError：steps 和 max_steps 都不是 None</li><li>ValueError：steps 或 max_steps &lt;= 0</li></ul><h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><p><code>evaluate(self, input_fn, steps=None, hooks=None, checkpoint_path=None, name=None)</code></p><p>根据所给数据 input_fn， 对模型进行验证。</p><p>对于每一步，执行 input_fn（返回数据的一个 batch）。一直进行验证，直到：</p><ul><li>steps 个 batches 进行完毕，或者</li><li>input_fn 抛出了越界异常（OutOfRangeError 或 StopIteration）</li></ul><p>参数：</p><ol><li>input_fn：一个函数，构造了验证所需的输入数据，必须返回以下之一：<ol><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出必须是一个元组 (features, labels)，元组要求如下。</li><li>一个元组 (features, labels)：features 是一个 Tensor 或者一个字典（特征名为 Tensor），labels 是一个 Tensor 或者一个字典（特征名为 Tensor）。features 和 labels 都被 model_fn 所使用，应该符合 model_fn 输入的要求。</li></ol></li><li>steps：模型验证的步数。如果是 None， 则一直验证，直到 input_fn 抛出了超过界限的异常。</li><li>hooks：SessionRunHook 子类实例的列表。用于在验证内部执行。</li><li>checkpoint_path： 用于验证的检查点路径。如果是 None， 则使用 model_dir 中最新的检查点。</li><li>name：验证的名字。使用者可以针对不同的数据集运行多个验证操作，比如训练集 vs 测试集。不同验证的结果被保存在不同的文件夹中，且分别出现在 tensorboard 中。</li></ol><p>返回：</p><ul><li>返回一个字典，包括 model_fn 中指定的评价指标、global_step（包含验证进行的全局步数）</li></ul><p>异常：</p><ul><li>ValueError：如果 step 小于等于 0</li><li>ValueError：如果 model_dir 指定的模型没有被训练，或者指定的 checkpoint_path 为空。</li></ul><h4 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h4><p><code>predict(self, input_fn, predict_keys=None, hooks=None, checkpoint_path=None, yield_single_examples=True)</code></p><p>对给出的特征进行预测。</p><p>参数：</p><ol><li>input_fn：一个函数，构造特征。预测一直进行下去，直到 input_fn 抛出了越界异常（OutOfRangeError 或 StopIteration）。函数必须返回以下之一：<ol><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出和以下的限制相同。</li><li>features：一个 Tensor 或者一个字典（特征名为 Tensor）。features 被 model_fn 所使用，应该符合 model_fn 输入的要求。</li><li>一个元组，其中第一项为 features。</li></ol></li><li>predict_keys：字符串列表，要预测的键值。当 EstimatorSpec.predictions 是一个 dict 时使用。如果使用了 predict_keys， 那么剩下的预测值会从字典中过滤掉。如果是 None，则返回全部。</li><li>hooks：SessionRunHook 子类实例的列表。用于在预测内部回调。</li><li>checkpoint_path： 用于预测的检查点路径。如果是 None， 则使用 model_dir 中最新的检查点。</li><li>yield_single_examples：If False, yield the whole batch as returned by the model_fn instead of decomposing the batch into individual elements. This is useful if model_fn returns some tensors whose first dimension is not equal to the batch size.</li></ol><p>返回：</p><ul><li>predictions tensors 的值</li></ul><p>异常：</p><ul><li>ValueError：model_dir 中找不到训练好的模型。</li><li>ValueError：预测值的 batch 长度不同，且 yield_single_examples 为 True。</li><li>ValueError：predict_keys 和 predictions 之间有冲突。例如，predict_keys 不是 None，但是 EstimatorSpec.predictions 不是一个 dict。</li></ul><h3 id="estimator-train-evaluate"><a href="#estimator-train-evaluate" class="headerlink" title="estimator.train/evaluate"></a>estimator.train/evaluate</h3><p>了解了 Estimator 可以理解上面的<code>estimator.train</code>和<code>estimator.evaluate</code>了</p><p>搞定了上面的部分就可以完成<a href="#model_fn_builder">model_fn_builder</a>部分的理解，然后返回<a href="#main-of-pretrain">main</a>。这下只剩<code>estimator.train</code>和<code>estimator.evaluate</code>了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">estimator = tf.contrib.tpu.TPUEstimator(</span><br><span class="line">    use_tpu=FLAGS.use_tpu,</span><br><span class="line">    model_fn=model_fn,</span><br><span class="line">    config=run_config,</span><br><span class="line">    train_batch_size=FLAGS.train_batch_size,</span><br><span class="line">    eval_batch_size=FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">  tf.logging.info(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.train_batch_size)</span><br><span class="line">  <span class="comment"># 从tfrecord解析出BERT的输入数据</span></span><br><span class="line">  train_input_fn = input_fn_builder(</span><br><span class="line">      input_files=input_files,</span><br><span class="line">      max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">      max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">      is_training=<span class="literal">True</span>)</span><br><span class="line">  estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> FLAGS.do_eval:</span><br><span class="line">  tf.logging.info(<span class="string">"***** Running evaluation *****"</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.eval_batch_size)</span><br><span class="line">  <span class="comment"># 如果不训练只是eval，输入也是一样的tfrecord</span></span><br><span class="line">  eval_input_fn = input_fn_builder(</span><br><span class="line">      input_files=input_files,</span><br><span class="line">      max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">      max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">      is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># evaluate操作得到结果</span></span><br><span class="line">  result = estimator.evaluate(</span><br><span class="line">      input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)</span><br><span class="line">`</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="post-tags"><a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a> <a href="/tags/BERT/" rel="tag"><i class="fa fa-tag"></i> BERT</a> <a href="/tags/Pretrain/" rel="tag"><i class="fa fa-tag"></i> Pretrain</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MachineLearning/2021-02-23-bert-finetune-analysis" rel="prev" title="BERT-FineTune源码理解"><i class="fa fa-chevron-left"></i> BERT-FineTune源码理解</a></div><div class="post-nav-item"><a href="/Notes/2021-03-22-jobhunting-interview-notes" rel="next" title="实习准备和总结">实习准备和总结 <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">403k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">16:47</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis";
    this.page.identifier = "MachineLearning/2021-02-23-bert-create-pretrain-data-analysis.html";
    this.page.title = "BERT-预训练源码理解";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : 'd58c988e42197d7063fabc8454111673',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>