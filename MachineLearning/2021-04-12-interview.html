<!DOCTYPE html><html lang="en,default"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.2.1"><link rel="apple-touch-icon" sizes="180x180" href="/favicon.ico"><link rel="icon" type="image/png" sizes="32x32" href="/avatar.jpg"><link rel="icon" type="image/png" sizes="16x16" href="/favicon.ico"><link rel="mask-icon" href="/favicon.ico" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><link rel="stylesheet" href="/lib/animate-css/animate.min.css"><script class="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"hanielxx.com",root:"/",scheme:"Mala",version:"8.0.0-rc.4",exturl:!1,sidebar:{position:"right",display:"always",padding:18,offset:12},copycode:!0,bookmark:{enable:!0,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"buttons",active:"disqus",storage:!0,lazyload:!1,nav:null,activeClass:"disqus"},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"fadeInDown",post_body:"fadeInDown",coll_header:"fadeInLeft",sidebar:"fadeInUp"}},prism:!1,path:"search.xml"}</script><meta name="description" content="一些面试问题的笔记"><meta property="og:type" content="article"><meta property="og:title" content="ML&#x2F;DL面试问题笔记"><meta property="og:url" content="https://hanielxx.com/MachineLearning/2021-04-12-interview"><meta property="og:site_name" content="Catch Your Dream"><meta property="og:description" content="一些面试问题的笔记"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XaoLIh.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4acX0Q.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8vnRpc.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/C6HUrh.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DQ7D7G.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TcyRmb.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JkU2ty.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YZEkTs.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/q98QDu.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CFCgNp.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gNSNNs.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/iRu0Z5.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/h6sqyp.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LZ1mK5.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/22yTOJ.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/c1mmPx.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n02z4h.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UjzCec.jpg"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/S9M9HO.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gKr2db.png"><meta property="og:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OOsxNF.png"><meta property="article:published_time" content="2021-04-12T05:11:01.000Z"><meta property="article:modified_time" content="2021-10-18T13:08:33.875Z"><meta property="article:author" content="Hanielxx"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="MachineLearning"><meta property="article:tag" content="Notes"><meta property="article:tag" content="Interview"><meta property="article:tag" content="Q&amp;A"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XaoLIh.png"><link rel="canonical" href="https://hanielxx.com/MachineLearning/2021-04-12-interview.html"><script class="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"en"}</script><title>ML/DL面试问题笔记 | Catch Your Dream</title><noscript><style>body{margin-top:2rem}.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header,.use-motion .sidebar{visibility:visible}.use-motion .footer,.use-motion .header,.use-motion .site-brand-container .toggle{opacity:initial}.use-motion .custom-logo-image,.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line{transform:scaleX(1)}.search-pop-overlay,.sidebar-nav{display:none}.sidebar-panel{display:block}</style></noscript><style>mjx-container[jax=SVG]{direction:ltr}mjx-container[jax=SVG]>svg{overflow:visible}mjx-container[jax=SVG]>svg a{fill:#00f;stroke:#00f}mjx-container[jax=SVG][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=SVG][justify=left]{text-align:left}mjx-container[jax=SVG][justify=right]{text-align:right}g[data-mml-node=merror]>g{fill:red;stroke:red}g[data-mml-node=merror]>rect[data-background]{fill:#ff0;stroke:none}g[data-mml-node=mtable]>line[data-line]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>rect[data-frame]{stroke-width:70px;fill:none}g[data-mml-node=mtable]>.mjx-dashed{stroke-dasharray:140}g[data-mml-node=mtable]>.mjx-dotted{stroke-linecap:round;stroke-dasharray:0,140}g[data-mml-node=mtable]>svg{overflow:visible}[jax=SVG] mjx-tool{display:inline-block;position:relative;width:0;height:0}[jax=SVG] mjx-tool>mjx-tip{position:absolute;top:0;left:0}mjx-tool>mjx-tip{display:inline-block;padding:.2em;border:1px solid #888;font-size:70%;background-color:#f8f8f8;color:#000;box-shadow:2px 2px 5px #aaa}g[data-mml-node=maction][data-toggle]{cursor:pointer}mjx-status{display:block;position:fixed;left:1em;bottom:1em;min-width:25%;padding:.2em .4em;border:1px solid #888;font-size:90%;background-color:#f8f8f8;color:#000}foreignObject[data-mjx-xml]{font-family:initial;line-height:normal;overflow:visible}.MathJax path{stroke-width:3}mjx-container[display=true]{overflow:auto hidden}mjx-container[display=true]+br{display:none}</style><link rel="alternate" href="/atom.xml" title="Catch Your Dream" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><main class="main"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="Toggle navigation bar"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><h1 class="site-title">Catch Your Dream</h1><i class="logo-line"></i></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-algorithm"><a href="/tags/Algorithm/" rel="section"><i class="fa fa-code fa-fw"></i>Algorithm</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="Searching..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">Table of Contents</li><li class="sidebar-nav-overview">Overview</li></ul><section class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习基础"><span class="nav-number">1.</span> <span class="nav-text">深度学习基础</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积模型"><span class="nav-number">2.</span> <span class="nav-text">卷积模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#预训练相关"><span class="nav-number">3.</span> <span class="nav-text">预训练相关</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#计算机视觉"><span class="nav-number">4.</span> <span class="nav-text">计算机视觉</span></a></li></ol></div></section><section class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Hanielxx" src="/avatar.jpg"><p class="site-author-name" itemprop="name">Hanielxx</p><div class="site-description" itemprop="description">Hanielxx | Blog</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">150</span> <span class="site-state-item-name">posts</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">8</span> <span class="site-state-item-name">categories</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">235</span> <span class="site-state-item-name">tags</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/HanielF" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;HanielF" rel="external nofollow noopener noreferrer" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="mailto:hanielxx@outlook.com" title="E-Mail → mailto:hanielxx@outlook.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div></section></div></aside><div id="sidebar-dimmer"></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><div class="reading-progress-bar"></div><a role="button" class="book-mark-link book-mark-link-fixed"></a> <a href="https://github.com/HanielF" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="external nofollow noopener noreferrer" target="_blank"><svg width="80" height="80" viewbox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><noscript><div id="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><div class="main-inner"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en"><link itemprop="mainEntityOfPage" href="https://hanielxx.com/MachineLearning/2021-04-12-interview"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/avatar.jpg"><meta itemprop="name" content="Hanielxx"><meta itemprop="description" content="Hanielxx | Blog"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Catch Your Dream"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">ML/DL面试问题笔记</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">Posted on</span> <time title="Created: 2021-04-12 13:11:01" itemprop="dateCreated datePublished" datetime="2021-04-12T13:11:01+08:00">2021-04-12</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">Edited on</span> <time title="Modified: 2021-10-18 21:08:33" itemprop="dateModified" datetime="2021-10-18T21:08:33+08:00">2021-10-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">In</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/MachineLearning/" itemprop="url" rel="index"><span itemprop="name">MachineLearning</span></a> </span></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-comment"></i> </span><span class="post-meta-item-text">Disqus: </span><a title="disqus" href="/MachineLearning/2021-04-12-interview#disqus_thread" itemprop="discussionUrl"><span class="post-comments-count disqus-comment-count" data-disqus-identifier="MachineLearning/2021-04-12-interview.html" itemprop="commentCount"></span></a></span><br><span class="post-meta-item" title="Symbols count in article"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">Symbols count in article: </span><span>11k</span> </span><span class="post-meta-item" title="Reading time"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">Reading time &asymp;</span> <span>28 mins.</span></span></div></header><div class="post-body" itemprop="articleBody"><meta name="referrer" content="no-referrer"><div class="note info"><p>一些面试问题的笔记</p></div><a id="more"></a><h2 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h2><ol><li>为什么要归一化？为什么归一化能够提高求解最优解的速度？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XaoLIh.png" width="600"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4acX0Q.png" width="600"></li></ol></li><li>归一化与标准化<ol><li>归一化/标准化的目的是为了获得某种“无关性”——偏置无关、尺度无关、长度无关……当归一化/标准化方法背后的物理意义和几何含义与当前问题的需要相契合时，其对解决该问题就有正向作用，反之，就会起反作用。所以，“何时选择何种方法”取决于待解决的问题，即problem-dependent。</li><li><a href="https://www.cnblogs.com/shine-lee/p/11779514.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/shine-lee/p/11779514.html</a></li><li>有什么联系和区别？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8vnRpc.png" width="600"></li><li>归一化和标准化的区别：归一化是将样本的特征值转换到同一量纲下把数据映射到[0,1]或者[-1, 1]区间内，仅由变量的极值决定，因区间放缩法是归一化的一种。标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，转换为标准正态分布，和整体样本分布相关，每个样本点都能对标准化产生影响。它们的相同点在于都能取消由于量纲不同引起的误差；都是一种线性变换，都是对向量X按照比例压缩再进行平移。</li></ol></li><li>归一化类型：<ol><li>min-max</li><li>线型比例，就是除以最大值</li><li>zscore转到标准正态分布</li></ol></li><li>什么时候需要使用feature scaling？<ol><li>涉及或隐含距离计算的算法，比如K-means、KNN、PCA、SVM等，一般需要feature scaling，因为<ul><li>zero-mean一般可以增加样本间余弦距离或者内积结果的差异，区分力更强，假设数据集集中分布在第一象限遥远的右上角，将其平移到原点处，可以想象样本间余弦距离的差异被放大了。在模版匹配中，zero-mean可以明显提高响应结果的区分度。</li><li>就欧式距离而言，增大某个特征的尺度，相当于增加了其在距离计算中的权重，如果有明确的先验知识表明某个特征很重要，那么适当增加其权重可能有正向效果，但如果没有这样的先验，或者目的就是想知道哪些特征更重要，那么就需要先feature scaling，对各维特征等而视之。</li></ul></li><li>增大尺度的同时也增大了该特征维度上的方差<ol><li>PCA算法倾向于关注方差较大的特征所在的坐标轴方向，其他特征可能会被忽视，因此，在PCA前做Standardization效果可能更好，如下图所示，图片来自scikit learn-Importance of Feature Scaling，<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/C6HUrh.png" width="600"></li></ol></li><li>损失函数中含有正则项时，一般需要feature scaling：<ol><li>对于线性模型𝑦=𝑤𝑥+𝑏而言，𝑥的任何线性变换（平移、放缩），都可以被𝑤和𝑏“吸收”掉，理论上，不会影响模型的拟合能力。但是，如果损失函数中含有正则项，如𝜆||𝑤||2，𝜆为超参数，其对𝑤的每一个参数施加同样的惩罚，但对于某一维特征𝑥𝑖而言，其scale越大，系数𝑤𝑖越小，其在正则项中的比重就会变小，相当于对𝑤𝑖惩罚变小，即损失函数会相对忽视那些scale增大的特征，这并不合理，所以需要feature scaling，使损失函数平等看待每一维特征。</li></ol></li><li>梯度下降算法，需要feature scaling。<ol><li>梯度下降的参数更新公式:<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.816ex" xmlns="http://www.w3.org/2000/svg" width="26.671ex" height="3.185ex" role="img" focusable="false" viewbox="0 -1047.1 11788.5 1407.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mo" transform="translate(1048, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1437, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(2020.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mn" transform="translate(3020.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(3520.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(4187.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(5243, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mo" transform="translate(6291, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(6680, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"/></g><g data-mml-node="mo" transform="translate(7041, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(7652.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(8652.4, 0)"><g data-mml-node="mo"><path data-c="1D702" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q156 442 175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336V326Q503 302 439 53Q381 -182 377 -189Q364 -216 332 -216Q319 -216 310 -208T299 -186Q299 -177 358 57L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mfrac" transform="translate(9149.4, 0)"><g data-mml-node="mrow" transform="translate(220, 516.8) scale(0.707)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mi" transform="translate(520, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"/></g><g data-mml-node="mo" transform="translate(1284, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1673, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mo" transform="translate(2721, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="mrow" transform="translate(765.2, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"/></g><g data-mml-node="mi" transform="translate(520, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g></g><rect width="2399.1" height="60" x="120" y="220"/></g></g></g></svg></mjx-container>。𝐸(𝑊) 为损失函数，收敛速度取决于：参数的初始位置到local minima的距离，以及学习率𝜂的大小。</li><li>一维情况下，在local minima附近，不同学习率对梯度下降的影响如下图所示，<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DQ7D7G.png" width="600"></li><li>多维情况下可以分解成多个上图，每个维度上分别下降，参数𝑊为向量，但学习率只有1个，即所有参数维度共用同一个学习率（暂不考虑为每个维度都分配单独学习率的算法）。收敛意味着在每个参数维度上都取得极小值，每个参数维度上的偏导数都为0，但是每个参数维度上的下降速度是不同的，为了每个维度上都能收敛，学习率应取所有维度在当前位置合适步长中最小的那个。下面讨论feature scaling对gradient descent的作用</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TcyRmb.png" width="600"></li></ol></li><li>对于传统的神经网络，对输入做feature scaling也很重要，因为采用sigmoid等有饱和区的激活函数，如果输入分布范围很广，参数初始化时没有适配好，很容易直接陷入饱和区，导致梯度消失，所以，需要对输入做Standardization或映射到[0,1]、[−1,1]，配合精心设计的参数初始化方法，对值域进行控制。但自从有了Batch Normalization，每次线性变换改变特征分布后，都会重新进行Normalization，似乎可以不太需要对网络的输入进行feature scaling了？但习惯上还是会做feature scaling。</li></ol></li><li>什么时候不需要feature scaling?<ol><li>与距离计算无关的概率模型，不需要feature scaling，比如Naive Bayes；</li><li>与距离计算无关的基于树的模型，不需要feature scaling，比如决策树、随机森林等，树中节点的选择只关注当前特征在哪里切分对分类更好，即只在意特征内部的相对大小，而与特征间的相对大小无关。</li></ol></li><li>什么时候用z-score 归一化？<ol><li>当特征的区间相差非常大时使用。比如X1区间是[0,2000]，X2区间是[1,5]，其所形成的等高线非常尖。当使用梯度下降法寻求最优解时，很有可能走“之字型”路线（垂直等高线走），从而导致需要迭代很多次才能收敛。</li></ol></li><li>什么时候用min-max归一化？<ol><li>适和于数值比较集中的情况。如果max和min不稳定，很容易使得归一化结果不稳定，使得后续使用效果也不稳定，实际使用中可以用经验常量值来替代max和min。而且当有新数据加入时，可能导致max和min的变化，需要重新定义。</li></ol></li></ol></li><li>学习率的影响<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JkU2ty.png" width="600"></li><li>常用的学习率衰减方法，包括：分段常数衰减、指数衰减、自然指数衰减、多项式衰减、间隔衰减、多间隔衰减、逆时间衰减、Lambda衰减、余弦衰减、诺姆衰减、loss自适应衰减、线性学习率热身</li></ol></li><li>batch size的影响<ol><li>传统的梯度下降用所有数据<ol><li>缺点1：如果是凸问题，局部最优就是全局最优，准确的梯度计算可以使梯度更新朝着正确的方向进行，以较快的速度达到全局最优解。但是真实场景中，很多都是非凸，这样容易陷入局部最优点或者鞍点。</li><li>缺点2：当整体样本数不大时，采用全体样本做batch size不会非常耗时，而当样本数很大时，每一次的计算将会非常耗时，也会导致内存爆炸。</li></ol></li><li>随机梯度下降SGD：<ol><li>随机梯度下降法时，batch size值为1，每次获得的梯度都是根据当前的随机样本计算得来。由一个样本的梯度来近似所有的样本，会导致梯度估计不是很准确</li><li>梯度易收到极端值的影响，导致损失剧烈震荡。但因为batch size为1，随机梯度下降法的计算速度会非常快。</li></ol></li><li>mini-batch：<ol><li>优点:<ol><li>用部分样本来近似全部样本，梯度相对于batch size为1更为准确，同时相比与使用全部样本，计算量减小，计算速度和收敛速度都会得到提升</li><li>相对于训练全部样本，由于梯度存在不准确性，噪声的影响很可能会让梯度下降的过程中离开局部最优点或鞍点，从而有机会寻找全局最优点</li></ol></li><li>为什么10000样本训练1次会比100样本训练100次收敛慢呢？<ol><li>我们假设样本真实的标准差为 𝜎，则 𝑛 个样本均值的标准差为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-1.334ex" xmlns="http://www.w3.org/2000/svg" width="3.32ex" height="2.915ex" role="img" focusable="false" viewbox="0 -698.8 1467.4 1288.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(531.8, 394) scale(0.707)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g><g data-mml-node="msqrt" transform="translate(220, -436.1) scale(0.707)"><g transform="translate(853, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(0, 0.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"/></g><rect width="600" height="42.4" x="853" y="758.1"/></g><rect width="1227.4" height="60" x="120" y="220"/></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.491ex" xmlns="http://www.w3.org/2000/svg" width="3.287ex" height="2.398ex" role="img" focusable="false" viewbox="0 -843 1453 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"/></g></g><g data-mml-node="mo" transform="translate(0, -17)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"/></g><rect width="600" height="60" x="853" y="723"/></g></g></g></svg></mjx-container>表明使用更多样本来估计梯度的方法回报是低于线性的。10000个样本训练一次和100个样本训练一次，由于计算量是线性的，前者的计算量是后者的100倍，但均值标准差只比后者降低了10倍，那么在相同的计算量下（同样训练10000个样本），小样本的收敛速度是远快于使用整个样本集的</li></ol></li><li>如何选择一个适当的batch size呢?<ol><li>更大的batch size会得到更精确的梯度估计值，但其估计梯度的回报是低于线性的。要更久收敛</li><li>如果训练集较小，可以直接使用梯度下降法，batch size等于样本集大小。</li><li>使用GPU时，通常使用2的幂数作为batch size可以获得更少的运行时间。</li></ol></li></ol></li></ol></li><li>在参数初始化时，为什么不能全零初始化？(转自paddlepedia)<ol><li>梯度更新后，由于前面的初始值都是一样的，所以会导致梯度更新后的参数在每一层都是一样的。再因为初始值相同，所以会导致无论经过多少次网络训练，相同网络内的参数都是相同的。使得网络在学习的时候没有重点，对所有特征处理相同时，导致模型无法收敛训练失败。这种现象也称为对称失效。</li><li>同样，如果被初始化为相同的非零值时，也是这样的情况。此时的模型就和线性模型的效果相似。</li></ol></li><li>常见的初始化方法<ol><li>非常常见的方式是采用高斯分布或均匀分布来对权重进行随机初始化。高斯分布和均匀分布的选择似乎没有很大差别，但初始分布的大小对于优化过程的结果和网络泛化能力都有很大影响。</li><li>高斯分布：高斯分布初始化：使用一个均值为 𝜇，方差为 𝜎2 的高斯分布 𝑁(𝜇,𝜎2) 对每个参数进行随机初始化，通常情况下，𝜇=0，并对生成的数乘上一个小数，把权重初始化为很小的随机数。比如：𝑤=0.01∗𝑛𝑝.𝑟𝑎𝑛𝑑𝑜𝑚.𝑟𝑎𝑛𝑑(𝐷,𝐻)，这里选择乘以0.01初始化为一个很小的数是因为，如果最初随机到的 𝑤 值很大，当我们选择 sigmoid 或 tanh 激活函数时，函数值 𝑠𝑖𝑔𝑚𝑜𝑖𝑑(⋅) 或 𝑡𝑎𝑛ℎ(⋅) 会停留在一个很平坦的地方，激活值接近饱和，导致梯度下降时，梯度很小，学习变得缓慢。但也不是说权重值越小越好，如果权重值过小，会导致在反向传播时计算得到很小的梯度值，在不断的反向传播过程中，引起梯度消失。</li><li>均匀分布：均匀分布初始化：在一个给定区间 [−𝑟,𝑟] 内采取均匀分布进行初始化。假设随机变量在区间 [𝑎,𝑏] 内均匀分布，则其方差为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.781ex" xmlns="http://www.w3.org/2000/svg" width="15.049ex" height="3.284ex" role="img" focusable="false" viewbox="0 -1106.5 6651.6 1451.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(485, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(1014, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1465, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1854, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(2426, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3092.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mfrac" transform="translate(4148.6, 0)"><g data-mml-node="mrow" transform="translate(220, 516.8) scale(0.707)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(818, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mi" transform="translate(1596, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="msup" transform="translate(2125, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><g data-mml-node="mn" transform="translate(898, -345) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500, 0)"/></g><rect width="2263" height="60" x="120" y="220"/></g></g></g></svg></mjx-container>。因此，当在 [−𝑟,𝑟] 的区间内均匀分布采样，并满足<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.566ex" xmlns="http://www.w3.org/2000/svg" width="11.591ex" height="2.452ex" role="img" focusable="false" viewbox="0 -833.9 5123.1 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"/></g><g data-mml-node="mi" transform="translate(485, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mi" transform="translate(1014, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(1465, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(1854, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(2426, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g><g data-mml-node="mo" transform="translate(3092.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msup" transform="translate(4148.6, 0)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g><g data-mml-node="mn" transform="translate(571, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></svg></mjx-container>时，则有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align:-.186ex" xmlns="http://www.w3.org/2000/svg" width="9.303ex" height="2.44ex" role="img" focusable="false" viewbox="0 -996.5 4112.1 1078.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(728.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="msqrt" transform="translate(1784.6, 0)"><g transform="translate(853, 0)"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/></g><g data-mml-node="msup" transform="translate(500, 0)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g><g data-mml-node="mn" transform="translate(571, 289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g><g data-mml-node="mo" transform="translate(0, 136.5)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"/></g><rect width="1474.6" height="60" x="853" y="876.5"/></g></g></g></svg></mjx-container></li><li>上述两种基于固定方差的初始随机化方法中，关键点在于如何设置方差 𝜎2。过大或过小的方差都会导致梯度下降缓慢，网络训练效果不好等问题。为了降低固定方差对网络性能及优化效率的影响，基于固定方差的随机初始化一般要配合逐层归一化来使用。</li></ol></li><li>基于方差缩放的参数初始化<ol><li>方差缩放方法能够根据神经元的链接数量来自适应地调整初始化分布地方差，尽可能的保证每个神经元的输入和输出方差一致。那么，为什么要保证前后方差的一致性呢？这是因为如果输入空间和输出空间的方差差别较大，也就是说数据空间分布差异较大，那么在反向传播时可能会引起梯度消失或爆炸问题。比如，当输入空间稀疏，输出空间稠密时，将在输出空间计算得到的误差反向传播给输入空间时，这个误差可能会显得微不足道，从而引起梯度消失。而当输入空间稠密，输出空间稀疏时，将误差反向传播给输入空间，就可能会引起梯度爆炸，使得模型震荡。</li><li>Xavier初始化</li><li>Kaiming初始化</li></ol></li><li>激活函数<ol><li>作用：<ol><li>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，最终的输出都是输入的线性组合。 激活函数给神经元引入了非线性因素，使得神经网络可以任意逼近任何非线性函数。</li></ol></li><li>sigmoid：<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YZEkTs.png" width="600"></li><li>优点：<ol><li>sigmoid 函数的输出映射在 (0,1) 之间，单调连续，输出范围有限，优化稳定，可以用作输出层；</li><li>求导容易；</li></ol></li><li>缺点：<ol><li>由于其软饱和性，一旦落入饱和区梯度就会接近于0，根据反向传播的链式法则，容易产生梯度消失，导致训练出现问题；</li><li>Sigmoid函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢；</li><li>计算时，由于具有幂运算，计算复杂度较高，运算速度较慢。</li></ol></li></ol></li><li>tanh<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/q98QDu.png" width="600"></li><li>优点：<ol><li>tanh 比 sigmoid 函数收敛速度更快；</li><li>相比 sigmoid 函数，tanh 是以 0 为中心的；</li></ol></li><li>缺点：<ol><li>与 sigmoid 函数相同，由于饱和性容易产生的梯度消失；</li><li>与 sigmoid 函数相同，由于具有幂运算，计算复杂度较高，运算速度较慢。</li></ol></li></ol></li><li>ReLU<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CFCgNp.png" width="600"></li><li>优点：<ol><li>收敛速度快；</li><li>相较于 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 和 𝑡𝑎𝑛ℎ 中涉及了幂运算，导致计算复杂度高， ReLU​可以更加简单的实现；</li><li>当输入 𝑥&gt;=0 时，ReLU​ 的导数为常数，这样可有效缓解梯度消失问题；</li><li>当 𝑥&lt;0 时，ReLU​ 的梯度总是 0，提供了神经网络的稀疏表达能力；</li></ol></li><li>缺点：<ol><li>ReLU​ 的输出不是以 0 为中心的；</li><li>神经元坏死现象，某些神经元可能永远不会被激活，导致相应参数永远不会被更新；</li><li>不能避免梯度爆炸问题；</li></ol></li></ol></li><li>激活函数的选择<ol><li>浅层网络在分类器时，𝑠𝑖𝑔𝑚𝑜𝑖𝑑 函数及其组合通常效果更好。</li><li>由于梯度消失问题，有时要避免使用 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 和 𝑡𝑎𝑛ℎ 函数。</li><li>𝑟𝑒𝑙𝑢 函数是一个通用的激活函数，目前在大多数情况下使用。</li><li>如果神经网络中出现死神经元，那么 𝑝𝑟𝑒𝑙𝑢 函数就是最好的选择。</li><li>𝑟𝑒𝑙𝑢 函数只能在隐藏层中使用。</li><li>通常，可以从 𝑟𝑒𝑙𝑢 函数开始，如果 𝑟𝑒𝑙𝑢 函数没有提供最优结果，再尝试其他激活函数。</li></ol></li><li>激活函数相关问题<ol><li>为什么 𝑟𝑒𝑙𝑢 不是全程可微/可导也能用于基于梯度的学习？<ol><li>从数学的角度看 𝑟𝑒𝑙𝑢 在 0 点不可导，因为它的左导数和右导数不相等；但在实现时通常会返回左导数或右导数的其中一个，而不是报告一个导数不存在的错误，从而避免了这个问题。</li></ol></li><li>为什么 𝑡𝑎𝑛ℎ 的收敛速度比 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 快？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gNSNNs.png" width="300"></li><li>由上面两个公式可知 𝑡𝑎𝑛ℎ 引起的梯度消失问题没有 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 严重，所以 𝑡𝑎𝑛ℎ 收敛速度比 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 快。</li></ol></li><li>sigmoid 和 softmax 有什么区别？<ol><li>二分类问题时 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 和 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 是一样的，都是求 𝑐𝑟𝑜𝑠𝑠 𝑒𝑛𝑡𝑟𝑜𝑝𝑦 𝑙𝑜𝑠𝑠 ，而 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 可以用于多分类问题。</li><li>𝑠𝑜𝑓𝑡𝑚𝑎𝑥 是 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 的扩展，因为，当类别数 𝑘=2 时，𝑠𝑜𝑓𝑡𝑚𝑎𝑥 回归退化为 𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐 回归。</li><li>𝑠𝑜𝑓𝑡𝑚𝑎𝑥 建模使用的分布是多项式分布，而 𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐 则基于伯努利分布。</li><li>多个 𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐 回归通过叠加也同样可以实现多分类的效果，但是 𝑠𝑜𝑓𝑡𝑚𝑎𝑥 回归进行的多分类，类与类之间是互斥的，即一个输入只能被归为一类；多 𝑙𝑜𝑔𝑖𝑠𝑡𝑖𝑐 回归进行多分类，输出的类别并不是互斥的，即”苹果”这个词语既属于”水果”类也属于”3𝐶”类别。</li></ol></li></ol></li></ol></li><li>评价指标<ol><li>F1</li><li>PR曲线：（Recall，Precision）</li><li>mAP：<ol><li>AP（Average Precision）：某一类P-R曲线下的面积。</li><li>mAP（mean Average Precision）：所有类别的AP值取平均。</li></ol></li></ol></li><li>为什么交叉熵损失可以用于分类任务？<ol><li><a href="https://www.cnblogs.com/shine-lee/p/12032066.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/shine-lee/p/12032066.html</a></li><li>损失函数的角度<ol><li>损失函数是网络学习的指挥棒，它引导着网络学习的方向——能让损失函数变小的参数就是好参数。 所以，损失函数的选择和设计要能表达你希望模型具有的性质与倾向。 对比交叉熵和均方误差损失，可以发现，两者均在𝑦̂ =𝑦=1时取得最小值0，但在实践中𝑦̂ 𝑝只会趋近于1而不是恰好等于1，在𝑦̂ 𝑝&lt;1的情况下， 交叉熵只与label类别有关，𝑦̂ 𝑝越趋近于1越好 均方误差不仅与𝑦̂ 𝑝有关，还与其他项有关，它希望𝑦̂ 1,…,𝑦̂ 𝑝−1,𝑦̂ 𝑝+1,…,𝑦̂ 𝐾越平均越好，即在1−𝑦̂ 𝑝𝐾−1时取得最小值 分类问题中，对于类别之间的相关性，我们缺乏先验。</li><li>虽然我们知道，与“狗”相比，“猫”和“老虎”之间的相似度更高，但是这种关系在样本标记之初是难以量化的，所以label都是one hot。 在这个前提下，均方误差损失可能会给出错误的指示，比如猫、老虎、狗的3分类问题，label为[1,0,0]，在均方误差看来，预测为[0.8,0.1,0.1]要比[0.8,0.15,0.05]要好，即认为平均总比有倾向性要好，但这有悖我们的常识。 而对交叉熵损失，既然类别间复杂的相似度矩阵是难以量化的，索性只能关注样本所属的类别，只要𝑦̂ 𝑝越接近于1就好，这显示是更合理的。</li></ol></li><li>softmax反向传播角度<ol><li>𝑦̂ 𝑝为正确分类的概率，为0时表示分类完全错误，越接近于1表示越正确。根据链式法则，按理来讲，对与𝑧𝑝相连的权重，损失函数的偏导会含有𝑦̂ 𝑝(1−𝑦̂ 𝑝)这一因子项，𝑦̂ 𝑝=0时分类错误，但偏导为0，权重不会更新，这显然不对——分类越错误越需要对权重进行更新。</li><li>对交叉熵损失，恰好将𝑦̂ 𝑝(1−𝑦̂ 𝑝)中的𝑦̂ 𝑝消掉，避免了上述情形的发生，且𝑦̂ 𝑝越接近于1，偏导越接近于0，即分类越正确越不需要更新权重，这与我们的期望相符。</li><li>而对均方误差损失，看偏导，仍会发生上面所说的情况——𝑦̂ 𝑝=0，分类错误，但不更新权重。</li><li>综上，对分类问题而言，无论从损失函数角度还是softmax反向传播角度，交叉熵都比均方误差要好。</li></ol></li></ol></li></ol><h2 id="卷积模型"><a href="#卷积模型" class="headerlink" title="卷积模型"></a>卷积模型</h2><ol><li>相较于全连接网络，卷积在图像处理方面有什么样的优势？<ol><li>保留空间信息<ol><li>在卷积运算中，计算范围是在像素点的空间邻域内进行的，它代表了对空间邻域内某种特征模式的提取。对比全连接层将输入展开成一维的计算方式，卷积运算可以有效学习到输入数据的空间信息。</li></ol></li><li>局部连接<ol><li>在卷积操作中，每个神经元只与局部的一块区域进行连接。对于二维图像，局部像素关联性较强，这种局部连接保证了训练后的滤波器能够对局部特征有最强的响应，使神经网络可以提取数据的局部特征。</li></ol></li><li>权重共享<ol><li>卷积计算实际上是使用一组卷积核在图片上进行滑动，计算乘加和。因此，对于同一个卷积核的计算过程而言，在与图像计算的过程中，它的权重是共享的。这其实就大大降低了网络的训练难度</li></ol></li><li>不同层级卷积提取不同特征<ol><li>在CNN网络中，通常使用多层卷积进行堆叠，从而达到提取不同类型特征的作用。比如:浅层卷积提取的是图像中的边缘等信息；中层卷积提取的是图像中的局部信息；深层卷积提取的则是图像中的全局信息。</li></ol></li></ol></li><li>如何计算感受野？<ol><li><a href="https://www.cnblogs.com/shine-lee/p/12069176.html" target="_blank" rel="external nofollow noopener noreferrer">https://www.cnblogs.com/shine-lee/p/12069176.html</a></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/iRu0Z5.png" width="600"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/h6sqyp.png" width="600"></li></ol></li><li>感受野中心计算？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LZ1mK5.png" width="600"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/22yTOJ.png" width="600"></li></ol></li><li>1*1卷积的作用是什么？<ol><li>实现信息的跨通道交互与整合。考虑到卷积运算的输入输出都是3个维度（宽、高、多通道），所以1×1 卷积实际上就是对每个像素点，在不同的通道上进行线性组合，从而整合不同通道的信息。</li><li>对卷积核通道数进行<strong>降维和升维</strong>，<strong>减少参数量</strong>。经过1×1 卷积后的输出保留了输入数据的原有平面结构，通过调控通道数，从而完成升维或降维的作用。</li><li>利用1×1 卷积后的非线性激活函数，在保持特征图尺寸不变的前提下，大幅增加非线性</li></ol></li><li>深度可分离卷积的计算方式以及意义是什么？<ol><li>它不仅仅涉及空间维度，还涉及深度维度（即 channel 维度）</li><li>运算量相较标准卷积而言，计算量少了很多</li></ol></li></ol><h2 id="预训练相关"><a href="#预训练相关" class="headerlink" title="预训练相关"></a>预训练相关</h2><ol><li>Multi-Head Attention时间复杂度?<ol><li>Self-Attention时间复杂度：𝑂(𝑛2⋅𝑑) <strong>(这里的2是表示平方)</strong> ，这里，n是序列的长度，d是embedding的维度。</li><li>Self-Attention包括三个步骤：相似度计算，softmax和加权平均，它们分别的时间复杂度是：</li><li>相似度计算可以看作大小为(n,d)和(d,n)的两个矩阵相乘： (𝑛,𝑑)∗(𝑑,𝑛)=(𝑛2⋅𝑑)，得到一个(n,n)的矩阵</li><li>softmax就是直接计算了，时间复杂度为: 𝑂(𝑛2)</li><li>加权平均可以看作大小为(n,n)和(n,d)的两个矩阵相乘： (𝑛,𝑛)∗(𝑛,𝑑)=(𝑛2⋅𝑑)，得到一个(n,d)的矩阵</li><li>因此，Self-Attention的时间复杂度是: 𝑂(𝑛2⋅𝑑)</li></ol></li><li>Transformer的权重共享？<ol><li>Encoder和Decoder间的Embedding层权重共享；<ol><li>Transformer被应用在机器翻译任务中，源语言和目标语言是不一样的，但它们可以共用一张大词表，对于两种语言中共同出现的词（比如：数字，标点等等）可以得到更好的表示，而且对于Encoder和Decoder，嵌入时都只有对应语言的embedding会被激活，因此是可以共用一张词表做权重共享的。</li><li>Transformer词表用了bpe来处理，所以最小的单元是subword。英语和德语同属日耳曼语族，有很多相同的subword，可以共享类似的语义。而像中英这样相差较大的语系，语义共享作用可能不会很大。但是，共用词表会使得词表数量增大，增加softmax的计算时间，因此实际使用中是否共享可能要根据情况权衡。</li></ol></li><li>Decoder中Embedding层和FC层权重共享；<ol><li>Embedding层可以说是通过onehot去取到对应的embedding向量，FC层可以说是相反的，通过向量（定义为 x）去得到它可能是某个词的softmax概率，取概率最大（贪婪情况下）的作为预测值。</li><li>那哪一个会是概率最大的呢？在FC层的每一行量级相同的前提下，理论上和 x 相同的那一行对应的点积和softmax概率会是最大的（可类比本文问题1）。</li><li>因此，Embedding层和FC层权重共享，Embedding层中和向量 x 最接近的那一行对应的词，会获得更大的预测概率。实际上，Decoder中的Embedding层和FC层有点像互为逆过程。</li></ol></li></ol></li><li>BERT, GPT, ELMO区别<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/c1mmPx.png" width="600"></li></ol></li><li>NSP和SOP的区别？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n02z4h.png" width="600"></li></ol></li></ol><h2 id="计算机视觉"><a href="#计算机视觉" class="headerlink" title="计算机视觉"></a>计算机视觉</h2><ol><li>ResNet中Residual block解决了什么问题？<ol><li>解决了网络退化的问题。在Resnet中，56层的深层网络，在训练集和测试集上的表现都不如20层的浅层网络。这种随着网络层数加深，accuracy逐渐饱和，然后出现急剧下降，具体表现为深层网络的训练效果反而不如浅层网络好的现象，被称为网络退化（degradation）。</li><li>为什么会引起网络退化呢？按照理论上的想法，当浅层网络效果不错的时候，网络层数的增加即使不会引起精度上的提升也不该使模型效果变差。但事实上非线性的激活函数的存在，会造成很多不可逆的信息损失，网络加深到一定程度，过多的信息损失就会造成网络的退化。</li><li>而残差连接就是让网络拥有恒等映射的能力，使每一次能够学到更细化的特征从而提高网络精度。</li><li>恒等映射即为 𝐻(𝑥)=𝑥，已有的神经网络结构很难做到这一点，但是如果我们将网络设计成 𝐻(𝑥)=𝐹(𝑥)+𝑥，即 𝐹(𝑥)=𝐻(𝑥)−𝑥，那么只需要使残差函数 𝐹(𝑥)=0，就构成了恒等映射 𝐻(𝑥)=𝐹(𝑥)。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UjzCec.jpg" width="600"></li><li>残差结构的目的是，随着网络的加深，使 𝐹(𝑥) 逼近于0，使得深度网络的精度在最优浅层网络的基础上不会下降。看到这里你或许会有疑问，既然如此为什么不直接选取最优的浅层网络呢？这是因为最优的浅层网络结构并不易找寻，而ResNet可以通过增加深度，找到最优的浅层网络并保证深层网络不会因为层数的叠加而发生网络退化。</li></ol></li><li>计算IOU<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/S9M9HO.png" width="600"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gKr2db.png" width="600"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OOsxNF.png" width="600"></li><li>代码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_iou_xyxy</span><span class="params">(box1, box2)</span>:</span><span class="comment"># 坐标轴左上角是原点，往下是y，往右是x</span></span><br><span class="line">x1min, y1min, x1max, y1max = box1[<span class="number">0</span>], box1[<span class="number">1</span>], box1[<span class="number">2</span>], box1[<span class="number">3</span>]</span><br><span class="line">x2min, y2min, x2max, y2max = box2[<span class="number">0</span>], box2[<span class="number">1</span>], box2[<span class="number">2</span>], box2[<span class="number">3</span>]</span><br><span class="line">s1 = (x1max - x1min + <span class="number">1.</span>) * (y1max - y1min + <span class="number">1.</span>)</span><br><span class="line">s2 = (x2max - x2min + <span class="number">1.</span>) * (y2max - y2min + <span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">xmin = np.maximum(x1min, x2min)</span><br><span class="line">xmax = np.minimum(x1max, x2max)</span><br><span class="line">ymin = np.maximum(y1min, y2min)</span><br><span class="line">ymax = np.minimum(y1max, y2max)</span><br><span class="line"></span><br><span class="line">inter = np.maximum(xmax - xmin + <span class="number">1.</span>, <span class="number">0</span>) * np.maximum(ymax - ymin + <span class="number">1.</span>, <span class="number">0</span>)</span><br><span class="line">union = s1 + s2 - inter</span><br><span class="line">iou = inter / union</span><br><span class="line"><span class="keyword">return</span> iou</span><br></pre></td></tr></table></figure></li></ol></li><li>非极大值抑制（NMS）<ol><li>主要用于在目标检测过程中，消除重叠较大的冗余预测框</li><li>基本思想是，如果有多个预测框都对应同一个物体，则只选出得分最高的那个预测框，剩下的预测框被丢弃掉。</li><li>如何判断两个预测框对应的是同一个物体呢，标准该怎么设置？<ol><li>如果两个预测框的类别一样，而且他们的位置重合度比较大，则可以认为他们是在预测同一个目标。</li><li>非极大值抑制的做法是，选出某个类别得分最高的预测框，然后看哪些预测框跟它的IoU大于阈值，就把这些预测框给丢弃掉。这里IoU的阈值是超参数，需要提前设置，这里我们参考YOLOv3算法，里面设置的是0.5。</li></ol></li><li>如果是多分类，就要对每个类别进行NMS</li><li>代码<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">nms</span><span class="params">(bboxes, scores, score_thresh, nms_thresh)</span>:</span></span><br><span class="line">   inds = np.argsort(scores)</span><br><span class="line">   inds = inds[::<span class="number">-1</span>]  <span class="comment"># 从大到小排序的下标</span></span><br><span class="line">   keep_inds = []</span><br><span class="line">   <span class="keyword">while</span> (len(inds) &gt; <span class="number">0</span>):</span><br><span class="line">      cur_ind = inds[<span class="number">0</span>]</span><br><span class="line">      current_box = bboxes[cur_ind]</span><br><span class="line">      cur_score = scores[cur_ind]</span><br><span class="line">      <span class="keyword">if</span> cur_score &lt; score_thresh:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">      keep = <span class="literal">True</span></span><br><span class="line">      <span class="keyword">for</span> ind <span class="keyword">in</span> keep_inds:</span><br><span class="line">            remain_box = bboxes[ind]</span><br><span class="line">            iou = box_iou_xyxy(current_box, remain_box)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> iou &gt; nms_thresh:</span><br><span class="line">               keep = <span class="literal">False</span></span><br><span class="line">               <span class="keyword">break</span></span><br><span class="line">      <span class="keyword">if</span> keep:</span><br><span class="line">            keep_inds.append(cur_ind)</span><br><span class="line">      inds = inds[<span class="number">1</span>:]</span><br><span class="line"><span class="keyword">return</span> np.array(keep_inds)</span><br></pre></td></tr></table></figure></li></ol></li><li>写中值滤波<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">padding_0</span><span class="params">(img)</span>:</span></span><br><span class="line">   a = np.insert(img, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">   b = np.insert(a, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">   c = np.insert(b, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">   d = np.insert(c, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">   h_zeros = np.zeros((d.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">   e = np.hstack((d, h_zeros))</span><br><span class="line">   f = np.hstack((e, h_zeros))</span><br><span class="line">   w_zeros = np.zeros((<span class="number">1</span>, f.shape[<span class="number">1</span>]))</span><br><span class="line">   g = np.vstack((f, w_zeros))</span><br><span class="line">   img = np.vstack((g, w_zeros))</span><br><span class="line">   <span class="keyword">return</span> img</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">middle_filter</span><span class="params">(img)</span>:</span></span><br><span class="line">   img = padding_0(img)</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, img.shape[<span class="number">0</span>] - <span class="number">2</span>):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">2</span>, img.shape[<span class="number">0</span>] - <span class="number">2</span>):</span><br><span class="line">            mat = img[i - <span class="number">2</span>:i + <span class="number">3</span>, j - <span class="number">2</span>:j + <span class="number">3</span>]</span><br><span class="line">            middle = np.median(mat)</span><br><span class="line">            img[i, j] = middle</span><br><span class="line">   <span class="keyword">return</span> img</span><br></pre></td></tr></table></figure></li><li>根据要求写dataloader<ol><li>自定义dataset，实现<ol><li><code>__init__</code>初始化读入数据和label</li><li><code>__len__</code>返回数据集长度</li><li><code>__getitem__</code>返回index对应的数据</li></ol></li><li>自定义Dataloader中的collate_fn，这个是对每个batch数据进行的操作。参考<a href="https://zhuanlan.zhihu.com/p/30385675" target="_blank" rel="external nofollow noopener noreferrer">PyTorch实现自由的数据读取</a></li></ol></li></ol></div><footer class="post-footer"><div class="post-tags"><a href="/tags/DeepLearning/" rel="tag"><i class="fa fa-tag"></i> DeepLearning</a> <a href="/tags/MachineLearning/" rel="tag"><i class="fa fa-tag"></i> MachineLearning</a> <a href="/tags/Notes/" rel="tag"><i class="fa fa-tag"></i> Notes</a> <a href="/tags/Interview/" rel="tag"><i class="fa fa-tag"></i> Interview</a> <a href="/tags/Q-A/" rel="tag"><i class="fa fa-tag"></i> Q&A</a></div><div class="post-nav"><div class="post-nav-item"><a href="/MachineLearning/2021-03-31-bert-finetune-analysis" rel="prev" title="BERT-FineTune源码理解"><i class="fa fa-chevron-left"></i> BERT-FineTune源码理解</a></div><div class="post-nav-item"><a href="/MachineLearning/2021-04-14-allennlp-notes" rel="next" title="AllenNLP框架学习笔记（一）">AllenNLP框架学习笔记（一） <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comment-button-group"><a class="btn comment-button disqus">disqus</a> <a class="btn comment-button gitalk">gitalk</a></div><div class="comment-position disqus"><div class="comments"><div id="disqus_thread"><noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript></div></div></div><div class="comment-position gitalk"><div class="comments" id="gitalk-container"></div></div><script>(function() {
          let commentButton = document.querySelectorAll('.comment-button');
            commentButton.forEach(element => {
            let commentClass = element.classList[2];
            element.addEventListener('click', () => {
              commentButton.forEach(active => active.classList.toggle('active', active === element));
              document.querySelectorAll('.comment-position').forEach(active => active.classList.toggle('active', active.classList.contains(commentClass)));
              if (CONFIG.comments.storage) {
                localStorage.setItem('comments_active', commentClass);
              }
            });
          });
          let { activeClass } = CONFIG.comments;
          if (CONFIG.comments.storage) {
            activeClass = localStorage.getItem('comments_active') || activeClass;
          }
          if (activeClass) {
            let activeButton = document.querySelector(`.comment-button.${activeClass}`);
            if (activeButton) {
              activeButton.click();
            }
          }
        })();</script><script>window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }</script></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2021</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">Haniel Farnsworth</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">Symbols count total: </span><span title="Symbols count total">582k</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">Reading time total &asymp;</span> <span title="Reading time total">24:15</span></div><div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://github.com/next-geek/next-geek" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Next-geek</a></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script><script>!function(){var t=document.createElement("script"),e=window.location.protocol.split(":")[0];t.src="https"===e?"https://zz.bdstatic.com/linksubmit/push.js":"http://push.zhanzhang.baidu.com/push.js",(e=document.getElementsByTagName("script")[0]).parentNode.insertBefore(t,e)}()</script><script src="/js/local-search.js"></script><script>function loadCount(){var d=document,n=d.createElement("script");n.src="https://hanielxx.disqus.com/count.js",n.id="dsq-count-scr",(d.head||d.body).appendChild(n)}window.addEventListener("load",loadCount,!1)</script><script>var disqus_config = function() {
    this.page.url = "https://hanielxx.com/MachineLearning/2021-04-12-interview";
    this.page.identifier = "MachineLearning/2021-04-12-interview.html";
    this.page.title = "ML/DL面试问题笔记";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hanielxx.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : '27e3eba13ef3780f492b',
      clientSecret: '4e28d0b26bbf1501e220a7d94b983aec4e4c11df',
      repo        : 'CommentsRepo',
      owner       : 'HanielF',
      admin       : ['HanielF'],
      id          : 'cc6790bf7b29af3af0ce52d8c5715111',
        language: 'en',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});</script></body></html>