<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Catch Your Dream</title>
  
  
  <link href="https://hanielxx.com/atom.xml" rel="self"/>
  
  <link href="https://hanielxx.com/"/>
  <updated>2021-09-04T12:41:06.060Z</updated>
  <id>https://hanielxx.com/</id>
  
  <author>
    <name>Hanielxx</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VLN-R2R任务源码理解</title>
    <link href="https://hanielxx.com/MachineLearning/2021-09-02-vln-r2r-seq2seq-code"/>
    <id>https://hanielxx.com/MachineLearning/2021-09-02-vln-r2r-seq2seq-code</id>
    <published>2021-09-02T12:11:01.000Z</published>
    <updated>2021-09-04T12:41:06.060Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>论文《<a href="https://arxiv.org/abs/1711.07280" target="_blank" rel="external nofollow noopener noreferrer">Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments</a>》是VLN的开篇之作，这里记录下对它在R2R任务里面的源码理解。</p><p>github仓库地址：<a href="https://github.com/peteanderson80/Matterport3DSimulator/tree/master/tasks/R2R" target="_blank" rel="external nofollow noopener noreferrer">VLN-R2R</a></p></div><a id="more"></a><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><ol><li>程序入口：<code>train.py/train_val()</code></li><li>vocab的建立和保存</li><li>创建tokenizer</li><li>创建训练环境train_env，是一个R2RBatch类<ol><li><strong>R2RBatch类</strong>初始化init<ol><li>导入feature和image的信息并建立batch个Simulator()</li><li>加载数据集，保存scans信息，记录instruction信息，并通过vocab对instruction进行encode</li><li>load_nav_graphs，加载每个scan的链接图信息</li><li>all_pairs_dijkstra_path，计算所有的最短路径</li></ol></li></ol></li><li>创建val_envs，即验证环境，分为seen和unseen<ol><li>这里使用字典保存<code>val_seen: (R2RBatch(), Evaluation()), val_unseen: (R2RBatch(), Evaluation())</code><ol><li><strong>Evaluation类</strong><ol><li><code>_get_nearest()</code>是从path中找到距离goal id最近的一个节点id</li><li><code>_score_item()</code>是计算最终结果和目标点的<ol><li>nav error即最终点id和goal之间的距离</li><li>oracle error即和目标id最接近的id，它和goal之间的距离</li><li>trajectory steps即path的step个数减1</li><li>trajectory lengths即path的总距离</li></ol></li><li><code>score()</code>是通过和目标点的距离，验证每个agent 的轨迹<ol><li>通过<code>score_item()</code>得到每个path的分数</li><li>返回每个路径的平均分数</li><li>以及两个成功率：nav error小于目标值的概率，oracle error小于目标值的概率</li></ol></li></ol></li></ol></li></ol></li><li>创建模型，encoder和decoder<ol><li><strong>encoder</strong>是EncoderLSTM类，对navigation instruction进行embedding，并用lstm进行encode，返回hidden state、用于decoder初始化的一个state、以及cell state<ol><li>hidden和cell state初始化都是0</li><li>forward过程是embeedding-&gt;dropout-&gt;init-&gt;pack and pad-&gt;lstm-&gt;得到h_t和c_t-&gt;linear层处理hidden state再加上tanh得到decoder init state-&gt; pad and packed sequence-&gt; dropout-&gt; return ctx，decoder init，cell state</li></ol></li><li><strong>decoder</strong>是AttnDecoderLSTM类<ol><li>forward过程是：对action进行embedding-&gt;concat action embedding和feature -&gt; dropout -&gt; lstm -&gt; dropout -&gt; attention layer得到经过dot attention得到的h_tilde和attention权重 -&gt; h_tilde通过linear得到logit</li></ol></li></ol></li><li>训练过程train()<ol><li>agent用<strong>Seq2SeqAgent</strong>，基于seq2seq和attention和LSTM的agent<ol><li>用三维元组表示每个方向</li><li>feedback可选teacher，argmax，sample</li><li>初始化的encoder和decoder就是之前创建好的</li></ol></li><li>迭代过程<ol><li><strong>Seq2SeqAgent.train()</strong><ol><li>encoder.train()</li><li>decoder.train()</li><li>n_iter里面<ol><li>optimizer梯度置0</li><li><strong>rollout()</strong><ol><li>self.env.reset()，加载一个<strong>新的mini batch数据</strong></li><li>把输入，按照每个observation中的<strong>instructions</strong>的长度降序排序，方便padding</li><li>记录开始的observation信息</li><li><strong>encoder</strong>得到context state和hidden state cell state</li><li><strong>初始化</strong>start action和ended 标识，都是batch个</li><li>用encoder得到的context state, hidden state, cell state和每个observation里面的feature，输入到<strong>decoder</strong>中得到输出的hidden state，cell state, attention权重，和logit</li><li>对无法forward的部分进行<strong>mask</strong>，即把logit[idx, index of forward action]置为负无穷</li><li>_teacher_action()，提取ground truth的agent的方向信息，保存在<strong>target</strong>中</li><li>用decoder得到的logit和target计算<strong>交叉熵损失</strong></li><li>根据feedback策略获得a_t变量，即<strong>action target</strong><ol><li>teacher force策略，action target是ground truth</li><li>student force策略，action target是logit的argmax</li><li>sample策略，是按照概率对logit结果采样</li></ol></li><li><strong>更新结束标志</strong>，如果结束了，后面agent就不用再继续了</li><li>对所有的observation进行遍历，如果没有end就<strong>更新traj路径</strong>，即<code>traj[i][&#39;path&#39;].append((ob[&#39;viewpoint&#39;], ob[&#39;heading&#39;], ob[&#39;elevation&#39;]))</code></li><li>如果所有的agent都end了，就不用到下一个场景了，否则就继续<strong>下一个场景</strong></li><li>最后所有场景迭代完，保存每个场景的平均损失，并且返回agent的<strong>轨迹traj</strong></li></ol></li><li>loss反向传播</li><li>optimizer.step()</li></ol></li></ol></li><li>记录loss等</li><li>进行<strong>validation</strong><ol><li>agent.test(use_dropout=True)，保持和训练时的环境一样，即encoder,decoder都train()，再进行test()<ol><li>这里的test调用了BaseAgent的test()</li><li>**reset_epoch()**，重置self.id为1，即data index变成了epoch开始那会的index</li><li>一个looped标志，记录测试集是否跑完一遍<ol><li>只有在出现相同的instr_id时，才回退出循环，而想要出现相同的instr_id，就得遍历完一遍测试集</li><li>因为rollout函数里每次都会进行一个<code>self.env.reset()</code>，这个地方会进行<code>_next_minibatch()</code></li><li><code>_next_minibatch()</code>会在剩下的data不足一个batch时，shuffle所有的data，然后继续采样</li></ol></li></ol></li><li>agent.test(use_dropout=False)，encoder和decoder都eval()，再test()</li><li>记录loss和metric</li></ol></li></ol></li><li><code>agent.env=train_env</code></li><li>记录log，保存checkpoint</li></ol></li></ol><h2 id="模型验证"><a href="#模型验证" class="headerlink" title="模型验证"></a>模型验证</h2><ol><li>程序入口：<code>eval.py/eval_simple_agents()</code></li><li>对每个split进行验证，train, val_seen, val_unseen, test<ol><li>创建环境 <code>env=R2RBatch()</code>，细节见<a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">模型训练</a></li><li>创建evaluator，<code>ev=Evaluation()</code>，细节见<a href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83">模型训练</a></li><li>遍历不同类型的agent，StopAgent，ShortestAgent，RandomAgent<ol><li>创建上面指定的agent</li><li>agent.test()</li></ol></li></ol></li></ol><h2 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h2><h2 id="计划"><a href="#计划" class="headerlink" title="计划"></a>计划</h2><ol><li>11月的CVPR，至少投1篇</li><li>9月打基础<ol><li>一周泛读，总结所有的模型方法和点</li><li>每个人每周精读一篇论文，做ppt汇报，并研究源码</li><li>另外精读对方汇报的那篇文章</li></ol></li><li>10月份做实验</li><li>11月写文章，11.9截止注册，11.16截稿</li><li></li></ol>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;论文《&lt;a href=&quot;https://arxiv.org/abs/1711.07280&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments&lt;/a&gt;》是VLN的开篇之作，这里记录下对它在R2R任务里面的源码理解。&lt;/p&gt;&lt;p&gt;github仓库地址：&lt;a href=&quot;https://github.com/peteanderson80/Matterport3DSimulator/tree/master/tasks/R2R&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;VLN-R2R&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="AllenNLP" scheme="https://hanielxx.com/tags/AllenNLP/"/>
    
    <category term="NLP" scheme="https://hanielxx.com/tags/NLP/"/>
    
    <category term="OpenLibrary" scheme="https://hanielxx.com/tags/OpenLibrary/"/>
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="Pytorch" scheme="https://hanielxx.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>UCAS自然语言处理</title>
    <link href="https://hanielxx.com/Notes/2021-07-05-ucas-nlp-review"/>
    <id>https://hanielxx.com/Notes/2021-07-05-ucas-nlp-review</id>
    <published>2021-07-05T05:34:18.000Z</published>
    <updated>2021-07-05T17:40:50.384Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>国科大自然语言处理（宗成庆）笔记</p></div><a id="more"></a><h2 id="期末提示"><a href="#期末提示" class="headerlink" title="期末提示"></a>期末提示</h2><ul><li>并列句计算，好像是边界距离计算，好像是编辑距离计算</li><li>概念弄清楚</li><li>分类 聚类 评价的指标</li><li>句法分析<ul><li>依存句法分析</li><li>怎么评价性能</li></ul></li><li>分词<ul><li>分词错误类型</li><li>汉语分词</li><li>并列句计算</li></ul></li><li>机器翻译怎么度量译文质量</li></ul><h2 id="数学基础"><a href="#数学基础" class="headerlink" title="数学基础"></a>数学基础</h2><ol><li>熵<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.71ex" xmlns="http://www.w3.org/2000/svg" width="29.224ex" height="2.406ex" role="img" focusable="false" viewBox="0 -750 12917 1063.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(888, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1277, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2129, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2795.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3851.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munder" transform="translate(4796.2, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1239, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g><g data-mml-node="mi" transform="translate(7547.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(8050.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8439.4, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(9011.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(9400.4, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(9698.4, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(10183.4, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mn" transform="translate(477, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(11064, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(11567, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(11956, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(12528, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="9.261ex" height="2.034ex" role="img" focusable="false" viewBox="0 -694 4093.6 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(798, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1283, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mn" transform="translate(1760, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2537.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(3593.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container></li><li>单位为二进制位比特 (bit)</li><li>熵又称为自信息(self-information)，表示信源X每发一个信号所提供的平均信息量</li><li>熵也可以被视为描述一个随机变量 的不确定性的数量。一个随机变量的熵越大， 它的不确定性越大。那么，正确估计其值的可 能性就越小</li></ol></li><li>联合熵<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.974ex" xmlns="http://www.w3.org/2000/svg" width="42.136ex" height="2.67ex" role="img" focusable="false" viewBox="0 -750 18624.3 1180.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(888, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1277, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2129, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2573.7, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g><g data-mml-node="mo" transform="translate(3336.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4003.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(5059.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munder" transform="translate(6003.9, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1239, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g><g data-mml-node="munder" transform="translate(8755.1, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(490, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1157, 0)"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g></g></g><g data-mml-node="mi" transform="translate(11385.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(11888.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(12277.4, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(12849.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(13294.1, 0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(13784.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(14173.1, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(14471.1, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(14956.1, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mn" transform="translate(477, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(15836.6, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(16339.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(16728.6, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(17300.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(17745.3, 0)"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(18235.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>就是描述一对随机变量平均所需要的信息量</li></ol></li><li>条件熵<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4KtAKn.png" alt="4KtAKn"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oKA1so.png" alt="oKA1so"></div></li><li>可以不用直接求条件熵，而是通过连锁法则算</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/IpeTzR.png" alt="IpeTzR"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/cDZ51B.png" alt="cDZ51B"></div></li></ol></li><li>熵率<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n6X5IA.png" alt="n6X5IA"></div></li></ol></li><li>相对熵、KL散度、KL距离、Kullback-Leibler divergence<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9LzsTS.png" alt="9LzsTS"></div></li><li>注意这个前面没有负号</li></ol></li><li>交叉熵<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/S86Ck0.png" alt="S86Ck0"></div></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex" xmlns="http://www.w3.org/2000/svg" width="31.984ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 14137.1 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">交</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">叉</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">熵</text><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" transform="translate(2700, 0)"></path><text data-variant="normal" transform="translate(3478, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">相</text><text data-variant="normal" transform="translate(4378, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">对</text><text data-variant="normal" transform="translate(5278, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">熵</text><text data-variant="normal" transform="translate(6178, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">（</text></g><g data-mml-node="mi" transform="translate(7355.8, 0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(8244.8, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(9203.6, 0)"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">散</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">度</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">）</text></g><g data-mml-node="mo" transform="translate(12181.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(13237.1, 0)"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">熵</text></g></g></g></svg></mjx-container></li></ol></li><li>困惑度<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hrvNb3.png" alt="hrvNb3"></div></li></ol></li><li>互信息<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nTSQpH.png" alt="nTSQpH"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EIAbKt.png" alt="EIAbKt"></div></li></ol></li><li>双字耦合度<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/leizbn.png" alt="leizbn"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oyMOUX.png" alt="oyMOUX"></div></li></ol></li><li>噪声信道模型<ol><li>在信号传输的过程中都要进行双重性处理：一方面要 通过压缩消除所有的冗余，另一方面又要通过增加一定的可控冗余以保障输入信号经过噪声信道后可以很好地恢复原状。信息编码时要尽量占用少量的空间，但又必须保持 足够的冗余以便能够检测和校验错误。接收到的信号需要 被解码使其尽量恢复到原始的输入信号</li><li>噪声信道模型的目标就是优化噪声信道中信号传输的 吞吐量和准确率，其基本假设是一个信道的输出以一定的 概率依赖于输入。</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lfYUEI.png" alt="lfYUEI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VdpCaw.png" alt="VdpCaw"></div></li></ol></li><li>编辑距离<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zsqOCq.png" alt="zsqOCq"></div></li></ol></li></ol><h2 id="形式语言与自动机"><a href="#形式语言与自动机" class="headerlink" title="形式语言与自动机"></a>形式语言与自动机</h2><h3 id="形式语言定义"><a href="#形式语言定义" class="headerlink" title="形式语言定义"></a>形式语言定义</h3><ol><li>形式语言定义<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/A8sYHd.png" alt="A8sYHd"></div></li></ol></li><li>形式语言推导<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0GJf8D.png" alt="0GJf8D"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Iqc5mG.png" alt="Iqc5mG"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/L1ZdXY.png" alt="L1ZdXY"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aue0Ob.png" alt="aue0Ob"></div></li></ol></li><li>文法生成语言L(G)<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3gwO1G.png" alt="3gwO1G"></div></li></ol></li></ol><h3 id="乔姆斯基四类文法"><a href="#乔姆斯基四类文法" class="headerlink" title="乔姆斯基四类文法"></a>乔姆斯基四类文法</h3><ol><li>正则文法(regular grammar, RG), 或称 3型文法</li><li>上下文无关文法(context-free grammar, CFG), 或称2型文法</li><li>上下文有关文法(context-sensitive grammar, CSG), 或称1型文法</li><li>无约束文法(unrestricted grammar)，或称0型文法</li></ol><p>如果一种语言能由几种文法所产生，则把这种语言 称为在这几种文法中受限制最多的那种文法所产生的 语言</p><h4 id="正则文法"><a href="#正则文法" class="headerlink" title="正则文法"></a>正则文法</h4><p>3型文法</p><ol><li>定义<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0639Xc.png" alt="0639Xc"></div></li></ol></li><li>转换为正则文法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ILPEph.png" alt="ILPEph"></div></li></ol></li></ol><h4 id="上下文无关文法"><a href="#上下文无关文法" class="headerlink" title="上下文无关文法"></a>上下文无关文法</h4><p>2型文法</p><ol><li>定义<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1Z3Z4X.png" alt="1Z3Z4X"></div></li></ol></li><li>派生树<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oxvfYX.png" alt="oxvfYX"></div></li></ol></li><li>二义性<ol><li>一个文法 G，如果存在某个句子有不只一棵分析树与之对应，那么称这个文法是二义的</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DUvLQ1.png" alt="DUvLQ1"></div></li></ol></li></ol><h4 id="上下文有关文法"><a href="#上下文有关文法" class="headerlink" title="上下文有关文法"></a>上下文有关文法</h4><p>1型文法</p><ol><li>定义<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xz0b0e.png" alt="xz0b0e"></div></li></ol></li></ol><h4 id="无约束文法"><a href="#无约束文法" class="headerlink" title="无约束文法"></a>无约束文法</h4><p>0型文法</p><ol><li>定义<ol><li>、<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Ej9BQC.png" alt="Ej9BQC"></div></li></ol></li></ol><h3 id="自动机"><a href="#自动机" class="headerlink" title="自动机"></a>自动机</h3><p>自动机是有穷地表示无穷语言的另一种方法。每一 个语言的句子都能被某种自动机所接受。</p><ul><li>有限(状态)自动机(finite automata, FA)，用的最多，对应正则文法（3型）</li><li>下推自动机(push-down automata, PDA)，对应上下文无关文法（2型）</li><li>线性带限自动机(linear bounded automata)，对应上下文有关文法（1型）</li><li>图灵机(Turing Machine)，没有实际意义，一般不用，对应无约束文法（0型）</li></ul><h4 id="有限状态自动机"><a href="#有限状态自动机" class="headerlink" title="有限状态自动机"></a>有限状态自动机</h4><ol><li>确定性有限自动机(deterministic finite automata, DFA)<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0jL23K.png" alt="0jL23K"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rLJknI.png" alt="rLJknI"></div></li><li>终止状态用双圈表示，起始 状态用有“开始”标记的箭头表示</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/baKvCp.png" alt="baKvCp"></div></li></ol></li><li>非确定性有限自动机(non-deterministic finite automata, NFA)<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/G0YXAd.png" alt="G0YXAd"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BncGqs.png" alt="BncGqs"></div></li></ol></li></ol><h3 id="正则文法和有限自动机关系"><a href="#正则文法和有限自动机关系" class="headerlink" title="正则文法和有限自动机关系"></a>正则文法和有限自动机关系</h3><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mzcl0N.png" alt="mzcl0N"></div></li><li>文法转换为自动机<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9coCJG.png" alt="9coCJG"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wzN1fx.png" alt="wzN1fx"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Tb6Sj7.png" alt="Tb6Sj7"></div></li></ol></li><li>自动机转文法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rXD7HI.png" alt="rXD7HI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nR45tw.png" alt="nR45tw"></div></li><li>转换结果<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="18.143ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 8019.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(1063.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2119.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(2508.6, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="mo" transform="translate(3769.5, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4214.1, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(5344.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5789.6, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(6540.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(6985.3, 0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(7630.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="10.032ex" height="2.059ex" role="img" focusable="false" viewBox="0 -716 4434.1 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="mo" transform="translate(1538.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2594.5, 0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(645, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1089.7, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="8.844ex" height="1.984ex" role="img" focusable="false" viewBox="0 -683 3909 877"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(1408.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2464.4, 0)"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(944.7, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex" xmlns="http://www.w3.org/2000/svg" width="23.096ex" height="2.26ex" role="img" focusable="false" viewBox="0 -749.5 10208.3 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1028.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2084.6, 0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(922.8, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2200.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mi" transform="translate(2700.6, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(3450.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3895.2, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(4923, 0)"><path data-c="2192" d="M56 237T56 250T70 270H835Q719 357 692 493Q692 494 692 496T691 499Q691 511 708 511H711Q720 511 723 510T729 506T732 497T735 481T743 456Q765 389 816 336T935 261Q944 258 944 250Q944 244 939 241T915 231T877 212Q836 186 806 152T761 85T740 35T732 4Q730 -6 727 -8T711 -11Q691 -11 691 0Q691 7 696 25Q728 151 835 230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(6200.8, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(6700.8, 0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(7345.8, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(7623.8, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container></li></ol></li></ol></li></ol><h2 id="语料库与语言知识库"><a href="#语料库与语言知识库" class="headerlink" title="语料库与语言知识库"></a>语料库与语言知识库</h2><ol><li>语料库定义(corpus)<ul><li>用于存放语言数据的文件(语言数据库)。</li></ul></li><li>语料库语言学(corpus linguistics)<ul><li>基于语料库进行的语言学研究。研究自然语言文本的采集、存储、检索、统计、 词性和句法及语义信息的标注、以及具有上述功能的 语料库在语言定量分析、词典编纂、作品风格分析和 人类语言技术等领域中的应用。</li></ul></li><li>类型<ol><li>按内容构成和目的划分<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pWzZ3b.png" alt="pWzZ3b"></div></li></ol></li><li>按语言种类划分和标注<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/d78IOu.png" alt="d78IOu"></div></li></ol></li><li>平衡语料库<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3YS3YK.png" alt="3YS3YK"></div></li></ol></li><li>平行语料库<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xRQ2cf.png" alt="xRQ2cf"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/73e7ia.png" alt="73e7ia"></div></li></ol></li><li>共时语料库和历时语料库<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QnLIjp.png" alt="QnLIjp"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hXgxk0.png" alt="hXgxk0"></div></li></ol></li></ol></li><li>问题和现状<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/J9oFKI.png" alt="J9oFKI"></div></li></ol></li></ol><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h3 id="传统语言模型"><a href="#传统语言模型" class="headerlink" title="传统语言模型"></a>传统语言模型</h3><h4 id="n元文法"><a href="#n元文法" class="headerlink" title="n元文法"></a>n元文法</h4><ol><li>计算语句先验<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mer5Zp.png" alt="mer5Zp"></div></li></ol></li><li>历史基元过多问题<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HLHSqa.png" alt="HLHSqa"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hGyUpt.png" alt="hGyUpt"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oOPlHf.png" alt="oOPlHf"></div></li></ol></li><li>定义<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WEyp5V.png" alt="WEyp5V"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/77bC61.png" alt="77bC61"></div></li></ol></li><li>举例<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Fo0Hvv.png" alt="Fo0Hvv"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/2eXJGf.png" alt="2eXJGf"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BTWGv1.png" alt="BTWGv1"></div></li></ol></li></ol><h4 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NLoTjl.png" alt="NLoTjl"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/iam1RN.png" alt="iam1RN"></div></li><li>问题是，存在数据匮乏的情况，导致一些概率算出来是0， 因此要进行数据平滑</li></ol><h4 id="数据平滑"><a href="#数据平滑" class="headerlink" title="数据平滑"></a>数据平滑</h4><ol><li>基本思想<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OSdqEw.png" alt="OSdqEw"></div></li></ol></li><li>加一法<ol><li>其实就是分子加1，分母加上所有的类（词汇表V大小）</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tpr5BE.png" alt="tpr5BE"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Yoz8gY.png" alt="Yoz8gY"></div></li></ol></li><li>减值法/折扣法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zawpfF.png" alt="zawpfF"></div></li><li>自己查下，考试应该不考</li><li>Good-Turning估计法</li><li>后退法</li><li>绝对减值法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UlFscN.png" alt="UlFscN"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/vzTlnV.png" alt="vzTlnV"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CK8Vib.png" alt="CK8Vib"></div></li></ol></li><li>线性减值法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8DZbbq.png" alt="8DZbbq"></div></li></ol></li><li>四种减值法的比较<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0S2XO2.png" alt="0S2XO2"></div></li></ol></li></ol></li><li>删除插值法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Use3hq.png" alt="Use3hq"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3sSHtC.png" alt="3sSHtC"></div></li></ol></li></ol><h4 id="语言模型自适应"><a href="#语言模型自适应" class="headerlink" title="语言模型自适应"></a>语言模型自适应</h4><ol><li>方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PbjjmG.png" alt="PbjjmG"></div></li></ol></li></ol><h4 id="n元文法应用"><a href="#n元文法应用" class="headerlink" title="n元文法应用"></a>n元文法应用</h4><h5 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h5><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/h4gTu2.png" alt="h4gTu2"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/GohKle.png" alt="GohKle"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UADc8i.png" alt="UADc8i"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fl3r5s.png" alt="fl3r5s"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/SLFH06.png" alt="SLFH06"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4uGIp3.png" alt="4uGIp3"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/SlAJ61.png" alt="SlAJ61"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8PRkKj.png" alt="8PRkKj"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QyZzEE.png" alt="QyZzEE"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pGVDXG.png" alt="pGVDXG"></div></li></ol><h5 id="分词与词性标注一体化方法"><a href="#分词与词性标注一体化方法" class="headerlink" title="分词与词性标注一体化方法"></a>分词与词性标注一体化方法</h5><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JQf7bx.png" alt="JQf7bx"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NmF697.png" alt="NmF697"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NyBggw.png" alt="NyBggw"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RbxiwB.png" alt="RbxiwB"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/SixCcQ.png" alt="SixCcQ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XXVw6J.png" alt="XXVw6J"></div></li></ol><h3 id="神经语言模型"><a href="#神经语言模型" class="headerlink" title="神经语言模型"></a>神经语言模型</h3><ol><li>传统语言模型的问题<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Nb2IDK.png" alt="Nb2IDK"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WtXBRs.png" alt="WtXBRs"></div></li></ol></li></ol><h4 id="前馈神经网络语言模型"><a href="#前馈神经网络语言模型" class="headerlink" title="前馈神经网络语言模型"></a>前馈神经网络语言模型</h4><p>这部分省略</p><h4 id="循环神经网络语言模型"><a href="#循环神经网络语言模型" class="headerlink" title="循环神经网络语言模型"></a>循环神经网络语言模型</h4><p>RNN和LSTM，以及Self-attention的内容，也省略</p><p>附录有BP算法的推导，可以自己看</p><h3 id="文本表示"><a href="#文本表示" class="headerlink" title="文本表示"></a>文本表示</h3><h4 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h4><ol><li>概念<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QD0jV4.png" alt="QD0jV4"></div></li></ol></li><li>特征项<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8F6kjL.png" alt="8F6kjL"></div></li></ol></li><li>特征项权重：词频，TF-IDF，逆文档频率<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5ZkYOc.png" alt="5ZkYOc"></div></li></ol></li><li>规范化Normalization<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TPch3B.png" alt="TPch3B"></div></li></ol></li></ol><h4 id="分布式表示"><a href="#分布式表示" class="headerlink" title="分布式表示"></a>分布式表示</h4><h5 id="表示学习模型"><a href="#表示学习模型" class="headerlink" title="表示学习模型"></a>表示学习模型</h5><ol><li>文本概念表示模型：以（概率）潜在语义分析和潜在狄利克雷分布为代表的主题模型，旨在挖掘 文本中的隐含主题或概念，文本将被表示为主题 的分布向量<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wHrikn.png" alt="wHrikn"></div></li></ol></li><li>深度表示学习模型：通过深度学习模型以最优化特定目标函数（例如语言模型似然度）的方式在 分布式向量空间中学习文本的低维实数向量表示</li></ol><h5 id="词语的表示学习"><a href="#词语的表示学习" class="headerlink" title="词语的表示学习"></a>词语的表示学习</h5><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nMq1HE.png" alt="nMq1HE"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/6zzDLR.png" alt="6zzDLR"></div></li><li><p>NNLM模型</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7xuDs1.png" alt="7xuDs1"></div></li></ol></li><li><p>C&amp;W模型</p><ol><li>中心词进行随机替换</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/iGmEOz.png" alt="iGmEOz"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Z6lwRb.png" alt="Z6lwRb"></div></li></ol></li><li><p>word2vec: CBOW和Skip-Gram</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/T0DfuB.png" alt="T0DfuB"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/cnuyDy.png" alt="cnuyDy"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ETseu1.png" alt="ETseu1"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1kFxdA.png" alt="1kFxdA"></div></li></ol></li><li><p>Glove模型</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/GgkdYe.png" alt="GgkdYe"></div></li><li>太多了，省略</li></ol></li><li><p>负采样和噪声对比估计</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/cAN7bM.png" alt="cAN7bM"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UXoSDI.png" alt="UXoSDI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TIYeVg.png" alt="TIYeVg"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/74GXZu.png" alt="74GXZu"></div></li><li>负采样技术的目标函数与噪声对比估计相同，但是不同于<br>噪声对比估计方法，负采样技术不对样本集合进行概率归<br>一化，而直接采用神经网络语言模型输出</li></ol></li><li><p>字词混合的表示学习</p><ol><li>词语由字或字符构成，一方面词语作为不可分割的单元可以获得一个表示；另一方面词语作为字的组合，通过字的表示也可以获得一个表示；两种表示结合得到更优表示</li><li>在低维、稠密的实数向量空间中，相似的词聚集在一起， 在相同的历史上下文中具有相似的概率分布</li></ol></li></ol><h5 id="短语的表示学习"><a href="#短语的表示学习" class="headerlink" title="短语的表示学习"></a>短语的表示学习</h5><ol><li>词袋模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dygNUx.png" alt="dygNUx"></div></li></ol></li><li>递归自编码器<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/bbp65q.png" alt="bbp65q"></div></li></ol></li><li>双语约束模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RILezO.png" alt="RILezO"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ornsRQ.png" alt="ornsRQ"></div></li></ol></li></ol><h5 id="句子的表示学习"><a href="#句子的表示学习" class="headerlink" title="句子的表示学习"></a>句子的表示学习</h5><ol><li>词袋模型<ol><li>平均</li><li>加权平均</li></ol></li><li>PV-DM模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aGkCb4.png" alt="aGkCb4"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/sipIq0.png" alt="sipIq0"></div></li></ol></li><li>PV-DBOW模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fD86KP.png" alt="fD86KP"></div></li></ol></li><li>Skip-Thought模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/B98gwn.png" alt="B98gwn"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/FIzhtA.png" alt="FIzhtA"></div></li></ol></li><li>CNN模型<ol><li>对于一个句子，卷积神经网络CNN以每个词的词向量为输入，通过顺 序地对上下文窗口进行卷积（Convolution）总结局部信息，并利用池 化层（Pooling）提取全局的重要信息，再经过其他网络层（卷积池化 层、Dropout层、线性层等），得到固定维度的句子向量表达，以刻 画句子全局性的语义信息</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/IBrKZV.png" alt="IBrKZV"></div></li></ol></li></ol><h5 id="文档的表示学习"><a href="#文档的表示学习" class="headerlink" title="文档的表示学习"></a>文档的表示学习</h5><ol><li>词袋模型</li><li>CNN模型</li><li>层次化模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aYw5C1.png" alt="aYw5C1"></div></li></ol></li></ol><h3 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h3><h4 id="ELMo预训练模型"><a href="#ELMo预训练模型" class="headerlink" title="ELMo预训练模型"></a>ELMo预训练模型</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QOtrjw.png" alt="QOtrjw"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kvikby.png" alt="kvikby"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oZ0Bd7.png" alt="oZ0Bd7"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Piw4Jo.png" alt="Piw4Jo"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fcW8KX.png" alt="fcW8KX"></div></li></ol><h3 id="GPT预训练模型"><a href="#GPT预训练模型" class="headerlink" title="GPT预训练模型"></a>GPT预训练模型</h3><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F9zWTP.png" alt="F9zWTP"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oRFTOw.png" alt="oRFTOw"></div></li><li>下游任务<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eBgaSI.png" alt="eBgaSI"></div></li><li>分类，对最后一个hidden进行变换后softmax</li></ol></li><li>优点：<ol><li>任务统一: 大部分语言处理任务形式化为语言模型任务</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hu0T7Z.png" alt="hu0T7Z"></div></li></ol></li></ol><h3 id="BERT预训练模型"><a href="#BERT预训练模型" class="headerlink" title="BERT预训练模型"></a>BERT预训练模型</h3><p>参考以前的博客<a href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis">BERT-预训练源码理解</a>和<a href="https://hanielxx.com/Papers/2020-08-15-attention-is-all-you-need">论文笔记 | Attention Is All You Need</a></p><h2 id="概率图模型"><a href="#概率图模型" class="headerlink" title="概率图模型"></a>概率图模型</h2><p>概率图模型的演变</p><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BG66M9.png" alt="BG66M9"></div><h3 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h3><ol><li>描述<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1EuLfN.png" alt="1EuLfN"></div></li></ol></li><li>假设<ol><li>当前状态只和前一个状态有关</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/KomjQi.png" alt="KomjQi"></div></li><li>状态的转移和时间t无关</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7COEYj.png" alt="7COEYj"></div></li></ol></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Qdfobv.png" alt="Qdfobv"></div></li><li>和NFA关系<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JrlYTh.png" alt="JrlYTh"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/M4j6eH.png" alt="M4j6eH"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/L14DVB.png" alt="L14DVB"></div></li></ol></li></ol><h3 id="隐马尔科夫模型HMM"><a href="#隐马尔科夫模型HMM" class="headerlink" title="隐马尔科夫模型HMM"></a>隐马尔科夫模型HMM</h3><h4 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YwPViM.png" alt="YwPViM"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HJPBvn.png" alt="HJPBvn"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NV7Zik.png" alt="NV7Zik"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DaQLAl.png" alt="DaQLAl"></div></li></ol><h4 id="核心问题"><a href="#核心问题" class="headerlink" title="核心问题"></a>核心问题</h4><h5 id="给定HMM求观察序列"><a href="#给定HMM求观察序列" class="headerlink" title="给定HMM求观察序列"></a>给定HMM求观察序列</h5><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/awVdVr.png" alt="awVdVr"></div></li></ol><h5 id="快速计算观察序列概率-p-O-mu"><a href="#快速计算观察序列概率-p-O-mu" class="headerlink" title="快速计算观察序列概率 $p(O|\mu)$"></a>快速计算观察序列概率 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.618ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2925 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(1655, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1933, 0)"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g><g data-mml-node="mo" transform="translate(2536, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></h5><p>前向算法和后向算法</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/s0BNax.png" alt="s0BNax"></div></li><li>上面的方法，会导致有 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="3.371ex" height="1.904ex" role="img" focusable="false" viewBox="0 -841.7 1490.1 841.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(942.3, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>个状态序列，指数级组合爆炸</li></ol><p>前向算法</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wHyhPU.png" alt="wHyhPU"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/6jeAb4.png" alt="6jeAb4"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1h6HH5.png" alt="1h6HH5"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/22ywb4.png" alt="22ywb4"></div></li><li>时间复杂度：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.124ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 3590.8 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mn" transform="translate(942.3, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(2497.8, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(3201.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol><p>后向算法</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/C3gMGt.png" alt="C3gMGt"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/K1e68X.png" alt="K1e68X"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eaWfbe.png" alt="eaWfbe"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Vnn6Az.png" alt="Vnn6Az"></div></li><li>时间复杂度：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.124ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 3590.8 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mn" transform="translate(942.3, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(2497.8, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(3201.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol><h4 id="发现最优状态序列"><a href="#发现最优状态序列" class="headerlink" title="发现最优状态序列"></a>发现最优状态序列</h4><p>如何发现“最优”状态序列能够“最好地解释”观察序列？</p><p>维特比算法</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Lx7HWu.png" alt="Lx7HWu"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UgUT9u.png" alt="UgUT9u"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xtnKV1.png" alt="xtnKV1"></div></li></ol><h4 id="模型参数学习"><a href="#模型参数学习" class="headerlink" title="模型参数学习"></a>模型参数学习</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/62tZpf.png" alt="62tZpf"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hmouIt.png" alt="hmouIt"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Uitu5d.png" alt="Uitu5d"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qVapaE.png" alt="qVapaE"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UUpMQb.png" alt="UUpMQb"></div></li></ol><h3 id="HMM应用"><a href="#HMM应用" class="headerlink" title="HMM应用"></a>HMM应用</h3><ol><li>分词</li><li>词性标注</li></ol><h3 id="条件随机场模型CRF"><a href="#条件随机场模型CRF" class="headerlink" title="条件随机场模型CRF"></a>条件随机场模型CRF</h3><p>省略</p><h2 id="形态分析、汉语分词与词性标注"><a href="#形态分析、汉语分词与词性标注" class="headerlink" title="形态分析、汉语分词与词性标注"></a>形态分析、汉语分词与词性标注</h2><ol><li>词性标注（消岐）中的并列鉴别规则</li><li>标出命名实体，说明类型</li></ol><h3 id="英语形态分析"><a href="#英语形态分析" class="headerlink" title="英语形态分析"></a>英语形态分析</h3><ol><li>有规律变化单词的形态还原</li><li>动词、名词、形容词、副词不规则变化单词的形态还原</li><li>对于表示年代、时间、百分数、货币、序数词的数字形态还原</li><li>合成词的形态还原</li></ol><p>形态分析的一般方法</p><ol><li>查词典，如果词典中有该词，直接确定该词的原形；</li><li>根据不同情况查找相应规则对单词进行还原处理，如果还原后在词典中找到该词，则得到该词的原形；如果找不到相 应变换规则或者变换后词典中仍查不到该词，则作为未登 录词处理；</li><li>进入未登录词处理模块。</li></ol><h3 id="汉语自动分词"><a href="#汉语自动分词" class="headerlink" title="汉语自动分词"></a>汉语自动分词</h3><ol><li>交集型歧义<ol><li>中国/ 人为/ 了/ 实现/ 自己/ 的/ 梦想</li><li>中国人/ 为了/ 实现/ 自己/ 的/ 梦想</li><li>中/ 国人/ 为了/ 实现/ 自己/ 的/ 梦想</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wK9RSZ.png" alt="wK9RSZ"></div></li></ol></li><li>组合型歧义<ol><li>门/ 把/ 手/ 弄/ 坏/ 了/ 。</li><li>门/ 把手/ 弄/ 坏/ 了/ 。</li></ol></li><li>分词基本原则<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wGvI8W.png" alt="wGvI8W"></div></li></ol></li><li>辅助原则<ol><li>div<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/agpOW5.png" alt="agpOW5"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nYwi51.png" alt="nYwi51"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/vDMQpc.png" alt="vDMQpc"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qweK8J.png" alt="qweK8J"></div></li></ol></li><li>评价指标<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rKPJRt.png" alt="rKPJRt"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F7e1Uy.png" alt="F7e1Uy"></div></li></ol></li></ol><h3 id="汉语自动分词方法"><a href="#汉语自动分词方法" class="headerlink" title="汉语自动分词方法"></a>汉语自动分词方法</h3><ol><li>最大匹配法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rXR7j4.png" alt="rXR7j4"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WekZiC.png" alt="WekZiC"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1b88gN.png" alt="1b88gN"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NBufKR.png" alt="NBufKR"></div></li></ol></li><li>.最少分词法(最短路径法) 有词典切分<ol><li>基本思想<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RiPvBo.png" alt="RiPvBo"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AH3HXc.png" alt="AH3HXc"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/L6UtHY.png" alt="L6UtHY"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/KTbTEd.png" alt="KTbTEd"></div></li></ol></li><li>基于语言模型的分词方法<ol><li>基本思路<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7BfEvd.png" alt="7BfEvd"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/okF7us.png" alt="okF7us"></div></li></ol></li><li>基于HMM的分词</li><li>由字构词 (基于字标注)的分词方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3KhY0L.png" alt="3KhY0L"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/FRgOGF.png" alt="FRgOGF"></div></li></ol></li><li>生成式和判别式模型区别<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aY38KN.png" alt="aY38KN"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rgywBR.png" alt="rgywBR"></div></li></ol></li><li>生成式方法与区分式方法的结合<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dOBzEQ.png" alt="dOBzEQ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gmaCw4.png" alt="gmaCw4"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CTPScM.png" alt="CTPScM"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/sJannE.png" alt="sJannE"></div></li></ol></li></ol><h3 id="未登录词"><a href="#未登录词" class="headerlink" title="未登录词"></a>未登录词</h3><ol><li>命名实体<ol><li>人名、地名、机构名、时间、日期、货币、百分比，数字如果在这些类别中的话就算命名实体，如果不在，通常也有单独的数字识别任务</li></ol></li><li>其他新词，如专业词汇和新的普通词</li></ol><h3 id="子词切分（BPE算法）"><a href="#子词切分（BPE算法）" class="headerlink" title="子词切分（BPE算法）"></a>子词切分（BPE算法）</h3><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rzE69U.png" alt="rzE69U"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kx3Lz7.png" alt="kx3Lz7"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/05EFmu.png" alt="05EFmu"></div></li></ol><p>例子</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VL7oRq.png" alt="VL7oRq"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qc9sb9.png" alt="qc9sb9"></div></li></ol><h3 id="词性标注"><a href="#词性标注" class="headerlink" title="词性标注"></a>词性标注</h3><ol><li>标注方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xfXoK9.png" alt="xfXoK9"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/sSc9uP.png" alt="sSc9uP"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8OUrEV.png" alt="8OUrEV"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OY6Zwp.png" alt="OY6Zwp"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/o8etvc.png" alt="o8etvc"></div></li></ol></li></ol><h2 id="句法分析"><a href="#句法分析" class="headerlink" title="句法分析"></a>句法分析</h2><h3 id="短语结构分析"><a href="#短语结构分析" class="headerlink" title="短语结构分析"></a>短语结构分析</h3><ul><li>基于CFG规则的分析方法<ul><li>线图分析法</li><li>CYK分析法</li></ul></li><li>基于PCFG的分析法</li><li>句法分析性能评估</li><li>局部句法分析</li></ul><h4 id="线图分析法"><a href="#线图分析法" class="headerlink" title="线图分析法"></a>线图分析法</h4><ol><li>三种策略<ol><li>自底向上 (Bottom-up)</li><li>从上到下 (Top-down)</li><li>从上到下和从下到上结合</li></ol></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NXf5ep.png" alt="NXf5ep"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LMmCOn.png" alt="LMmCOn"></div></li><li>例子<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/12g1PJ.png" alt="12g1PJ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/47efc0.png" alt="47efc0"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CsyLZ2.png" alt="CsyLZ2"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mWyZUn.png" alt="mWyZUn"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Q0h00l.png" alt="Q0h00l"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Kaam05.png" alt="Kaam05"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QUP4Ys.png" alt="QUP4Ys"></div></li></ol></li></ol><h4 id="CYK分析法"><a href="#CYK分析法" class="headerlink" title="CYK分析法"></a>CYK分析法</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/vRxA1l.png" alt="vRxA1l"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ND8JbD.png" alt="ND8JbD"></div></li></ol><p>例子</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NsyP3k.png" alt="NsyP3k"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/KqxED3.png" alt="KqxED3"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xhCf2y.png" alt="xhCf2y"></div></li></ol><h4 id="基于PCFG的分析法"><a href="#基于PCFG的分析法" class="headerlink" title="基于PCFG的分析法"></a>基于PCFG的分析法</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EHl9uU.png" alt="EHl9uU"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/sei9sx.png" alt="sei9sx"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zJze5n.png" alt="zJze5n"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xSxdzT.png" alt="xSxdzT"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/MxbpCo.png" alt="MxbpCo"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EVlYPo.png" alt="EVlYPo"></div></li></ol><h4 id="句法分析性能评估"><a href="#句法分析性能评估" class="headerlink" title="句法分析性能评估"></a>句法分析性能评估</h4><ol><li>精度precision</li><li>召回recall</li><li>F1分数</li><li>交叉括号数：（括号有重叠都算）：一棵分析树中与其他分析树中边界相交叉的成分个数的平均值。通常是对应位置的短语比较</li><li>交叉准确率</li><li>词性标注准确率</li></ol><h3 id="依存句法分析"><a href="#依存句法分析" class="headerlink" title="依存句法分析"></a>依存句法分析</h3><p>在依存语法理论中，“依存”就是指词与词之间支配与被 支配的关系，这种关系不是对等的，而是有方向的。处于支配 地位的成分称为支配者(governor, regent, head)，而处于被支配 地位的成分称为从属者(modifier, subordinate, dependency)。</p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oNfzPc.png" alt="oNfzPc"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AWOPEp.png" alt="AWOPEp"></div></li></ol><h4 id="依存句法四条公理"><a href="#依存句法四条公理" class="headerlink" title="依存句法四条公理"></a>依存句法四条公理</h4><p>(1) 一个句子只有一个独立的成分；即单一父结点(single headed)<br>(2) 句子的其他成分都从属于某一成分；即连通(connective)<br>(3) 任何一成分都不能依存于两个或多个成分；即无环(acyclic)<br>(4) 如果成分A直接从属于成分B，而成分C在句子中位于A和B<br>之间，那么，成分C或者从属于A，或者从属于B，或者从 属于A和B之间的某一成分；即可投射(projective)</p><h4 id="依存分析方法"><a href="#依存分析方法" class="headerlink" title="依存分析方法"></a>依存分析方法</h4><p>目前依存句法结构描述一般采用有向图方法或依存树方法，<br>所采用的句法分析算法可大致归为以下4类：</p><ul><li>生成式分析方法(generative parsing)</li><li>判别式分析方法(discriminative parsing)</li><li>决策式(确定性)分析方法(deterministic parsing)</li><li>基于约束满足的分析方法(constraint satisfaction parsing)</li></ul><p><strong>Arc-eager algorithmX算法</strong></p><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/V3tuqI.png" alt="V3tuqI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DHMGF0.png" alt="DHMGF0"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/H6bYpv.png" alt="H6bYpv"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pA2BGM.png" alt="pA2BGM"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZbW6wN.png" alt="ZbW6wN"></div></li></ol><p><strong>评价指标</strong></p><ol><li>无标记依存正确率(unlabeled attachment score, UA 或 UAS)：所有词中找到其正确支配词的词所占的百分比，没有找到支配词的词(即根结点)也算在内</li><li>带标记依存正确率(labeled attachment score, LA 或 LAS)：所有词中找到其正确支配词并且依存关系类型也标注正确的词所占的百分比，根结点也算在内。就是在UA里面找依存关系标注正确的</li><li>依存正确率(dependency accuracy, DA)：所有非根结点词中找到其正确支配词的词所占的百分比。就是UA的分子分母同时减去1</li><li>根正确率(root accuracy, RA)：有两种定义方式：<ul><li>(1)正确根结点的个数与句子个数的比值；</li><li>(2)所有句子中找到正确根结点的<strong>句子所占的百分比</strong>。</li><li>对单根结点语言或句子来说，二者是等价的。</li></ul></li><li>完全匹配率(complete match, CM)：所有句子中无标记依存结构完全正确的<strong>句子所占的百分比</strong></li></ol><h4 id="短语结构和依存关系转换"><a href="#短语结构和依存关系转换" class="headerlink" title="短语结构和依存关系转换"></a>短语结构和依存关系转换</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3UxmT2.png" alt="3UxmT2"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NYzV2G.png" alt="NYzV2G"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qdY0M5.png" alt="qdY0M5"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/oSZaem.png" alt="oSZaem"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/J70c86.png" alt="J70c86"></div></li></ol><h3 id="基于深度学习的句法分析"><a href="#基于深度学习的句法分析" class="headerlink" title="基于深度学习的句法分析"></a>基于深度学习的句法分析</h3><p>略</p><h3 id="英汉句法结构特点对比"><a href="#英汉句法结构特点对比" class="headerlink" title="英汉句法结构特点对比"></a>英汉句法结构特点对比</h3><p>略</p><h2 id="语义分析"><a href="#语义分析" class="headerlink" title="语义分析"></a>语义分析</h2><h3 id="语义网络的概念"><a href="#语义网络的概念" class="headerlink" title="语义网络的概念"></a>语义网络的概念</h3><p>语义网络通过由实体、概念或动作、状态及语义关系组成的 有向图来表达知识、描述语义。</p><ol><li>有向图：图的结点表示实体或概念，图的边表示实体或概念<br>之间的关系。</li><li>边的类型：<ul><li>“是一种抽象(IS-A)”: A到B的边表示“A是B的一种特例”;</li><li>“是一部分(PART-OF)”: A到B的边表示“A是B的一部分”;</li><li>“是属性(IS)”：A到B的边表示“A是B的一种属性”；</li><li>“拥有/占用(HAVE)”: A到B的边表示 “A拥有B”；</li><li>“次序在先/前(BEFORE)”: A到B的边表示 “A在B之前”</li></ul></li><li>事件的语义网络表示<ol><li>当语义网络表示事件时，结点之间的关系可以是施事、受事、 时间等。这里所说的“事件”指某个具体的动作或状态，并非 我们日常生活中所说的事件。</li></ol></li><li>事件的语义关系<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HBWoCI.png" alt="HBWoCI"></div></li></ol></li></ol><h3 id="词义消岐"><a href="#词义消岐" class="headerlink" title="词义消岐"></a>词义消岐</h3><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fT6XwB.png" alt="fT6XwB"></div></li><li>其他略</li></ol><h3 id="语义角色标注"><a href="#语义角色标注" class="headerlink" title="语义角色标注"></a>语义角色标注</h3><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/6C3LxP.png" alt="6C3LxP"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/P4avHL.png" alt="P4avHL"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PfH4JQ.png" alt="PfH4JQ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dIiHO8.png" alt="dIiHO8"></div></li></ol><h2 id="语义的表征与解码"><a href="#语义的表征与解码" class="headerlink" title="语义的表征与解码"></a>语义的表征与解码</h2><p>略</p><h2 id="篇章分析"><a href="#篇章分析" class="headerlink" title="篇章分析"></a>篇章分析</h2><p>略</p><h2 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h2><ol><li>问题和现状<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nPIqor.png" alt="nPIqor"></div></li></ol></li><li>基本观点<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EbHxCA.png" alt="EbHxCA"></div></li></ol></li></ol><h3 id="基本翻译方法"><a href="#基本翻译方法" class="headerlink" title="基本翻译方法"></a>基本翻译方法</h3><ol><li>直接转换法<ol><li>从源语言句子的表层出发，将单词、短语或句子 直接置换成目标语言译文，必要时进行简单的词序调 整。对原文句子的分析仅满足于特定译文生成的需要。 这类翻译系统一般针对某一个特定的语言对，将分析 与生成、语言数据、文法和规则与程序等都融合在一 起</li></ol></li><li>基于规则的翻译方法<ol><li>对源语言和目标语言均进行适当描述、把翻译机制与 语法分开、用规则描述语法的实现思想，这就是基于 规则的翻译方法</li><li>步骤<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/S3N0nO.png" alt="S3N0nO"></div></li><li>评价：<div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RfUXiN.png" alt="RfUXiN"></div></li></ol></li><li>基于中间语言的翻译方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OF4wtQ.png" alt="OF4wtQ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/unrXxh.png" alt="unrXxh"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zec67l.png" alt="zec67l"></div></li></ol></li><li>基于语料库的翻译方法<ol><li>基于事例的翻译方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rrPIMH.png" alt="rrPIMH"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3cR0Wo.png" alt="3cR0Wo"></div></li></ol></li><li>统计机器翻译</li><li>神经网络机器翻译</li></ol></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ynUfbt.png" alt="ynUfbt"></div></li></ol><h2 id="统计机器翻译"><a href="#统计机器翻译" class="headerlink" title="统计机器翻译"></a>统计机器翻译</h2><ol><li>基本思想<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UkZOgO.png" alt="UkZOgO"></div></li></ol></li><li>噪声信道模型<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/j02jGI.png" alt="j02jGI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CTcGwa.png" alt="CTcGwa"></div></li></ol></li><li>统计翻译三个关键问题<ol><li>估计语言模型概率 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.491ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 1985 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(1596, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>；<ol><li>n-gram问题</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fxGc1k.png" alt="fxGc1k"></div></li></ol></li><li>估计翻译概率 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.579ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2908 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(1537, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1815, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(2519, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>；<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YQd0sG.png" alt="YQd0sG"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/J3h1qa.png" alt="J3h1qa"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aURzN4.png" alt="aURzN4"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/aQl752.png" alt="aQl752"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/bfLzc1.png" alt="bfLzc1"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pNF8Vo.png" alt="pNF8Vo"></div></li></ol></li><li>快速有效地搜索T 使得 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="13.836ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6115.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(1596, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2207.2, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(3207.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(3710.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4099.4, 0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mo" transform="translate(4744.4, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(5022.4, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(5726.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 最大。</li></ol></li></ol><h2 id="神经网络机器翻译"><a href="#神经网络机器翻译" class="headerlink" title="神经网络机器翻译"></a>神经网络机器翻译</h2><ol><li>对比<ol><li>统计机器翻译<ol><li>数据稀疏（离散符号表示）</li><li>复杂结构无能为力</li><li>强烈依赖先验</li><li>可解释性高</li></ol></li><li>神经机器翻译<ol><li>连续稠密分布式表示</li><li>双语平行语料库 相同</li><li>可解释性低</li><li>翻译性能高，简单有效</li></ol></li></ol></li><li>系统融合方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pNI7lu.png" alt="pNI7lu"></div></li><li>句子级系统融合<ol><li>针对同一个源语言句子，利用最小贝叶斯风险解码或重打分方法比较多个机器翻译系统的译文输 出，将最优的翻译结果作为最终的一致翻译结果</li></ol></li><li>短语级系统融合<ol><li>利用多个翻译系统的输出结果，重新抽取短语翻译规则集合，并利用新的短语翻译规则进行重新 解码</li></ol></li><li>词语级系统融合<ol><li>首先将多个翻译系统的译文输出进行词语对齐，构建一个混淆网络，对混淆网络中的每个位置的 候选词进行置信度估计，最后进行混淆网络解码</li></ol></li><li>深度学习的系统融合<ol><li>首先将多个翻译系统的译文进行编码，然后采用层次注意机制模型预测每个时刻目标语言的输出： 底层注意机制决定该时刻更关注哪个源语言片段， 上层注意机制决定该时刻更依赖哪个系统的输出</li></ol></li></ol></li><li>译文评估方法<ol><li>句子错误率：译文与参考答案不完全相同的句子为错误句子。错误句子占全部译文的比率。</li><li>单词错误率(Multiple Word Error Rate on Multiple Reference, 记作 mWER)：分别计算译文与每个参考译文的编辑距离，以最短的为评分依据，进行归一化处理</li><li>与位置无关的单词错误率 ( Position independent mWER, 记作mPER )：不考虑单词在句子中的顺序</li><li>METEOR 评测方法：对候选译文与参考译文进行词对齐，计算词汇完全 匹配、词干匹配、同义词匹配等各种情况的准确率 ( P)、召回率(R)和F平均值</li><li>BLEU方法<ol><li>将机器翻译产生的候选译文与人翻译的多个参考 译文相比较，越接近，候选译文的正确率越高。</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/NAuVJt.png" alt="NAuVJt"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AI0dxk.png" alt="AI0dxk"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8QT3Ds.png" alt="8QT3Ds"></div></li></ol></li><li>NIST<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/KKBq5L.png" alt="KKBq5L"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lgq0Uq.png" alt="lgq0Uq"></div></li></ol></li></ol></li></ol><h2 id="文本分类和聚类"><a href="#文本分类和聚类" class="headerlink" title="文本分类和聚类"></a>文本分类和聚类</h2><h3 id="传统机器学习方法"><a href="#传统机器学习方法" class="headerlink" title="传统机器学习方法"></a>传统机器学习方法</h3><h3 id="深度学习方法"><a href="#深度学习方法" class="headerlink" title="深度学习方法"></a>深度学习方法</h3><h3 id="文本分类性能评估"><a href="#文本分类性能评估" class="headerlink" title="文本分类性能评估"></a>文本分类性能评估</h3><ol><li>TP</li><li>FP</li><li>TN</li><li>FN</li><li>Precision</li><li>Recall</li><li>Acc</li><li>宏平均：<ol><li>分别算完，再加起来除以类别数</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/MTdEXG.png" alt="MTdEXG"></div></li></ol></li><li>微平均<ol><li>整体的TP等加起来，只算一次</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/j3knan.png" alt="j3knan"></div></li></ol></li><li>P-R曲线<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/bXBRFk.png" alt="bXBRFk"></div></li></ol></li><li>ROC曲线<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/CLgOq6.png" alt="CLgOq6"></div></li></ol></li></ol><h3 id="文本聚类指标"><a href="#文本聚类指标" class="headerlink" title="文本聚类指标"></a>文本聚类指标</h3><h4 id="两个文本对象相似度"><a href="#两个文本对象相似度" class="headerlink" title="两个文本对象相似度"></a>两个文本对象相似度</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7JsKJC.png" alt="7JsKJC"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/m5k96o.png" alt="m5k96o"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7ykEro.png" alt="7ykEro"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WvyzqJ.png" alt="WvyzqJ"></div></li></ol><h4 id="两个文本集合相似度"><a href="#两个文本集合相似度" class="headerlink" title="两个文本集合相似度"></a>两个文本集合相似度</h4><ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tdhVHA.png" alt="tdhVHA"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tf5h15.png" alt="tf5h15"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/yX4Ini.png" alt="yX4Ini"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LOBhoM.png" alt="LOBhoM"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/m16sd6.png" alt="m16sd6"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TBlQDn.png" alt="TBlQDn"></div></li></ol><h4 id="文本和簇的相似度"><a href="#文本和簇的相似度" class="headerlink" title="文本和簇的相似度"></a>文本和簇的相似度</h4><ul><li>样本与簇之间的相似性通常转化为样本间的相似度或 簇间的相似度进行计算。</li><li>如果用均值向量来表示一个簇，那么样本与簇之间的 相似性可以转化为样本与均值向量的样本相似性。</li><li>如果将一个样本视为一个簇，那么就可以采用前面介 绍的簇间的相似性度量方法进行计算</li></ul><h4 id="衡量聚类指标"><a href="#衡量聚类指标" class="headerlink" title="衡量聚类指标"></a>衡量聚类指标</h4><ol><li>外部指标<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4xa3wj.png" alt="4xa3wj"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/neYvWw.png" alt="neYvWw"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HK182v.png" alt="HK182v"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xYojNN.png" alt="xYojNN"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9Yp2Or.png" alt="9Yp2Or"></div></li></ol></li><li>内部标准<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/a7nkUU.png" alt="a7nkUU"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Y5kJt3.png" alt="Y5kJt3"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/fR4wEp.png" alt="fR4wEp"></div></li></ol></li></ol><h2 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h2><ul><li>信息抽取是从非结构化、半结构化的自然语言文本 （如网页新闻、学术文献、社交媒体等）中抽取实 体、实体属性、实体间的关系以及事件等事实信息 ，并形成结构化数据输出的一种文本数据挖掘技术 [ Sarawagi, 2008]</li><li>从非结构化文本到机器可读信息的一种转换技术 [Wu, 2010]</li></ul><h3 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h3><ol><li>概念<ul><li>信息抽取的一项基础任务</li><li>自动识别出文本中指定类别的实体，包括人名、地名、 机构名、日期、时间和货币等七类</li><li>人名（例如“乔布斯”）、地名（例如“北京”）、组 织机构名（例如“中国科学院”）、时间（例如“10点 30分”）、日期（例如“2017年6月1日”）、货币（例 如“1000美元”）、百分比（例如“百分之五十”）</li><li>由于时间、日期、货币和百分比规则性强，利用模板或 正则表达式基本可处理</li><li>人名、地名和组织机构名是关注重点</li></ul></li><li>目标<ul><li>包含两个任务：实体检测和实体分类</li><li>实体检测：检测出文本中哪些词串属于实体，也即发现 实体的左边界和右边界</li><li>实体分类：判别检测出的实体具体属于哪个类别</li></ul></li><li>方法（将实体检测和分类任务联合建模）<ol><li>基于规则的方法</li><li>基于有监督的机器学习方法<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1t3iG1.png" alt="1t3iG1"></div></li><li>基于HMM的NER</li><li>基于CRF的NER</li><li>基于DNN的NER</li></ol></li></ol></li><li>评价指标<ol><li>以实体为单位</li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9XCO8l.png" alt="9XCO8l"></div></li></ol></li></ol><h3 id="实体消岐"><a href="#实体消岐" class="headerlink" title="实体消岐"></a>实体消岐</h3><ol><li>概念<ol><li>一篇文档中同一实体可能有多种不同的指称（Mention） ，不同文档中相同名称的实体也可能表示不同的含义。 前者需要共指消解（Coreference Resolution）技术，后者 需要实体链接（Entity Linking）技术</li><li>共指消解就是为文本中的指称确定其具体实体的过程(一篇文档中同一实体)<ol><li>[李克强]接见[美国总统][特朗普]，[他]首先对[美国总统]的到来表示热烈欢迎。</li><li>“美国总统”和“特朗普”表示同一实体，“他”实际 指代的是“李克强”，即例子中的所有指称聚为两类</li></ol></li><li>实体链接就是确定实体指称所对应的真实世界实体的过程(不同文档中相同名称)<ol><li>“张杰多次参加春节联欢晚会。原上海交通大学校长张杰院士出任中国科学院副院长。</li><li>在第一句话中，“张杰”指的是“歌手张杰”，而第二句话中的“张杰”指的是“中科院院士张杰”</li></ol></li></ol></li><li>典型方法</li><li>实体链接评价<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/2OZzdZ.png" alt="2OZzdZ"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/41Q0A0.png" alt="41Q0A0"></div></li></ol></li></ol><h3 id="关系抽取"><a href="#关系抽取" class="headerlink" title="关系抽取"></a>关系抽取</h3><ol><li>概念<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hjXqkd.png" alt="hjXqkd"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/agnCE1.png" alt="agnCE1"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gHP3rR.png" alt="gHP3rR"></div></li></ol></li><li>评价指标<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pAC5pO.png" alt="pAC5pO"></div></li></ol></li></ol><h3 id="事件抽取"><a href="#事件抽取" class="headerlink" title="事件抽取"></a>事件抽取</h3><ol><li>概念<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mVIfnI.png" alt="mVIfnI"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OOjrkU.png" alt="OOjrkU"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JJ4YwC.png" alt="JJ4YwC"></div></li></ol></li><li>实例<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7gdLg8.png" alt="7gdLg8"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/yItl9M.png" alt="yItl9M"></div></li><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7Hm1dY.png" alt="7Hm1dY"></div></li></ol></li><li>典型方法<ol><li>基于联合模型的事件抽取方法</li><li>基于分布式模型的事件抽取方法</li></ol></li><li>评价指标<ol><li><div style="width:500px;word-wrap:break-word;white-space:normal"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/jshQYf.png" alt="jshQYf"></div></li><li>几乎所有模型都将事件抽取任务分解为触发词识别和事件角色 分类两个步骤，其中触发词识别又可分解为触发词定位与事件 类型分类两个子任务，事件角色分类又可分解为事件元素识别 与角色分类两个子任务；因此，客观评测一般对四个子任务分 别进行测试</li><li>若模型找到了触发词的在事件描述中的具体位置， 那么触发 词定位正确；在定位正确的基础上，如果事件的类型也预测 正确，那么事件类型分类任务得到正确结果。若某候选事件 元素被正确识别为触发词的关联属性，那么事件元素识别是 正确的，如果正确识别的事件元素进一步被预测为正确的事 件角色，那么最终的事件角色分类也是正确的。根据匹配结 果，利用准确率（ Precision）、召回率（Recall）和F1值分别度量各个子任务的性能</li></ol></li></ol><h2 id="人机对话"><a href="#人机对话" class="headerlink" title="人机对话"></a>人机对话</h2><h3 id="任务型对话系统"><a href="#任务型对话系统" class="headerlink" title="任务型对话系统"></a>任务型对话系统</h3><h3 id="聊天型对话系统"><a href="#聊天型对话系统" class="headerlink" title="聊天型对话系统"></a>聊天型对话系统</h3><h3 id="问答系统"><a href="#问答系统" class="headerlink" title="问答系统"></a>问答系统</h3><h3 id="系统搭建方法"><a href="#系统搭建方法" class="headerlink" title="系统搭建方法"></a>系统搭建方法</h3>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;国科大自然语言处理（宗成庆）笔记&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="NLP" scheme="https://hanielxx.com/tags/NLP/"/>
    
    <category term="UCAS" scheme="https://hanielxx.com/tags/UCAS/"/>
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
    <category term="Review" scheme="https://hanielxx.com/tags/Review/"/>
    
  </entry>
  
  <entry>
    <title>似水流年中的碎碎念</title>
    <link href="https://hanielxx.com/Daily/2021-07-02-spiritual-whispers-as-time-goes-by"/>
    <id>https://hanielxx.com/Daily/2021-07-02-spiritual-whispers-as-time-goes-by</id>
    <published>2021-07-02T04:53:30.000Z</published>
    <updated>2021-09-03T02:02:30.280Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note warning"><p>似水流年中的碎碎念</p></div><a id="more"></a><h2 id="2021年7月"><a href="#2021年7月" class="headerlink" title="2021年7月"></a>2021年7月</h2><h3 id="2021-7-5"><a href="#2021-7-5" class="headerlink" title="2021-7-5"></a>2021-7-5</h3><div class="note default"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nvRP1S.png" alt="nvRP1S" width="700/"> 期末虽然只有一门课要考试，但是一周搞定一学期的三学分核心课，真的太顶了...小小的脑袋瓜里要塞下这么多PPT<p>复习累了，和悦姐聊了会天，她说双子座的都比较跳脱和情绪化…不知道其他人咋样，感觉自己确实还是有点情绪化的。</p><p>可能是压力大，复习的时候总是很丧，有对一事无成的不满，有对垃圾实验室的无奈，有对当初保研选择错误的后悔，唉🙁要是当初选了南大，最后那次机会，或许就不是现在这么难受了。不过心里也很清楚，在来一次，那个时候还是会选计算所…毕竟在外面看，计算所还是比南大强不少的。</p><p>研一可以说是结束了，感觉也没啥东西出来，在外面实习了一学期，现在三个月了，996的生活真的太难了。现在真的好想安心搞科研，实习了才感觉到，自己支配所有时间的感觉真的太美好了…就想安下心把自己之前的工作梳理下，往前推动一点，争取出篇paper啊，没有paper真的太难了..</p><p>还两天刷完了《黄金时代》，抛开里面对文革和性的赤裸裸的描述，王二整个的人生还是触到我了，从陈清扬到线条，再到小转铃，最后的二妞子，仿佛看到王二的黄金时代慢慢逝去，除了无法重来的爱情，整个人也被生活磨平了棱角。印象比较深的还有中年王二的《虚伪论》，仿佛看到了未来的自己，似乎也会被生活裹挟着，被茫茫人流推搡着…《黄金时代》包含了太多人生真实的一面，年少轻狂（学生时代和云南插队时期）、对性的渴望（陈清扬）、对爱情的态度（拒绝小转铃）、对死亡的态度（几位死去的人），以及对文革的描述…所有人在生活这个泥沼中挣扎，所有人都是时代车轮中微不足道的一员，没有任何人可以逃过…</p><p>要考试了，感觉以及彻底过拟合了，为了提高泛化能力，扩大数据集，但是cpu和gpu都超负荷了，脑子嗡嗡的..晚上好运吧🙏</p></div><h3 id="2021-7-6"><a href="#2021-7-6" class="headerlink" title="2021-7-6"></a>2021-7-6</h3><div class="note info"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8ZPdeo.png" alt="8ZPdeo" width="700/"><p><a href="https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk" target="_blank" rel="external nofollow noopener noreferrer">Video Speed Controller</a>真的是刷网课神器，16倍速太爽了吧！绝绝子</p></div><h3 id="2021-7-7"><a href="#2021-7-7" class="headerlink" title="2021-7-7"></a>2021-7-7</h3><div class="note warning"><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dfnQQH.jpg" alt="dfnQQH" width="700/"><p>过生日之后，隔了这么久才打开女票送的生日礼物，开始看里面的明信片。</p><p>为什么过了这么久才打开看呢？可能一方面是不知道如何面对这样一份礼物，又因为目前的感情状态，可能很难承受这样一份礼物的重量，我不知道看完之后自己应该是一个什么样的心态去面对，或者说去进行下一个阶段的异地</p><p>清楚的记得，收到这份礼物的时候，当时拆开的那一瞬间，是惊喜带着惊吓的。在我的世界里，很难想象每天写明信片写100天…换成是我，可能想说的话都聊天的时候说完了。</p><p>很幸运能有一个女生能这么用心</p></div>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note warning&quot;&gt;&lt;p&gt;似水流年中的碎碎念&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Daily" scheme="https://hanielxx.com/categories/Daily/"/>
    
    
    <category term="Daily" scheme="https://hanielxx.com/tags/Daily/"/>
    
    <category term="LifeNotes" scheme="https://hanielxx.com/tags/LifeNotes/"/>
    
    <category term="Private" scheme="https://hanielxx.com/tags/Private/"/>
    
  </entry>
  
  <entry>
    <title>UCAS自动课程评估</title>
    <link href="https://hanielxx.com/Daily/2021-06-08-ucas-course-evaluation"/>
    <id>https://hanielxx.com/Daily/2021-06-08-ucas-course-evaluation</id>
    <published>2021-06-08T10:07:52.000Z</published>
    <updated>2021-07-02T07:31:15.919Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>国科大课程一键评估，自动评估脚本。</p></div><a id="more"></a><p>chrome浏览器F12打开开发者工具，然后Console中输入下面的命令。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> tds = <span class="built_in">document</span>.getElementsByTagName(<span class="string">'td'</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">var</span> i = <span class="number">0</span>; i&lt;tds.length;i++)&#123;</span><br><span class="line">    <span class="keyword">var</span> cur_input = tds[i].getElementsByTagName(<span class="string">"input"</span>)[<span class="number">0</span>];</span><br><span class="line">    <span class="keyword">if</span> (cur_input.value==<span class="number">5</span> ) cur_input.checked=<span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">$(<span class="string">"textarea[id='item_"</span>+<span class="number">316</span>+<span class="string">"']"</span>).text(<span class="string">"第一部分的基础知识讲的非常快，后面的都很详细，最喜欢的就就是数量合理的作业，加深了对基础概念的理解"</span>)</span><br><span class="line">$(<span class="string">"textarea[id='item_"</span>+<span class="number">317</span>+<span class="string">"']"</span>).text(<span class="string">"第一部分的基础知识讲的非常快，如果再详细点就更好了"</span>)</span><br><span class="line">$(<span class="string">"textarea[id='item_"</span>+<span class="number">318</span>+<span class="string">"']"</span>).text(<span class="string">"我平均每周在这门课程上花费8、9个小时"</span>)</span><br><span class="line">$(<span class="string">"textarea[id='item_"</span>+<span class="number">319</span>+<span class="string">"']"</span>).text(<span class="string">"课程是非常指导实践的基础课程，一直很感兴趣"</span>)</span><br><span class="line">$(<span class="string">"textarea[id='item_"</span>+<span class="number">320</span>+<span class="string">"']"</span>).text(<span class="string">"本课程我是满勤，良好的完成作业"</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">document</span>.getElementById(<span class="number">322</span>).checked=<span class="literal">true</span></span><br><span class="line"><span class="built_in">document</span>.getElementById(<span class="number">329</span>).checked=<span class="literal">true</span></span><br><span class="line"><span class="built_in">document</span>.getElementById(<span class="number">331</span>).checked=<span class="literal">true</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;国科大课程一键评估，自动评估脚本。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Daily" scheme="https://hanielxx.com/categories/Daily/"/>
    
    
    <category term="Daily" scheme="https://hanielxx.com/tags/Daily/"/>
    
    <category term="UCAS" scheme="https://hanielxx.com/tags/UCAS/"/>
    
    <category term="Automatic" scheme="https://hanielxx.com/tags/Automatic/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记 | Attributed Graph Clustering via Adaptive Graph Convolution</title>
    <link href="https://hanielxx.com/Papers/2021-05-29-gnn-agc-IJCAI-2019"/>
    <id>https://hanielxx.com/Papers/2021-05-29-gnn-agc-IJCAI-2019</id>
    <published>2021-05-29T03:34:18.000Z</published>
    <updated>2021-07-02T07:31:15.919Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>提出当前对GCN如何影响聚类效果以及如何优化GCN聚类在不同图上的效果，还不是很清楚。存在的方法都是对固定的low-order几个邻居进行gcn，弱化了节点关联信息，并且忽视了图的多样性。</p><p>这篇论文主要提出了一种自适应的图卷积方法，应用在图聚类问题上。应用了high-order的GCN来捕获全局的结构信息，并且自适应地对不同的图选择合适的order。</p></div><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ol><li>主要还是强调了当前的方法<ol><li>没有应用更多的节点信息</li><li>多数方法直接用gcn作为特征提取器，而没有研究如何让聚类效果最大化</li><li>低阶邻居，没有考虑大图中全局cluster结构</li><li>忽视了现实中图的多样性</li></ol></li><li>出发点是<ol><li>相邻节点更可能在同一个cluster</li><li>对同一个cluster的有相似特征的节点聚类会更容易</li></ol></li><li>设计了k-order的GCN来filte node feature，得到更smooth的feature embedding，k是用intra-cluster distance自动确定</li><li>模型主要分两步<ol><li>k-order的GCN得到embedding</li><li>对embedding进行spectral clustering</li></ol></li><li>感觉关键是能够自动选择合适的k，以及捕获全局结构信息</li></ol><h2 id="The-Proposed-Method"><a href="#The-Proposed-Method" class="headerlink" title="The Proposed Method"></a>The Proposed Method</h2><ol><li>给定无向图 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="13.964ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6171.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(1063.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2119.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2508.6, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(3277.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3722.2, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(4486.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(4930.9, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(5782.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，V是顶点集合，有n个点，E是边表，用邻接矩阵A表示，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="9.457ex" height="1.71ex" role="img" focusable="false" viewBox="0 -716 4180.2 756"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(1972.6, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(759, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1378, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="27.767ex" height="2.497ex" role="img" focusable="false" viewBox="0 -853.7 12272.8 1103.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(1129.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2185.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(2463.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3439.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3883.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(4859.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(5304, 0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(5804.2, 0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(6304.4, 0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(6582.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(7027.1, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(8073.4, 0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mi" transform="translate(278, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(9177, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(10121.7, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(759, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1378, 0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></g></svg></mjx-container>。如果 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>可以让 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.762ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 779 600.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>通过k个边到达，就说 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.87ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 826.3 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.762ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 779 600.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="7.483ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 3307.4 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(743.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1743.4, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(2319.4, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2804.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g></g></g></svg></mjx-container>邻居。</li><li>目标是将G分成m个不相交的clusters</li><li>定义，graph singal <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.956ex" xmlns="http://www.w3.org/2000/svg" width="19.77ex" height="2.742ex" role="img" focusable="false" viewBox="0 -789.6 8738.3 1212.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(827.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1883.6, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(2650.6, 0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(3393.3, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(4449.1, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1238, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(6950.7, 0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g><g data-mml-node="msub" transform="translate(7791, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>，U是拉普拉斯矩阵分解得到的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.355ex" xmlns="http://www.w3.org/2000/svg" width="12.742ex" height="2.242ex" role="img" focusable="false" viewBox="0 -833.9 5632.1 991"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(681, -150) scale(0.707)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g></g><g data-mml-node="mo" transform="translate(1340.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2396.2, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(3163.2, 0)"><path data-c="39B" d="M320 708Q326 716 340 716H348H355Q367 716 372 708Q374 706 423 547T523 226T575 62Q581 52 591 50T634 46H661V0H653Q644 3 532 3Q411 3 390 0H379V46H392Q464 46 464 65Q463 70 390 305T316 539L246 316Q177 95 177 84Q177 72 198 59T248 46H253V0H245Q230 3 130 3Q47 3 38 0H32V46H45Q112 51 127 91Q128 92 224 399T320 708Z"></path></g><g data-mml-node="msup" transform="translate(3857.2, 0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(821.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="2.143ex" height="1.65ex" role="img" focusable="false" viewBox="0 -442 947.3 729.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>就是特征向量，z是各个特征向量的系数，也就是embedding，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="1.901ex" height="1.65ex" role="img" focusable="false" viewBox="0 -442 840.3 729.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>越大表明基向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="2.143ex" height="1.65ex" role="img" focusable="false" viewBox="0 -442 947.3 729.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>在f中的强度越大。</li><li>定义，smooth是表示相邻节点在图中有相似的特征embedding。</li></ol>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;提出当前对GCN如何影响聚类效果以及如何优化GCN聚类在不同图上的效果，还不是很清楚。存在的方法都是对固定的low-order几个邻居进行gcn，弱化了节点关联信息，并且忽视了图的多样性。&lt;/p&gt;&lt;p&gt;这篇论文主要提出了一种自适应的图卷积方法，应用在图聚类问题上。应用了high-order的GCN来捕获全局的结构信息，并且自适应地对不同的图选择合适的order。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://hanielxx.com/categories/Papers/"/>
    
    
    <category term="GNN" scheme="https://hanielxx.com/tags/GNN/"/>
    
    <category term="IJCAI-2019" scheme="https://hanielxx.com/tags/IJCAI-2019/"/>
    
    <category term="AGC" scheme="https://hanielxx.com/tags/AGC/"/>
    
    <category term="GraphClustering" scheme="https://hanielxx.com/tags/GraphClustering/"/>
    
    <category term="NodeClustering" scheme="https://hanielxx.com/tags/NodeClustering/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记 | Attributed Graph Clustering A Deep Attentional Embedding Approach</title>
    <link href="https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019"/>
    <id>https://hanielxx.com/Papers/2021-05-26-gnn-daegc-IJCAI-2019</id>
    <published>2021-05-26T11:34:18.000Z</published>
    <updated>2021-05-28T18:46:50.330Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>之前研究了DAEGC模型的源码和论文，补个笔记。</p><p>论文《Attributed Graph Clustering: A Deep Attentional Embedding Approach》，模型结果在 Node Clustering on Cora 上，Acc、NMI、ARI排第5， F1排第4。</p></div><a id="more"></a><h2 id="Abstract-amp-Introduction"><a href="#Abstract-amp-Introduction" class="headerlink" title="Abstract & Introduction"></a>Abstract &amp; Introduction</h2><ol><li>Graph clustering<ol><li>在网络中挖掘communities和groups</li><li>目标是将节点划分成不想交的group</li><li>Attributed graph cluster关键问题是如何捕获结构关系和节点信息</li><li>输入是一个图，输出是 ${G_1, G_2,…,G_k$，同一个cluster的节点可能离得比较近，或者有相似的attribute values</li></ol></li><li>近期大多数工作都是学一个graph embedding，然后用传统的聚类方法，比如k-means或者谱聚类，谱聚类在之前文章<a href="https://hanielxx.com/MachineLearning/2021-05-09-gnn-graph-clustering">《GNN和图聚类》</a>中有。</li><li>之前的工作主要是two-step框架下的，文章认为这种方式不是goal-directed，比如面向一些特殊的clustering任务，所以提出了一个goal-directed的方法，</li><li>使用了GAT来捕获邻居特征的重要性，同时encode网络的拓扑结构和节点信息，后面用简单的inner product decoder来重建图信息。用这个GAE生成预测的邻接矩阵A_pred和graph中每个node embedding，作为后面的初始的soft label，用生成的soft label来supervise后面的self-training。</li><li>主要是分两个节点，一个是GAE阶段，得到一个初始的soft label，就是一个k-menas的结果，然后用self-training的一个算法进行迭代更新聚类中心</li><li>这篇文章主要针对大图的复杂度问题和计算量，在对邻居aggregate的时候是sample</li></ol><h2 id="Proposed-Method"><a href="#Proposed-Method" class="headerlink" title="Proposed Method"></a>Proposed Method</h2><h3 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h3><ol><li>用的是GAE的变种作为graph autoencoder，目标是学习每个节点的embedding，每个节点的权重是通过将相邻节点的embedding拼接在一起，然后做一个全连接+softmax就得到了。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dePxtv.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/AKc7H7.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ih9MBs.png" width="500"></li></ol></li><li>GAT其实在拓扑信息上只考虑了1-hop邻居节点，他们为了获得更强的关系信息，用了t-orer邻居节点信息。上面的权重加入结构信息之后就成了下面这样<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F6QEol.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EqGkNT.png" width="500"></li></ol></li><li>Decoder只是简单的Inner Product<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LCleco.png" width="500"></li></ol></li><li>这部分的损失其实就是binary_cross_entropy</li></ol><h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>整体的代码如下，分成了几个部分方便看</p><h5 id="GAE主函数"><a href="#GAE主函数" class="headerlink" title="GAE主函数"></a>GAE主函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GAT</span><br><span class="line"><span class="keyword">from</span> evaluation <span class="keyword">import</span> eva</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pretrain</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># 这部分只是Graph Attentional Encoder的过程，只算了重构损失</span></span><br><span class="line">    model = GAT(</span><br><span class="line">        num_features=args.input_dim,</span><br><span class="line">        hidden_size=args.hidden_size,</span><br><span class="line">        embedding_size=args.embedding_size,</span><br><span class="line">        alpha=args.alpha,</span><br><span class="line">    ).to(device)</span><br><span class="line">    print(model)</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data process</span></span><br><span class="line">    dataset = utils.data_preprocessing(dataset)</span><br><span class="line">    adj = dataset.adj.to(device)</span><br><span class="line">    adj_label = dataset.adj_label.to(device)</span><br><span class="line">    M = utils.get_M(adj).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data and label</span></span><br><span class="line">    x = torch.Tensor(dataset.x).to(device)</span><br><span class="line">    print(dataset)</span><br><span class="line">    print(<span class="string">"GAT:"</span>,model)</span><br><span class="line">    print(<span class="string">"M.shape:"</span>, M.shape)</span><br><span class="line">    y = dataset.y.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(args.max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="comment"># model return reconstructed structure matrix A(N, N) and encoded z(N, output_feat)</span></span><br><span class="line">        A_pred, z = model(x, adj, M)</span><br><span class="line">        loss = F.binary_cross_entropy(A_pred.view(<span class="number">-1</span>), adj_label.view(<span class="number">-1</span>))</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            _, z = model(x, adj, M)</span><br><span class="line">            <span class="comment"># n_init: Number of time the k-means algorithm will be run with different centroid seeds. </span></span><br><span class="line">            <span class="comment"># The final results will be the best output of n_init consecutive runs in terms of inertia.</span></span><br><span class="line">            kmeans = KMeans(n_clusters=args.n_clusters, n_init=<span class="number">20</span>).fit(</span><br><span class="line">                z.data.cpu().numpy()</span><br><span class="line">            )</span><br><span class="line">            acc, nmi, ari, f1 = eva(y, kmeans.labels_, epoch)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">1</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(</span><br><span class="line">                model.state_dict(), <span class="string">f"./pretrain/predaegc_<span class="subst">{args.name}</span>_<span class="subst">{epoch}</span>.pkl"</span></span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=<span class="string">"train"</span>, formatter_class=argparse.ArgumentDefaultsHelpFormatter</span><br><span class="line">    )</span><br><span class="line">    parser.add_argument(<span class="string">"--name"</span>, type=str, default=<span class="string">"Citeseer"</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--max_epoch"</span>, type=int, default=<span class="number">100</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--lr"</span>, type=float, default=<span class="number">0.001</span>)</span><br><span class="line">    parser.add_argument(<span class="string">"--n_clusters"</span>, default=<span class="number">6</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--hidden_size"</span>, default=<span class="number">256</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--embedding_size"</span>, default=<span class="number">16</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">"--weight_decay"</span>, type=int, default=<span class="number">5e-3</span>)</span><br><span class="line">    parser.add_argument(</span><br><span class="line">        <span class="string">"--alpha"</span>, type=float, default=<span class="number">0.2</span>, help=<span class="string">"Alpha for the leaky_relu."</span></span><br><span class="line">    )</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.cuda = torch.cuda.is_available()</span><br><span class="line">    print(<span class="string">"use cuda: {}"</span>.format(args.cuda))</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> args.cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">    datasets = utils.get_dataset(args.name)</span><br><span class="line">    dataset = datasets[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name == <span class="string">"Citeseer"</span>:</span><br><span class="line">        args.lr = <span class="number">0.005</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Cora"</span>:</span><br><span class="line">        args.lr = <span class="number">0.005</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Pubmed"</span>:</span><br><span class="line">        args.lr = <span class="number">0.001</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.input_dim = dataset.num_features</span><br><span class="line"></span><br><span class="line">    print(args)</span><br><span class="line">    pretrain(dataset)</span><br></pre></td></tr></table></figure><h5 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GATLayer</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_features, out_features, alpha=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(GATLayer, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.alpha = alpha</span><br><span class="line"></span><br><span class="line">        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))</span><br><span class="line">        <span class="comment"># 均匀分布初始化输入 Tensor，gain是缩放因子，https://pytorch.apachecn.org/docs/1.0/nn_init.html?h=nn.init.xavier_uniform_</span></span><br><span class="line">        nn.init.xavier_uniform_(self.W.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.a_self = nn.Parameter(torch.zeros(size=(out_features, <span class="number">1</span>)))</span><br><span class="line">        nn.init.xavier_uniform_(self.a_self.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.a_neighs = nn.Parameter(torch.zeros(size=(out_features, <span class="number">1</span>)))</span><br><span class="line">        nn.init.xavier_uniform_(self.a_neighs.data, gain=<span class="number">1.414</span>)</span><br><span class="line"></span><br><span class="line">        self.leakyrelu = nn.LeakyReLU(self.alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, adj, M, concat=True)</span>:</span></span><br><span class="line">        <span class="comment"># x: [samples_cnt=N, input_feat]</span></span><br><span class="line">        <span class="comment"># w: [input_feat, output_feat]</span></span><br><span class="line">        <span class="comment"># h: [N, output_feat]</span></span><br><span class="line">        h = torch.mm(input, self.W) </span><br><span class="line"></span><br><span class="line">        attn_for_self = torch.mm(h, self.a_self)  <span class="comment"># (N,1)</span></span><br><span class="line">        attn_for_neighs = torch.mm(h, self.a_neighs)  <span class="comment"># (N,1)</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; a</span></span><br><span class="line">        <span class="comment"># tensor([[1],</span></span><br><span class="line">        <span class="comment">#         [2],</span></span><br><span class="line">        <span class="comment">#         [3]])</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; torch.transpose(a, 0, 1)</span></span><br><span class="line">        <span class="comment"># tensor([[1, 2, 3]])</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; a+torch.transpose(a, 0, 1)</span></span><br><span class="line">        <span class="comment"># tensor([[2, 3, 4],</span></span><br><span class="line">        <span class="comment">#         [3, 4, 5],</span></span><br><span class="line">        <span class="comment">#         [4, 5, 6]])</span></span><br><span class="line">        attn_dense = attn_for_self + torch.transpose(attn_for_neighs, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># [N, N]*[N, N]=&gt;[N, N]</span></span><br><span class="line">        attn_dense = torch.mul(attn_dense, M) </span><br><span class="line">        attn_dense = self.leakyrelu(attn_dense)  <span class="comment"># (N,N)</span></span><br><span class="line"></span><br><span class="line">        zero_vec = <span class="number">-9e15</span> * torch.ones_like(adj) <span class="comment"># [N, N]</span></span><br><span class="line">        <span class="comment"># torch.where: Return a tensor of elements selected from either x or y, depending on condition</span></span><br><span class="line">        <span class="comment"># torch.where(condition, x, y) → Tensor, xi if condition else yi</span></span><br><span class="line">        adj = torch.where(adj &gt; <span class="number">0</span>, attn_dense, zero_vec) <span class="comment"># [N, N]</span></span><br><span class="line">        <span class="comment"># 对每一行的样本所有邻居softmax</span></span><br><span class="line">        attention = F.softmax(adj, dim=<span class="number">1</span>) <span class="comment"># N, N</span></span><br><span class="line">        h_prime = torch.matmul(attention, h) <span class="comment"># N, output_feat</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> concat:</span><br><span class="line">            <span class="comment"># torch.nn.function.elu: Applies element-wise, ELU(x)=max(0,x)+min(0,α∗(exp(x)−1)) .</span></span><br><span class="line">            <span class="keyword">return</span> F.elu(h_prime)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> h_prime</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> (</span><br><span class="line">            self.__class__.__name__</span><br><span class="line">            + <span class="string">" ("</span></span><br><span class="line">            + str(self.in_features)</span><br><span class="line">            + <span class="string">" -&gt; "</span></span><br><span class="line">            + str(self.out_features)</span><br><span class="line">            + <span class="string">")"</span></span><br><span class="line">        )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GAT</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, hidden_size, embedding_size, alpha)</span>:</span></span><br><span class="line">        super(GAT, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.embedding_size = embedding_size</span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.conv1 = GATLayer(num_features, hidden_size, alpha)</span><br><span class="line">        self.conv2 = GATLayer(hidden_size, embedding_size, alpha)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, adj, M)</span>:</span></span><br><span class="line">        h = self.conv1(x, adj, M)</span><br><span class="line">        h = self.conv2(h, adj, M)</span><br><span class="line">        <span class="comment"># p是Lp normalize中的p，dim是the dimension to reduce. Default: 1</span></span><br><span class="line">        <span class="comment"># z: [N, output_feat]</span></span><br><span class="line">        z = F.normalize(h, p=<span class="number">2</span>, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># decoder, A: [N, N]</span></span><br><span class="line">        A_pred = self.dot_product_decode(z)</span><br><span class="line">        <span class="keyword">return</span> A_pred, z</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">dot_product_decode</span><span class="params">(self, Z)</span>:</span></span><br><span class="line">        A_pred = torch.sigmoid(torch.matmul(Z, Z.t()))</span><br><span class="line">        <span class="keyword">return</span> A_pred</span><br></pre></td></tr></table></figure><h5 id="Evaluate"><a href="#Evaluate" class="headerlink" title="Evaluate"></a>Evaluate</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> munkres <span class="keyword">import</span> Munkres</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.cluster <span class="keyword">import</span> normalized_mutual_info_score <span class="keyword">as</span> nmi_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> adjusted_rand_score <span class="keyword">as</span> ari_score</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linear_sum_assignment <span class="keyword">as</span> linear</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"></span><br><span class="line"><span class="comment"># similar to https://github.com/karenlatong/AGC-master/blob/master/metrics.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cluster_acc</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    <span class="comment"># 对y_pred标签重新分配，然后计算acc等指标</span></span><br><span class="line">    <span class="comment"># 可以用匈牙利算法 (Kuhn-Munkres or Hungarian Algorithm) 实现</span></span><br><span class="line">    y_true = y_true - np.min(y_true)</span><br><span class="line"></span><br><span class="line">    l1 = list(set(y_true))</span><br><span class="line">    numclass1 = len(l1)</span><br><span class="line"></span><br><span class="line">    l2 = list(set(y_pred))</span><br><span class="line">    numclass2 = len(l2)</span><br><span class="line"></span><br><span class="line">    ind = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> numclass1 != numclass2:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> l1:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">in</span> l2:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y_pred[ind] = i</span><br><span class="line">                ind += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    l2 = list(set(y_pred))</span><br><span class="line">    numclass2 = len(l2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> numclass1 != numclass2:</span><br><span class="line">        print(<span class="string">"error"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">    cost = np.zeros((numclass1, numclass2), dtype=int)</span><br><span class="line">    <span class="keyword">for</span> i, c1 <span class="keyword">in</span> enumerate(l1):</span><br><span class="line">        mps = [i1 <span class="keyword">for</span> i1, e1 <span class="keyword">in</span> enumerate(y_true) <span class="keyword">if</span> e1 == c1]</span><br><span class="line">        <span class="keyword">for</span> j, c2 <span class="keyword">in</span> enumerate(l2):</span><br><span class="line">            mps_d = [i1 <span class="keyword">for</span> i1 <span class="keyword">in</span> mps <span class="keyword">if</span> y_pred[i1] == c2]</span><br><span class="line">            cost[i][j] = len(mps_d)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># match two clustering results by Munkres algorithm</span></span><br><span class="line">    m = Munkres()</span><br><span class="line">    cost = cost.__neg__().tolist()</span><br><span class="line">    indexes = m.compute(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get the match results</span></span><br><span class="line">    new_predict = np.zeros(len(y_pred))</span><br><span class="line">    <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(l1):</span><br><span class="line">        <span class="comment"># correponding label in l2:</span></span><br><span class="line">        c2 = l2[indexes[i][<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># ai is the index with label==c2 in the pred_label list</span></span><br><span class="line">        ai = [ind <span class="keyword">for</span> ind, elm <span class="keyword">in</span> enumerate(y_pred) <span class="keyword">if</span> elm == c2]</span><br><span class="line">        new_predict[ai] = c</span><br><span class="line"></span><br><span class="line">    acc = metrics.accuracy_score(y_true, new_predict)</span><br><span class="line">    f1_macro = metrics.f1_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    precision_macro = metrics.precision_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    recall_macro = metrics.recall_score(y_true, new_predict, average=<span class="string">"macro"</span>)</span><br><span class="line">    f1_micro = metrics.f1_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    precision_micro = metrics.precision_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    recall_micro = metrics.recall_score(y_true, new_predict, average=<span class="string">"micro"</span>)</span><br><span class="line">    <span class="keyword">return</span> acc, f1_macro</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">eva</span><span class="params">(y_true, y_pred, epoch=<span class="number">0</span>)</span>:</span></span><br><span class="line">    acc, f1 = cluster_acc(y_true, y_pred)</span><br><span class="line">    nmi = nmi_score(y_true, y_pred, average_method=<span class="string">"arithmetic"</span>)</span><br><span class="line">    ari = ari_score(y_true, y_pred)</span><br><span class="line">    print(<span class="string">f"epoch <span class="subst">{epoch}</span>:acc <span class="subst">{acc:<span class="number">.4</span>f}</span>, nmi <span class="subst">{nmi:<span class="number">.4</span>f}</span>, ari <span class="subst">{ari:<span class="number">.4</span>f}</span>, f1 <span class="subst">{f1:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    <span class="keyword">return</span> acc, nmi, ari, f1</span><br></pre></td></tr></table></figure><h5 id="Utils和Dataset"><a href="#Utils和Dataset" class="headerlink" title="Utils和Dataset"></a>Utils和Dataset</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_dataset</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># Planetoid参考 https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/datasets/planetoid.html#Planetoid</span></span><br><span class="line">    datasets = Planetoid(<span class="string">'./dataset'</span>, dataset)</span><br><span class="line">    <span class="keyword">return</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_preprocessing</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    <span class="comment"># 其实就是用边构建邻接矩阵，参考 https://pytorch.apachecn.org/docs/1.0/torch_tensors.html</span></span><br><span class="line">    dataset.adj = torch.sparse_coo_tensor(</span><br><span class="line">        dataset.edge_index, torch.ones(dataset.edge_index.shape[<span class="number">1</span>]), torch.Size([dataset.x.shape[<span class="number">0</span>], dataset.x.shape[<span class="number">0</span>]])</span><br><span class="line">    ).to_dense()</span><br><span class="line">    dataset.adj_label = dataset.adj</span><br><span class="line"></span><br><span class="line">    <span class="comment"># torch.eye: 返回二维张量，对角线上是1，其它地方是0.</span></span><br><span class="line">    <span class="comment"># 给邻接矩阵加上节点到自己的边</span></span><br><span class="line">    dataset.adj += torch.eye(dataset.x.shape[<span class="number">0</span>]) <span class="comment"># (x.shape[0], x.shape[0])</span></span><br><span class="line">    <span class="comment"># 每个元素除以每行的l1范数，即每行元素和，如果是l2就是除以每行样本的l2范数</span></span><br><span class="line">    <span class="comment"># 这里的adj就是论文中的 transition matrix B_{ij}=1/d_i if e_{ij} \in E</span></span><br><span class="line">    dataset.adj = normalize(dataset.adj, norm=<span class="string">"l1"</span>)</span><br><span class="line">    dataset.adj = torch.from_numpy(dataset.adj).to(dtype=torch.float)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dataset</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_M</span><span class="params">(adj)</span>:</span></span><br><span class="line">    adj_numpy = adj.cpu().numpy()</span><br><span class="line">    <span class="comment"># t_order</span></span><br><span class="line">    t=<span class="number">2</span></span><br><span class="line">    tran_prob = normalize(adj_numpy, norm=<span class="string">"l1"</span>, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># M就是论文中的proximity matrix M</span></span><br><span class="line">    M_numpy = sum([np.linalg.matrix_power(tran_prob, i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, t + <span class="number">1</span>)]) / t</span><br><span class="line">    <span class="keyword">return</span> torch.Tensor(M_numpy)</span><br></pre></td></tr></table></figure><h3 id="Self-optimizing-Embedding"><a href="#Self-optimizing-Embedding" class="headerlink" title="Self-optimizing Embedding"></a>Self-optimizing Embedding</h3><ol><li>这部分刚开始其实一直没怎么看懂，直到看完源码，然后看了之前那篇综述，对整个套路有了大概的了解之后才懂…</li><li>主要是学习两个分部，一个P分布一个Q分布</li><li>用上面GAE跑出来的node embedding作为初始化，然后跑一次k-means得到初始的簇头，然后在后面的训练过程中不断更新簇头</li><li>Q分布是通过node embedding和簇头embedding得到的。簇头初始化通过上面的GAE+k-means得到，node embedding在GAE的基础上更新。所以后面的训练的每个epoch，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.489ex" xmlns="http://www.w3.org/2000/svg" width="1.364ex" height="1.489ex" role="img" focusable="false" viewBox="0 -442 603 658"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D707" d="M58 -216Q44 -216 34 -208T23 -186Q23 -176 96 116T173 414Q186 442 219 442Q231 441 239 435T249 423T251 413Q251 401 220 279T187 142Q185 131 185 107V99Q185 26 252 26Q261 26 270 27T287 31T302 38T315 45T327 55T338 65T348 77T356 88T365 100L372 110L408 253Q444 395 448 404Q461 431 491 431Q504 431 512 424T523 412T525 402L449 84Q448 79 448 68Q448 43 455 35T476 26Q485 27 496 35Q517 55 537 131Q543 151 547 152Q549 153 557 153H561Q580 153 580 144Q580 138 575 117T555 63T523 13Q510 0 491 -8Q483 -10 467 -10Q446 -10 429 -4T402 11T385 29T376 44T374 51L368 45Q362 39 350 30T324 12T288 -4T246 -11Q199 -11 153 12L129 -85Q108 -167 104 -180T92 -202Q76 -216 58 -216Z"></path></g></g></g></svg></mjx-container>和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.052ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 465 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container>都会更新。</li><li>Q分布中的每个值可以衡量每个节点和簇头有多接近，每个epoch中Q分布被认为trustworthy，被当成了soft label，相当于是当前epoch的node embedding和簇头embedding算距离，够近就认为你就是这个簇的，一个假标签。这是模型预测出来的y_pred。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mWV0oc.png" width="500"></li></ol></li><li>每个节点的soft label可以通过下面的argmax得到<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VVpH3P.png" width="500"></li></ol></li><li>而目标分布P才是真的”ground-truth label”，它是通过近期的Q算出来的，所以也依赖Q。P分布按照阶段更新，被当做是这个阶段内的ground-truth，真标签。</li><li>它不能像Q那样每个epoch都更新，不然目标也太不稳定了，Q都不知道朝哪里梯度下降，没法收敛。P代表了在这一个阶段内（论文中写的是5个epoch），Q应该是朝哪里更新，起的作用就是监督学习里的真标签 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.726ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 763 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44C" d="M66 637Q54 637 49 637T39 638T32 641T30 647T33 664T42 682Q44 683 56 683Q104 680 165 680Q288 680 306 683H316Q322 677 322 674T320 656Q316 643 310 637H298Q242 637 242 624Q242 619 292 477T343 333L346 336Q350 340 358 349T379 373T411 410T454 461Q546 568 561 587T577 618Q577 634 545 637Q528 637 528 647Q528 649 530 661Q533 676 535 679T549 683Q551 683 578 682T657 680Q684 680 713 681T746 682Q763 682 763 673Q763 669 760 657T755 643Q753 637 734 637Q662 632 617 587Q608 578 477 424L348 273L322 169Q295 62 295 57Q295 46 363 46Q379 46 384 45T390 35Q390 33 388 23Q384 6 382 4T366 1Q361 1 324 1T232 2Q170 2 138 2T102 1Q84 1 84 9Q84 14 87 24Q88 27 89 30T90 35T91 39T93 42T96 44T101 45T107 45T116 46T129 46Q168 47 180 50T198 63Q201 68 227 171L252 274L129 623Q128 624 127 625T125 627T122 629T118 631T113 633T105 634T96 635T83 636T66 637Z"></path></g></g></g></svg></mjx-container>。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/TgzYqR.png" width="500"></li></ol></li><li>最后，这部分的损失就是P和Q的KL散度。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5LTggJ.png" width="500"></li></ol></li><li>总的损失是两个部分的加权和<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tESTdL.png" width="500"></li></ol></li></ol><h4 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h4><p>这里的evaluate和上面的一样</p><h5 id="自监督模块"><a href="#自监督模块" class="headerlink" title="自监督模块"></a>自监督模块</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DAEGC</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_features, hidden_size, embedding_size, alpha, num_clusters, v=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(DAEGC, self).__init__()</span><br><span class="line">        self.num_clusters = num_clusters</span><br><span class="line">        self.v = v</span><br><span class="line"></span><br><span class="line">        <span class="comment"># get pretrain model</span></span><br><span class="line">        self.gat = GAT(num_features, hidden_size, embedding_size, alpha)</span><br><span class="line">        <span class="comment"># 初始化的时候加载pretrain的GAT模型</span></span><br><span class="line">        self.gat.load_state_dict(torch.load(args.pretrain_path, map_location=<span class="string">'cpu'</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># cluster layer，簇头embedding</span></span><br><span class="line">        self.cluster_layer = Parameter(torch.Tensor(num_clusters, embedding_size))</span><br><span class="line">        torch.nn.init.xavier_normal_(self.cluster_layer.data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, adj, M)</span>:</span></span><br><span class="line">        <span class="comment"># 得到reconstruct的邻接矩阵和[N, feat_size]的节点embedding Z</span></span><br><span class="line">        A_pred, z = self.gat(x, adj, M)</span><br><span class="line">        q = self.get_Q(z)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> A_pred, z, q</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_Q</span><span class="params">(self, z)</span>:</span></span><br><span class="line">        q = <span class="number">1.0</span> / (<span class="number">1.0</span> + torch.sum(torch.pow(z.unsqueeze(<span class="number">1</span>) - self.cluster_layer, <span class="number">2</span>), <span class="number">2</span>) / self.v)</span><br><span class="line">        q = q.pow((self.v + <span class="number">1.0</span>) / <span class="number">2.0</span>)</span><br><span class="line">        q = (q.t() / torch.sum(q, <span class="number">1</span>)).t()</span><br><span class="line">        <span class="keyword">return</span> q</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">target_distribution</span><span class="params">(q)</span>:</span></span><br><span class="line">    weight = q**<span class="number">2</span> / q.sum(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> (weight.t() / weight.sum(<span class="number">1</span>)).t()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainer</span><span class="params">(dataset)</span>:</span></span><br><span class="line">    model = DAEGC(num_features=args.input_dim, hidden_size=args.hidden_size,</span><br><span class="line">                  embedding_size=args.embedding_size, alpha=args.alpha, num_clusters=args.n_clusters).to(device)</span><br><span class="line">    print(model)</span><br><span class="line">    optimizer = Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data process</span></span><br><span class="line">    dataset = utils.data_preprocessing(dataset)</span><br><span class="line">    adj = dataset.adj.to(device)</span><br><span class="line">    adj_label = dataset.adj_label.to(device)</span><br><span class="line">    M = utils.get_M(adj).to(device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># data and label</span></span><br><span class="line">    data = torch.Tensor(dataset.x).to(device)</span><br><span class="line">    y = dataset.y.cpu().numpy()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 这里的GAT已经load了pretrain的模型</span></span><br><span class="line">        <span class="comment"># 相当于用那个epoch的模型做一次eval</span></span><br><span class="line">        _, z = model.gat(data, adj, M)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># get kmeans and pretrain cluster result</span></span><br><span class="line">    <span class="comment"># 这里是用pretrain的结果来初始化kmeans的中心</span></span><br><span class="line">    kmeans = KMeans(n_clusters=args.n_clusters, n_init=<span class="number">20</span>)</span><br><span class="line">    y_pred = kmeans.fit_predict(z.data.cpu().numpy())</span><br><span class="line">    model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)</span><br><span class="line">    eva(y, y_pred, <span class="string">'pretrain'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(args.max_epoch):</span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">if</span> epoch % args.update_interval == <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># update_interval</span></span><br><span class="line">            A_pred, z, Q = model(data, adj, M)</span><br><span class="line"></span><br><span class="line">            q = Q.detach().data.cpu().numpy().argmax(<span class="number">1</span>)  <span class="comment"># Q</span></span><br><span class="line">            eva(y, q, epoch)</span><br><span class="line"></span><br><span class="line">        A_pred, z, q = model(data, adj, M)</span><br><span class="line">        p = target_distribution(Q.detach())</span><br><span class="line"></span><br><span class="line">        kl_loss = F.kl_div(q.log(), p, reduction=<span class="string">'batchmean'</span>)</span><br><span class="line">        re_loss = F.binary_cross_entropy(A_pred.view(<span class="number">-1</span>), adj_label.view(<span class="number">-1</span>))</span><br><span class="line"></span><br><span class="line">        loss = <span class="number">10</span> * kl_loss + re_loss</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><h5 id="调用主函数"><a href="#调用主函数" class="headerlink" title="调用主函数"></a>调用主函数</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> Adam</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> Planetoid</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> GAT</span><br><span class="line"><span class="keyword">from</span> evaluation <span class="keyword">import</span> eva</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># !python3 daegc.py --update_interval 5 --name Cora --epoch 45 --max_epoch 200</span></span><br><span class="line">    parser = argparse.ArgumentParser(</span><br><span class="line">        description=<span class="string">'train'</span>,</span><br><span class="line">        formatter_class=argparse.ArgumentDefaultsHelpFormatter)</span><br><span class="line">    parser.add_argument(<span class="string">'--name'</span>, type=str, default=<span class="string">'Citeseer'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--epoch'</span>, type=int, default=<span class="number">30</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--max_epoch'</span>, type=int, default=<span class="number">100</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--lr'</span>, type=float, default=<span class="number">0.0001</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--n_clusters'</span>, default=<span class="number">6</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--update_interval'</span>, default=<span class="number">1</span>, type=int)  <span class="comment"># [1,3,5]</span></span><br><span class="line">    parser.add_argument(<span class="string">'--hidden_size'</span>, default=<span class="number">256</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--embedding_size'</span>, default=<span class="number">16</span>, type=int)</span><br><span class="line">    parser.add_argument(<span class="string">'--weight_decay'</span>, type=int, default=<span class="number">5e-3</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--alpha'</span>, type=float, default=<span class="number">0.2</span>, help=<span class="string">'Alpha for the leaky_relu.'</span>)</span><br><span class="line">    args = parser.parse_args()</span><br><span class="line">    args.cuda = torch.cuda.is_available()</span><br><span class="line">    print(<span class="string">"use cuda: {}"</span>.format(args.cuda))</span><br><span class="line">    device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> args.cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">    datasets = utils.get_dataset(args.name)</span><br><span class="line">    dataset = datasets[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> args.name == <span class="string">'Citeseer'</span>:</span><br><span class="line">      args.lr = <span class="number">0.0001</span></span><br><span class="line">      args.k = <span class="literal">None</span></span><br><span class="line">      args.n_clusters = <span class="number">6</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">'Cora'</span>:</span><br><span class="line">      args.lr = <span class="number">0.0001</span></span><br><span class="line">      args.k = <span class="literal">None</span></span><br><span class="line">      args.n_clusters = <span class="number">7</span></span><br><span class="line">    <span class="keyword">elif</span> args.name == <span class="string">"Pubmed"</span>:</span><br><span class="line">        args.lr = <span class="number">0.001</span></span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line">        args.n_clusters = <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        args.k = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    args.pretrain_path = <span class="string">f'./pretrain/predaegc_<span class="subst">{args.name}</span>_<span class="subst">{args.epoch}</span>.pkl'</span></span><br><span class="line">    args.input_dim = dataset.num_features</span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>仔细一想，确实是自监督的，毕竟soft label和target label都是自己的embedding算出来的，然后再更新自己的embedding。</li><li>主要是依赖GAE算出来的那个embedding吧，初始化的影响可能比较大…核心还是GAT，Attention 牛批</li><li>虽然感觉同时更新这两个分布，还用自己算出来的分布来拟合另一个自己算出来的分布，这两个分布还有依赖关系，是有点扯…但是人家的效果还就是好emmmm 玄学</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tlKLqz.png" width="500"></li></ul><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><ul><li>贴一下实验对比的baseline</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8BF7AI.png" width="500"></li><li>贴一下在Cora，Citeseer和Pubmed上的结果对比，这里其实都是他们自己跑出来的结果，我看和AGC那篇论文，同样的baseline，效果都不一样…</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/n02Knc.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PmeTG5.png" width="500"></li><li>贴一下分类的效果图，t-SNE可视化的node embedding</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/JbFPVC.png" alt="figure3"></li><li>提到了超参数的设置，还有不同参数的效果对比，实验做的还是挺多的</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RdR0WV.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kMzoKK.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xlw5Xl.png" width="500"></li></ul>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;之前研究了DAEGC模型的源码和论文，补个笔记。&lt;/p&gt;&lt;p&gt;论文《Attributed Graph Clustering: A Deep Attentional Embedding Approach》，模型结果在 Node Clustering on Cora 上，Acc、NMI、ARI排第5， F1排第4。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://hanielxx.com/categories/Papers/"/>
    
    
    <category term="GNN" scheme="https://hanielxx.com/tags/GNN/"/>
    
    <category term="DAEGC" scheme="https://hanielxx.com/tags/DAEGC/"/>
    
    <category term="IJCAI-2019" scheme="https://hanielxx.com/tags/IJCAI-2019/"/>
    
    <category term="GraphClustering" scheme="https://hanielxx.com/tags/GraphClustering/"/>
    
    <category term="NodeClustering" scheme="https://hanielxx.com/tags/NodeClustering/"/>
    
    <category term="GAE" scheme="https://hanielxx.com/tags/GAE/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记 | A Comprehensive Survey on Graph Neural Networks</title>
    <link href="https://hanielxx.com/Papers/2021-05-20-gnn-survey-IEEE-2021"/>
    <id>https://hanielxx.com/Papers/2021-05-20-gnn-survey-IEEE-2021</id>
    <published>2021-05-20T05:34:18.000Z</published>
    <updated>2021-05-27T15:30:56.302Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><ul><li>一篇关于GNN的综述，主要讲<ul><li>namely recurrent graph neural networks,</li><li>convolutional graph neural networks,</li><li>graph autoencoders,</li><li>spatial-temporal graph neural networks</li></ul></li><li>文章结构是<ul><li>GNN发展</li><li>GNN分类</li><li>GNN模型的overview</li><li>GNN模型在各个领域的应用</li><li>总结了开源代码和benchmark dataset</li><li>当前的challenges和research directions</li></ul></li></ul></div><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><ul><li>有很多场景中的数据是graph格式的，e-commence，用户和商品，社交网络，化学分子，引文网络</li><li>机器学习的一个核心假设：样本独立性，在图数据中不成立，各个节点对邻居有依赖关系</li><li>图卷积可以被认为是特殊的2D convolution<ul><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/CfhMIc.png" width="400"></li></ul></li><li>主要贡献<blockquote><p>• New taxonomy We propose a new taxonomy of graph neural networks. Graph neural networks are categorized into four groups: recurrent graph neural networks, convo- lutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks.<br>• Comprehensive review We provide the most compre- hensive overview of modern deep learning techniques for graph data. For each type of graph neural network, we provide detailed descriptions on representative models, make the necessary comparison, and summarise the cor- responding algorithms.<br>• Abundant resources We collect abundant resources on graph neural networks, including state-of-the-art models, benchmark data sets, open-source codes, and practical applications. This survey can be used as a hands-on guide for understanding, using, and developing different deep learning approaches for various real-life applications.<br>• Future directions We discuss theoretical aspects of graph neural networks, analyze the limitations of exist- ing methods, and suggest four possible future research directions in terms of model depth, scalability trade-off, heterogeneity, and dynamicity.</p></blockquote></li></ul><h2 id="Background-and-Definition"><a href="#Background-and-Definition" class="headerlink" title="Background and Definition"></a>Background and Definition</h2><h3 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h3><ul><li>1997年首先在图上用神经网络</li><li>GNN在2005年被首次提出</li><li>早期是研究RecGNN，通过迭代地传播邻居信息直到收敛，得到node representation</li><li>CNN火了之后开始有ConvGNN，分为谱域卷积和空域卷积the spectral-based approaches and the spatial-based approaches</li><li>近几年发展了GAEs和STCNN，图自编码器和时空图神经网络，他们都可以建立在RecGNN和ConvGNN等架构之上</li></ul><ol><li>GNN和network embedding区别：<ol><li>network embedding将网络表示为低维向量，并保存网络结构和节点信息，用于下游分类，聚类，推荐等任务</li><li>GNN用端到端的方式解决类似的问题</li><li>区别在于，GNN包括了很多神经网络模型，能够解决network embedding同样的问题，但是network embedding还包括了一些非DL的方法，比如矩阵分解和随机游走random walk</li></ol></li><li>GNN和graph kernel methods区别:<ol><li>graph kernel 是以前用于解决图分类的主要方法，都是用一些核函数为基础的方法，比如支持向量机SVM。和GNN一样会把graph和node通过核函数映射到高维空间</li><li>区别在于，核函数的方法是pair-wise的，计算量大，有计算瓶颈。并且核函数的方法是确定性的，而不是科学系的</li></ol></li></ol><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/ZZh1nn.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/FYBPOW.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/Bmr5Di.png" width="500"></li></ol><h2 id="Categorization"><a href="#Categorization" class="headerlink" title="Categorization"></a>Categorization</h2><h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ol><li>RecGNN<ol><li>最早期的GNN，目标是学习节点表示，用循环神经网络结构，他们假设节点会和邻居节点交换信息，直到一个稳定的状态</li><li>消息交换的思路被后来的空域图卷积继承</li></ol></li><li>ConvGNN<ol><li>在图上使用卷积操作。主要思路是通过aggregate当前节点和邻居节点的信息来生成节点embedding。</li><li>ConvGNN会堆叠很多层来提取高维特征表示</li></ol></li><li>GAEs<ol><li>将节点或者图编码到latent vector space并且从encoded information中重构图（比如邻接矩阵），使得重构的图和原图损失最小。</li><li>GAE可以用于学习network embedding和graph generative distributions</li></ol></li><li>时空图神经网络STGNN<ol><li>主要是学习时空图的潜在特征</li><li>假定图中的信息是随时间变化的</li><li>同时考虑空间依赖和实践依赖</li></ol></li></ol><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><ol><li>node-level：输出和节点回归or分类有关，一般是提取高维特征后用MLP或者softmax分类</li><li>edge-level: 输出和边分类or连接预测相关。两个节点的GNN编码作为输入，用相似度函数或者NN来预测连接关系和强度</li><li>graph-level：输出和图分类先关的。得到一整个图的表示，经常和pooling，readout等操作结合。</li></ol><h3 id="训练模式"><a href="#训练模式" class="headerlink" title="训练模式"></a>训练模式</h3><ol><li>半监督学习下的节点分类：给定单独的网络和部分标注节点，其他节点未标注。目标是识别未标注的节点。可以用stack一堆卷积层然后接softmax实现多分类</li><li>监督学习下的图分类：给定真个图，预测图类别。一般会结合GCN，pooling层，readout层等。graph pooling一般用于down-sampling，每次获得部分图结构。readout用于收集节点表示然后生成graph embedding。</li><li>无监督学习下的graph embedding：一般使用GAE来编码整个图。最小化重构损失。还会用负采样来得到部分节点对作为负样本，存在连接的是正样本。用逻辑回归分辨是正例还是负例样本。</li></ol><h3 id="图示"><a href="#图示" class="headerlink" title="图示"></a>图示</h3><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/rML4yp.png" width="600"><h2 id="Recurrent-GNN"><a href="#Recurrent-GNN" class="headerlink" title="Recurrent GNN"></a>Recurrent GNN</h2><ol><li>RecGNN对图上所有的点使用相同的parameters，来提取高维节点representations</li><li>早起主要受限算力，多用于有向无环图</li><li>基于信息传播机制，用所有的邻居节点来更新自己节点，周期性交换，直到稳定下来</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.211ex" xmlns="http://www.w3.org/2000/svg" width="34.509ex" height="3.611ex" role="img" focusable="false" viewBox="0 -1060.7 15253 1596"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(750, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(576, -139.6) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mo" transform="translate(1709.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(2765, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(572, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msub" transform="translate(1239, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(803, -176.7) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(874, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></g><g data-mml-node="mi" transform="translate(6148.4, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(6698.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(7087.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mo" transform="translate(8052.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(8497, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -355.1) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(874, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1724, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(10613.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(11057.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(12084.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msubsup" transform="translate(12528.9, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, 530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(750, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1528, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2028, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(576, -138.9) scale(0.707)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(14864, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>初始化是随机的</li><li>对所有节点做上面的操作</li><li>为了确保收敛，f函数必须是<strong>收缩性映射</strong>，就是会缩小两个点在投影空间的距离</li><li>如果f是一个NN，需要在参数的Jacobian matrix雅可比矩阵上加惩罚项。其实就是w加惩罚项。<ol><li>NN从n到m维的话，相当于是m个f函数</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/g5uRAV.png" width="500"></li></ol></li><li>收敛之后把最后一个时间步的hidden进行readout</li></ol><ul><li>GraphESN包括了一个可训练的encoder</li><li>Gated Graph Neural Network(GGNN)是用GRU作为recurrent function。好处是不需要约束参数一定要收敛。用BPTT进行优化。但是因为它要对所有节点进行好几次recurrent function，并且都是存在memory中，对于大图来说不友好</li><li>Stochastic Steady-state Embedding (SSE)对大图更加友好，它只采样一个batch的接地那来更新state，并且只对一个batch的节点进行梯度计算。SSE的recurrent function是对历史状态和新状态的weighted average。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.448ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 640 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FC" d="M34 156Q34 270 120 356T309 442Q379 442 421 402T478 304Q484 275 485 237V208Q534 282 560 374Q564 388 566 390T582 393Q603 393 603 385Q603 376 594 346T558 261T497 161L486 147L487 123Q489 67 495 47T514 26Q528 28 540 37T557 60Q559 67 562 68T577 70Q597 70 597 62Q597 56 591 43Q579 19 556 5T512 -10H505Q438 -10 414 62L411 69L400 61Q390 53 370 41T325 18T267 -2T203 -11Q124 -11 79 39T34 156ZM208 26Q257 26 306 47T379 90L403 112Q401 255 396 290Q382 405 304 405Q235 405 183 332Q156 292 139 224T121 120Q121 71 146 49T208 26Z"></path></g></g></g></svg></mjx-container>是超参<ul><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/wpZYPJ.png" width="500"></li></ul></li></ul><h2 id="Convolutional-GNN"><a href="#Convolutional-GNN" class="headerlink" title="Convolutional GNN"></a>Convolutional GNN</h2><ol><li>GCN用固定层数但权重不同的层，在结构上，处理循环相互依存问题</li><li>比RecGNN更加高效和方便</li><li>分两类，谱域图卷积和空域图卷积<ol><li>Spectral based GCN，从图信号处理的角度引入滤波器，卷积操作被解释为去除图信号中的噪声</li><li>Spatial based GCN，继承了RecGNN的思想，用GCN来进行信息传递</li><li>自从弥合了基于频谱的方法和基于空间的方法之间的差距以来，基于空间的方法由于其引人注目的效率，灵活性和通用性而迅速发展</li></ol></li></ol><h3 id="Spectral-based-GCN"><a href="#Spectral-based-GCN" class="headerlink" title="Spectral based GCN"></a>Spectral based GCN</h3><ol><li>它有一整台完备的图信号处理理论基础</li><li>假定是无向图</li><li>normalized Laplacian matrix图拉布拉斯矩阵，是无向图的数学表示，定义为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="20.09ex" height="2.562ex" role="img" focusable="false" viewBox="0 -974.6 8879.8 1132.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(958.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(2014.6, 0)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(440, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3151, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(4151.3, 0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(828, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(778, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g></g></g><g data-mml-node="mi" transform="translate(6140.5, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="msup" transform="translate(6890.5, 0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(828, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(778, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g></g></g></g></g></svg></mjx-container>，其中D是节点的度矩阵，A是邻接矩阵，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.972ex" xmlns="http://www.w3.org/2000/svg" width="14.495ex" height="2.669ex" role="img" focusable="false" viewBox="0 -750 6406.6 1179.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(828, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1643.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(2699.5, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, -285.4) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(4096.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(4485.8, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(750, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6017.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>更多的拉普拉斯矩阵看<a href="https://zh.wikipedia.org/wiki/%E8%B0%83%E5%92%8C%E7%9F%A9%E9%98%B5" target="_blank" rel="external nofollow noopener noreferrer">wikipedia</a>和<a href="https://www.cnblogs.com/yyl424525/p/14361357.html" target="_blank" rel="external nofollow noopener noreferrer">别人的blog</a></li><li>性质：<ol><li>L是实对称正半定的，</li><li>最小特征值大于等于0，</li><li>特征值中0出现的次数就是图连通区域的个数，</li><li>最小特征值是0，因为拉普拉斯矩阵（普通形式：𝐿=𝐷−𝐴）每一行的和均为0，</li><li>最小特征值对应的特征向量是每个值全为1的向量</li><li>最小非零特征值是图的代数连通度。</li></ol></li><li>L可以进行特征值分解，就是矩阵分解为由其特征值和特征向量表示的矩阵之积，表示成 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="41.252ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 18233.3 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(958.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2014.6, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(2781.6, 0)"><path data-c="39B" d="M320 708Q326 716 340 716H348H355Q367 716 372 708Q374 706 423 547T523 226T575 62Q581 52 591 50T634 46H661V0H653Q644 3 532 3Q411 3 390 0H379V46H392Q464 46 464 65Q463 70 390 305T316 539L246 316Q177 95 177 84Q177 72 198 59T248 46H253V0H245Q230 3 130 3Q47 3 38 0H32V46H45Q112 51 127 91Q128 92 224 399T320 708Z"></path></g><g data-mml-node="msup" transform="translate(3475.6, 0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(821.2, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(5122.3, 0)"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">，</text></g><g data-mml-node="mi" transform="translate(6300.1, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(7344.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(8400.7, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(8678.7, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(9654.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(10098.9, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(11074.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(11519.1, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(11963.8, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(12408.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(12853.1, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1378, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(15080.8, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(16025.6, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(759, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(1378, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>，U是是列向量为单位特征向量的矩阵，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.57ex" height="1.62ex" role="img" focusable="false" viewBox="0 -716 694 716"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="39B" d="M320 708Q326 716 340 716H348H355Q367 716 372 708Q374 706 423 547T523 226T575 62Q581 52 591 50T634 46H661V0H653Q644 3 532 3Q411 3 390 0H379V46H392Q464 46 464 65Q463 70 390 305T316 539L246 316Q177 95 177 84Q177 72 198 59T248 46H253V0H245Q230 3 130 3Q47 3 38 0H32V46H45Q112 51 127 91Q128 92 224 399T320 708Z"></path></g></g></g></svg></mjx-container>的对角线是特征值。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="8.99ex" height="2.09ex" role="img" focusable="false" viewBox="0 -841.7 3973.6 923.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(821.2, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(1369, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(2413.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3469.6, 0)"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g></g></g></svg></mjx-container>。注意，特征分解最右边的是特征矩阵的逆，只是拉普拉斯矩阵是对称矩阵才可以写成特征矩阵的转置。</li><li>在图信号处理中，图信号 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.85ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 3027.8 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(849.8, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(1794.6, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(759, 363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>是所有节点的特征向量，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>是第i个节点的值。</li><li>对一个信号x的，图傅里叶变换graph Fourier transform，定义为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.338ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 5453.6 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="46" d="M199 579Q181 579 181 590Q181 598 188 611T212 639T260 666T335 682Q336 682 349 682T383 682T431 682T493 683T561 683Q776 682 784 681Q826 673 829 647Q829 620 797 600T744 580Q728 580 728 595Q729 607 713 610Q698 613 598 614H500L499 610Q499 598 467 486T428 367Q428 365 551 365H674Q683 360 684 355Q687 346 677 329Q666 312 642 299T598 285Q586 285 582 296H402L394 277Q386 258 373 229T346 167T315 102T286 51Q265 22 225 -5T133 -32Q108 -32 87 -25T54 -7T33 15T21 35T18 47Q18 60 44 80T98 103Q108 103 111 101T119 88Q130 66 150 54T179 39T195 37Q199 37 203 43Q217 67 245 125T318 300T391 532Q393 543 398 564T406 598T409 613T339 614H269Q229 579 199 579Z"></path></g></g><g data-mml-node="mo" transform="translate(829, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1218, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1790, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2456.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(3512.6, 0)"><g data-mml-node="mi"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mi" transform="translate(821.2, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(4881.6, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container>，逆傅里叶变换是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="10.976ex" height="2.296ex" role="img" focusable="false" viewBox="0 -765 4851.6 1015"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="46" d="M199 579Q181 579 181 590Q181 598 188 611T212 639T260 666T335 682Q336 682 349 682T383 682T431 682T493 683T561 683Q776 682 784 681Q826 673 829 647Q829 620 797 600T744 580Q728 580 728 595Q729 607 713 610Q698 613 598 614H500L499 610Q499 598 467 486T428 367Q428 365 551 365H674Q683 360 684 355Q687 346 677 329Q666 312 642 299T598 285Q586 285 582 296H402L394 277Q386 258 373 229T346 167T315 102T286 51Q265 22 225 -5T133 -32Q108 -32 87 -25T54 -7T33 15T21 35T18 47Q18 60 44 80T98 103Q108 103 111 101T119 88Q130 66 150 54T179 39T195 37Q199 37 203 43Q217 67 245 125T318 300T391 532Q393 543 398 564T406 598T409 613T339 614H269Q229 579 199 579Z"></path></g></g><g data-mml-node="mo" transform="translate(829, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1218, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1790, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2456.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3512.6, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4279.6, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.756ex" role="img" focusable="false" viewBox="0 -765 572 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>是图傅里叶变换的结果信号</li><li>图傅里叶将输入图信号投影到正交空间，正交空间的基础上由规范化图拉普拉斯算子的特征向量形成</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/pottwG.png" width="500"></li><li>Spectral CNN假定 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.943ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 858.6 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(477, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g></g></svg></mjx-container>是一组科学系的参数，并且认为graph signals是有多个通道的。<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/yJiXBV.png" width="500"></li><li>缺点是任何的对图的扰动都会导致特征基的变化，并且filter是domain dependent，不可以应用到其他结构上，最后，特诊值分解需要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.757ex" height="2.451ex" role="img" focusable="false" viewBox="0 -833.2 2544.6 1083.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g><g data-mml-node="mo" transform="translate(2155.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>的时间复杂度</li></ol></li></ol><h4 id="ChebNet"><a href="#ChebNet" class="headerlink" title="ChebNet"></a>ChebNet</h4><ol><li>是对spectral cnn即第二代GCN的改进</li><li>用特征值对角阵的切比雪夫多项式近似滤波器 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.943ex" height="1.464ex" role="img" focusable="false" viewBox="0 -442 858.6 647"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(477, -150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g></g></g></svg></mjx-container>，</li><li>切比雪夫多项式用的是第一类切比雪夫多项式，用递归关系得到的<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/KtvsJj.png" width="400"></li></ol></li><li>具体的还得看论文or别人的博客</li><li>关于切比雪夫多项式：<a href="https://zh.wikipedia.org/wiki/%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E5%A4%9A%E9%A1%B9%E5%BC%8F" target="_blank" rel="external nofollow noopener noreferrer">wikipedia</a></li><li>主要是把卷积核用 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex" xmlns="http://www.w3.org/2000/svg" width="17.478ex" height="3.127ex" role="img" focusable="false" viewBox="0 -1039 7725.4 1382.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(477, -150) scale(0.707)"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g></g><g data-mml-node="mo" transform="translate(1136.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(2192.2, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g><g data-mml-node="msub" transform="translate(4612.5, 0)"><g data-mml-node="mi"><path data-c="1D703" d="M35 200Q35 302 74 415T180 610T319 704Q320 704 327 704T339 705Q393 701 423 656Q462 596 462 495Q462 380 417 261T302 66T168 -10H161Q125 -10 99 10T60 63T41 130T35 200ZM383 566Q383 668 330 668Q294 668 260 623T204 521T170 421T157 371Q206 370 254 370L351 371Q352 372 359 404T375 484T383 566ZM113 132Q113 26 166 26Q181 26 198 36T239 74T287 161T335 307L340 324H145Q145 321 136 286T120 208T113 132Z"></path></g><g data-mml-node="mi" transform="translate(469, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msub" transform="translate(5375.4, 0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(584, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(6253.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(6642.4, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="39B" d="M320 708Q326 716 340 716H348H355Q367 716 372 708Q374 706 423 547T523 226T575 62Q581 52 591 50T634 46H661V0H653Q644 3 532 3Q411 3 390 0H379V46H392Q464 46 464 65Q463 70 390 305T316 539L246 316Q177 95 177 84Q177 72 198 59T248 46H253V0H245Q230 3 130 3Q47 3 38 0H32V46H45Q112 51 127 91Q128 92 224 399T320 708Z"></path></g><g data-mml-node="mo" transform="translate(97, 245)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7336.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>表示</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/np24Qf.png" width="500"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/HIIOvQ.png" width="500"></li></ol><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><ol><li>用cheb net的一阶邻居近似</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/lWZSXn.png" width="500"></li></ol><h4 id="DGCN"><a href="#DGCN" class="headerlink" title="DGCN"></a>DGCN</h4><p>用normalized 邻接矩阵 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.729ex" height="2.351ex" role="img" focusable="false" viewBox="0 -1039 764 1039"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(264, 245)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>和PPMI矩阵来捕获local和global的结构信息。使用两个parallel gcn。说的比较简单，想了解可以再看paper。</p><h3 id="Spatial-based-conv-gnn"><a href="#Spatial-based-conv-gnn" class="headerlink" title="Spatial based conv gnn"></a>Spatial based conv gnn</h3><ol><li>定义在节点空间关系上</li><li>图片是特殊的graph，cnn卷积核相当于是对周围的节点加权平均，gcn也是用邻居节点来更新中心节点</li><li>另一个角度来看，是继承了rec gnn的消息传递的思路</li><li>spatial base gcn是通过边来传递节点信息的</li></ol><h4 id="Neural-Network-for-Graphs-NN4G"><a href="#Neural-Network-for-Graphs-NN4G" class="headerlink" title="Neural Network for Graphs (NN4G)"></a>Neural Network for Graphs (NN4G)</h4><ol><li>first work towards spatial-based ConvGNNs</li><li>增量式地扩展邻居节点</li><li>直接对邻居节点进行summing up</li><li>应用了残差连接residual connection、skip connection来记忆化每层信息</li><li>每层节点状态更新为：<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/Ii0ztO.png" width="400"></li></ol></li></ol><h4 id="Contextual-Graph-Markov-Model-CGMM"><a href="#Contextual-Graph-Markov-Model-CGMM" class="headerlink" title="Contextual Graph Markov Model (CGMM)"></a>Contextual Graph Markov Model (CGMM)</h4><ol><li>用马尔科夫概率模型建模</li></ol><h4 id="Diffusion-Convolutional-Neural-Network-DCNN"><a href="#Diffusion-Convolutional-Neural-Network-DCNN" class="headerlink" title="Diffusion Convolutional Neural Network (DCNN)"></a>Diffusion Convolutional Neural Network (DCNN)</h4><ol><li>认为gcn是信息传播过程，以确定性概率 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="10.444ex" height="2.072ex" role="img" focusable="false" viewBox="0 -833.9 4616.2 915.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1028.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(2084.6, 0)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="TeXAtom" transform="translate(828, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(3866.2, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g></svg></mjx-container>传播</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="22.115ex" height="2.587ex" role="img" focusable="false" viewBox="0 -893.3 9774.9 1143.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(940.8, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2187.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3242.9, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g><g data-mml-node="mo" transform="translate(3792.9, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(4181.9, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1103.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6420.3, 0)"><path data-c="2A00" d="M555 -250Q420 -250 306 -185T124 -4T56 250Q56 453 193 595T526 749Q528 750 539 750Q554 750 562 749Q688 749 800 687T983 508T1054 250Q1054 112 987 -3T806 -184T555 -250ZM555 -165Q672 -165 767 -108T916 44T970 250Q970 418 861 532T600 664Q591 665 548 665Q446 665 353 614T200 466T140 250V243Q140 88 248 -30Q262 -46 280 -62T338 -105T434 -148T555 -165ZM478 250Q478 288 503 307T551 326Q586 326 609 305T632 250Q632 217 610 196T555 174T500 196T478 250Z"></path></g><g data-mml-node="msup" transform="translate(7698, 0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(806.5, 363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mi" transform="translate(8922.9, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g></g></g></svg></mjx-container></li><li>如果P是固定的X也是那么H之间的差异就在W不同类似多头atten？</li><li>最后拼接所有的H作为模型输出</li></ol><h4 id="Diffusion-Graph-Convolution-DGC"><a href="#Diffusion-Graph-Convolution-DGC" class="headerlink" title="Diffusion Graph Convolution (DGC)"></a>Diffusion Graph Convolution (DGC)</h4><ol><li>相比上面的DCNN直接对所有的H求和，而不是拼接</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex" xmlns="http://www.w3.org/2000/svg" width="23.174ex" height="2.949ex" role="img" focusable="false" viewBox="0 -960 10242.9 1303.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(2221.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(4766.3, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g><g data-mml-node="mo" transform="translate(5316.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(5705.3, 0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(806.5, 363) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mi" transform="translate(6930.2, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="msup" transform="translate(7782.2, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1103.2, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(9853.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol><h4 id="PGC-DGCNN"><a href="#PGC-DGCNN" class="headerlink" title="PGC-DGCNN"></a>PGC-DGCNN</h4><ol><li>结合了最短路径的思想，符合条件的为1，否则为0，代替邻接矩阵</li><li>超参控制接受域</li></ol><h4 id="Message-Passing-Neural-Network-MPNN"><a href="#Message-Passing-Neural-Network-MPNN" class="headerlink" title="Message Passing Neural Network (MPNN)"></a>Message Passing Neural Network (MPNN)</h4><ol><li>提出通用的spatial base gcn</li><li>认为gcn是消息沿着边有向传播过程</li><li>用k-step消息传播iteration，让消息传的更远</li><li>hk can be passed to an output layer to perform node-level prediction tasks or to a readout function to perform graph-level prediction tasks.</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/48ecPv.png" width="400"></li><li>M和U都是可训练参数</li></ol><h4 id="GIN"><a href="#GIN" class="headerlink" title="GIN"></a>GIN</h4><ol><li>认为MPNN第不同图结构的分辨能力不足</li><li>调整了中心节点的权重</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/s8zUwE.png" width="400"></li></ol><h4 id="GraphSage"><a href="#GraphSage" class="headerlink" title="GraphSage"></a>GraphSage</h4><ol><li>大图上节点的邻居可能很多，所以直接把所有邻居都考虑进来很低效</li><li>使用采样的方法得到固定数量的邻居</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/vUj7Xn.png" width="400"></li></ol><h4 id="Graph-Attention-Network-GAT"><a href="#Graph-Attention-Network-GAT" class="headerlink" title="Graph Attention Network (GAT)"></a>Graph Attention Network (GAT)</h4><ol><li>2017</li><li>认为每个邻居对中心点的贡献是不一样的</li><li>而GraphSAGE中是固定的，gcn是预先确定</li><li>使用了注意力机制来学习相对权重</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/HTLQiY.png" width="500"></li></ol><h4 id="Gated-Attention-Network-GAAN"><a href="#Gated-Attention-Network-GAAN" class="headerlink" title="Gated Attention Network (GAAN)"></a>Gated Attention Network (GAAN)</h4><ol><li>2018</li><li>引入自监督机制</li><li>在GAT基础上计算每个atten head的权重</li></ol><ul><li>后续还有人在此基础上用LSTM类似的门控机制来控制信息在卷积层的流动</li><li>也有其他的graph attention model提出，但是不属于conv gnn框架</li><li>也有MoNet提出用不同的方式来分配节点邻居的权重</li><li>后面有很多比如GCNN、ACNN等都是MoNet的基础上做的</li></ul><h4 id="获取权重的另一种方式"><a href="#获取权重的另一种方式" class="headerlink" title="获取权重的另一种方式"></a>获取权重的另一种方式</h4><ol><li>PATCHY-SAN<ol><li>对邻居排序，只选top q的邻居</li><li>只考虑结构信息来rank</li><li>然后用卷积来提取邻居特征</li></ol></li><li>Large- scale Graph Convolutional Network (LGCN)<ol><li>按照邻居的node feature information进行rank</li><li>构建一个包含所有邻居的feature matrix</li><li>对这个矩阵按照每一列排序，排序后选前q个作为中心节点的输入</li><li>没懂这个对feature matrix 排序是啥意思</li></ol></li></ol><h4 id="加快训练和提高效率"><a href="#加快训练和提高效率" class="headerlink" title="加快训练和提高效率"></a>加快训练和提高效率</h4><ol><li>GraphSage<ol><li>It samples a tree rooted at each node by recursively expanding the root node’s neighborhood by K steps with a fixed sample size. For each sampled tree, GraphSage computes the root node’s hidden representation by hierarchically aggregating hidden node representations from bottom to top.</li></ol></li><li>Fast-GCN<ol><li>Fast-GCN对每个gcn层采样固定数量的节点，而不是对每个节点采样固定数量邻居.</li><li>它将图卷积解释为概率测度下节点嵌入函数的积分变换。</li><li>用蒙特卡洛近似和方差降低技术</li></ol></li><li>Huang<ol><li>自适应的layer-wise采样</li><li>较低层的节点采样以顶层为条件</li><li>比Fast-GCN准确率更高</li></ol></li><li>StoGCN<ol><li>降低GCN接受域到一个随机的小的范围</li><li>用历史节点表示作为控制变量</li><li>缺点是要保存所有节点的中间状态，标记哦消耗内存</li></ol></li><li>Cluster-GCN<ol><li>使用图聚类算法采样子图，并在子图上进行gcn</li><li>能够适应大规模的图，以及更深的架构</li></ol></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/wKjzfW.png" width="500"></li></ol><h3 id="Comparison-between-Spectral-and-Spatial"><a href="#Comparison-between-Spectral-and-Spatial" class="headerlink" title="Comparison between Spectral and Spatial"></a>Comparison between Spectral and Spatial</h3><ol><li>spectral based<ol><li>有图信号处理的理论基础</li><li>效率更低，要么需要特征值分解，要么要同时处理整个图</li><li>依赖图傅里叶变换，对图的任何扰动都会造成特征向量，就是正交空间的基向量的变化</li><li>只能处理无向图</li></ol></li><li>spatial based<ol><li>更高效，通用，灵活</li><li>可扩展性更强，对大图友好</li><li>在图的局部进行gcn，并且权重可以在不同位置和结构之间共享</li><li>能够处理multi-source的图输入，更加灵活，因为它们都可以很容易的被aggregation方法处理</li></ol></li></ol><h3 id="Graph-Pooling"><a href="#Graph-Pooling" class="headerlink" title="Graph Pooling"></a>Graph Pooling</h3><ol><li>问题：直接利用gcn产生的feature计算量很大，需要一个down-sampling策略</li><li>主要分两种<ol><li>用于降低参数量，得到一个更小的表示，避免overfit</li><li>readout操作来产生graph-level表示</li></ol></li><li>最常用还是mean，max，sum pooling</li><li>Henaff提出在网络开始加一个简单的max/mean pooling对降维很重要</li><li>有人提出用attention机制提升mean/sum pooling</li><li>sum pooling<ol><li>可能会导致embedding不够好，因为它无视了graph size生成同样固定大小的embedding。</li><li>Vinyal提出用lstm提取顺序信息到embedding，在reduce之前，这样会让memory的大小随着输入变大</li></ol></li><li>Defferrard在了ChebNet中提出重排列图节点的方法。<ol><li>先粗化得到多个level的，</li><li>然后和输入节点一起重排得到一个平衡二叉树。</li><li>对这个二叉树从底向上进行aggregate，会把相似节点arrange到一起。</li><li>对重排后的信号进行pooling会比直接pooling更高效</li></ol></li><li>DGCNN中提出一个类似的SortPooling策略<ol><li>也是节点重排序后pooling。</li><li>按照节点在图的结构上的role排序</li><li>认为spatial graph得到的节点特征是连续的，并按照这个排序节点</li><li>调整graph size到q，多的删掉，少的补0</li></ol></li><li>DiffPool<ol><li>对比前面的强调了结构信息</li><li>可以生成图的层次化表示</li><li>不是简单的聚集节点信息，而是通过在第K层学习一个聚类分配向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="14.312ex" height="2.112ex" role="img" focusable="false" viewBox="0 -893.3 6326.1 933.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(696.6, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1942.9, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2887.7, 0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="TeXAtom" transform="translate(759, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(1018.4, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="msub" transform="translate(1796.4, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(600, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.304ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1018.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g></g></svg></mjx-container>是第k层节点数</li><li>这个S中的概率值是通过节点特征和拓扑结构得到的：</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="37.021ex" height="2.587ex" role="img" focusable="false" viewBox="0 -893.3 16363.4 1143.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(696.6, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(910, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1942.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(2998.7, 0)"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(954, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(1504, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1865, 0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2743, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3272, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g><g data-mml-node="mo" transform="translate(6842.7, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(7231.7, 0)"><g data-mml-node="mi"><path data-c="43" d="M201 -25Q167 -25 136 -14T75 23T29 94T12 202Q12 290 50 394T161 574Q227 642 303 673T433 704Q435 705 457 705Q533 701 533 640Q533 606 507 548T464 474Q431 444 396 444Q381 444 381 453Q381 459 388 473T407 513T428 563Q433 580 433 594Q433 636 381 636Q314 636 260 594T175 489T128 363T112 247Q112 157 153 101T273 44Q347 44 398 121Q413 144 437 157T481 171Q496 171 496 160Q496 150 476 123Q426 56 350 16T201 -25Z"></path></g><g data-mml-node="mi" transform="translate(527, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1012, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1612, 0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mi" transform="translate(9328.7, 0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(10114.7, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="msub" transform="translate(11002.7, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(803, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(12224.1, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(12613.1, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="TeXAtom" transform="translate(750, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(13781.5, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(14226.2, 0)"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="TeXAtom" transform="translate(940.8, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(15585.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(15974.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，这个ConvGNN可以是任何标准的GCN，核心思想是学习一个综合了拓扑结构和特征的assignment</li><li>缺点是在pooling之后生成了dense graphs，会导致计算复杂度变成 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.757ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 2544.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2155.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol></li><li>SAGPool<ol><li>用self-attention的方式</li><li>考虑node features和graph topology</li><li>相当于就是把上面的gcn换成attention</li></ol></li></ol><h3 id="理论层面的讨论分析"><a href="#理论层面的讨论分析" class="headerlink" title="理论层面的讨论分析"></a>理论层面的讨论分析</h3><ol><li>receptive field的大小<ol><li>接受域是能够对当前节点最后的node embedding产生影响的节点集合</li><li>stack多个spatial gcn的时候，会导致这个receptive field越来越大，每次向更远的邻居增长一步</li><li>spatial gcn层数是有限的，最终结果就是，每个节点的接受域都覆盖了所有节点，这时候，一个gcn就可以提取整个全局信息</li></ol></li><li>VC维<ol><li>VC维是衡量模型复杂度的，定义成模型可以shatter的最多的点的数量</li><li>GNN的VC维方面的研究比较少</li><li>Scarselli等推出，如果用sigmoid或者切线双曲线激活，GNN的VC维是 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.808ex" height="2.47ex" role="img" focusable="false" viewBox="0 -841.7 3451.1 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mn" transform="translate(503, 363) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g><g data-mml-node="msup" transform="translate(2058.6, 0)"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(600, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3062.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，n是节点数，p是模型参数</li><li>如果是分段多项式激活函数，即使 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="6.895ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 3047.6 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mn" transform="translate(503, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mi" transform="translate(2058.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2658.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>模型复杂度会随着p和n的数量上升飞快</li></ol></li><li>图同构，Graph isomorphism<ol><li>如果两个图的拓扑结构是相同的，就是同构的</li><li>如果两个图可以通过WL测试，得到不同的embedding，就是不同构的</li><li>被证明，如果GNN可以将两个图映射到不同的embedding，那这两个图就可以通过WL测试，证明是不同构的</li><li>被证明，如果aggregation和readout方法是内射函数，gnn就可以实现和WL test一样的效果</li><li>Weisfeiler-Lehman (WL) test，参考<a href="https://zhuanlan.zhihu.com/p/90645716" target="_blank" rel="external nofollow noopener noreferrer">什么是Weisfeiler-Lehman(WL)算法和WL Test？</a></li></ol></li><li>等方差和不变形<ol><li>在node level任务上，GNN必须是等方差的，并且在graph-level任务上，必须是不变函数</li><li>GNN的组件必须对节点顺序不变</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/0o8y9o.png" width="500"></li></ol></li><li>Universal approximation，通用逼近<ol><li>一个hidden layer的MLP可以近似任何Borel measurable functions</li><li>GNN的近似能力还没有相关研究</li><li>有研究表明，RecGNN可以近似以任何精度保留展开等价性的任何函数</li><li>被证明，一个不变图网络可以近似图上定义的任意不变函数</li></ol></li></ol><h2 id="GAE"><a href="#GAE" class="headerlink" title="GAE"></a>GAE</h2><ol><li>GAE是将节点映射到latent feature space，并且将其decode为graph information的深度网络架构</li><li>可以用于学习网络embedding，或者生成新图</li><li>主要的GAE如下</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/Zrwv2c.png" width="500"></li></ol><h3 id="Network-Embedding"><a href="#Network-Embedding" class="headerlink" title="Network Embedding"></a>Network Embedding</h3><ol><li>network embedding是node的低维向量表示，保存了节点的拓扑信息</li><li>用encoder提取网络信息进行encode，然后用decoder让网络保存graph拓扑信息，比如PPMI和邻接矩阵</li><li>DNGR<ol><li>用 stacked denoising autoencoder来encode</li></ol></li><li>SDNE<ol><li>用stacked autoencoder来保存接地那first-order proximity and second-order proximity</li><li>对encoder和decoder用两个损失函数</li><li>encoder的损失函数是minimize节点embedding和邻居embedding的距离</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/ky7lyS.png" width="400"></li><li>第二个损失函数确保embedding保存节点second-order proximity节点二阶接近度，minimize节点输入和重构输入之间的distance</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/SLG6Ne.png" width="400"></li></ol></li><li>DNGR和SDNE只考虑了节点关于连接的结构信息，忽略了节点可能包含描述节点本身属性的特征信息</li><li>GAE based on GCN<ol><li>用GCN来encode节点结构信息和feature信息</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/nsbTxn.png" width="500"></li></ol></li><li>(VGAE) Variational graph auto-encoders<ol><li>认为简单重构图的邻接矩阵会导致过拟合</li><li>VGAE是GAE变种，用于学习数据分布</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/qcpSsV.png" width="500"></li></ol></li><li>(ARVGA)Adversarially Regularized Variational Graph Autoencoder<ol><li>用GAN的训练模式，就是discriminator和generator进行competition，discriminator要努力分辨出generator生成的假样本</li><li>ARVGA训练一个encoder来产生经验分布 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.696ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4285.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(849, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(1572, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1850, 0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"></path></g><g data-mml-node="mo" transform="translate(2702, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3146.7, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(3896.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，尽量和先验分布 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.534ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2004 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><g data-mml-node="mo" transform="translate(1615, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>相同</li></ol></li><li>GraphSage<ol><li>和GAE相似，用两层gcn来encode节点特征</li><li>不是优化重构损失，而是认为可以通过负采样的方式来保存两个节点之间的相关性信息</li><li>？<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/cRUEl8.png" width="500"></li><li>这个损失函数强迫距离近的节点有相似的embedding，距离远的节点有不相似的embedding</li></ol></li><li>DGI<ol><li>通过最大化local互信息来驱动local embedding捕获全局结构信息。</li><li>比GraphSAGE提升很多</li></ol></li><li>对于上述方法，他们本质上是通过解决link预测问题来学习网络嵌入的。</li><li>网络稀疏性问题<ol><li>为了减轻学习网络嵌入中的数据稀疏性问题，有人通过随机排列或随机游走将图转换为序列。</li><li>变成序列之后，很多dl的模型都可以用上了</li></ol></li><li>DRNE，Deep Recursive Network Embedding<ol><li>假定节点embedding，需要近似邻居embedding的aggregation</li><li>用lstm进行aggregation</li><li>使用lstm训练embedding而不是直接用lstm生成embedding，避免了LSTM网络对于节点序列的排列不是不变的问题</li></ol></li><li>NetRA，Network Representations with Adversarially Regularized Autoencoders<ol><li>提出了一个通用的encoder-decoder结构，并带有通用loss function</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/CShiaF.png" width="500"></li><li>dis是node embedding z和重构的z之间的distance</li><li>encoder和decoder都是带有随机游走的lstm，输入是从每个节点开始的随机游走seq</li><li>和ARVGA一样，用一个对抗训练的prior来正则化，想起了DAEGC那个里面也是用类似的思想训练P分布和Q分布，但是不是对抗训练</li><li>虽然无视了node permutation variant问题，但是结果是可以的</li></ol></li></ol><h3 id="Graph-Generation"><a href="#Graph-Generation" class="headerlink" title="Graph Generation"></a>Graph Generation</h3><ol><li>GAE可以通过encode-decode得到图的generative distribution，想起来这个也被用在了DAEGC中</li><li>很多用GAE来生成graph generation的都被用来解决分子图生成问题，在药物发现里面有很大实用价值</li><li>一般生成新的网络分两种，一是sequential的一个个生成，另一个是global的一次性生成</li></ol><h4 id="Sequential-manner"><a href="#Sequential-manner" class="headerlink" title="Sequential manner"></a>Sequential manner</h4><ol><li>一步步的生成节点和边</li><li>SMILES<ol><li>把生成过程建模为string representation of molecular graphs</li><li>用CNN和RNN作为encoder和decoder</li><li>是domain-specific的，可以迭代将节点和边迭代添加到增长的图直到满足特定条件，替代解决方案适用于一般图</li></ol></li><li>DeepGMG<ol><li>假定图的可能性是所有节点permutation的sum，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.663ex" xmlns="http://www.w3.org/2000/svg" width="18.457ex" height="2.36ex" role="img" focusable="false" viewBox="0 -750 8157.9 1043.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(503, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(892, 0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(1678, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2344.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(3400.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, -285.4) scale(0.707)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g><g data-mml-node="mi" transform="translate(5076.3, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(5579.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5968.3, 0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(6754.3, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7198.9, 0)"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g><g data-mml-node="mo" transform="translate(7768.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.29ex" height="1ex" role="img" focusable="false" viewBox="0 -431 570 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70B" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path></g></g></g></svg></mjx-container>是一个节点ordering</li><li>生成过程是做一系列决策，决定是否添加节点和边以及连接到哪个节点上</li><li>每步的决策是依赖node state 和生成的图的graph state，用RecGNN</li></ol></li><li>GraphRNN<ol><li>用node级别和graph级别的RNN来建模生成过程</li><li>graph级别的RNN每次添加一个node进node seq，node级别的决定新节点和哪些节点相连，就是一串binary seq</li></ol></li></ol><h3 id="Global-manner"><a href="#Global-manner" class="headerlink" title="Global manner"></a>Global manner</h3><ol><li>就是一次生成整个graph</li><li>GraphVAE，Graph Vari- ational Autoencoder<ol><li>将存在的节点和边建模成独立的随机变量</li><li>假定encoder定义的后验分布q，还有decoder定义的generative distribution p，p分布是高斯先验分布，GraphVAE优化变分下界</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/ssh1X3.png" width="500"></li></ol></li><li>RGVAE，Regularized Graph Variational Au- toencoder<ol><li>在GVAE的基础上，进一步对变分自编码器增加有效性约束，来regularize decoder的输出分布</li></ol></li><li>MolGAN，Molecular Generative Adversarial Network<ol><li>结合了convGNN、GAN、强化学习目标来生成带有所需特点的图</li><li>主要是为了提高generator的真实性authenticity</li><li>生成器输出fake graph和feature matrix，discriminator来分辨fake sample</li><li>还加了一个和discriminator并行的reward network来让生成器获得特定的property</li></ol></li><li>NetGAN结合LSTM和GAN和随机游走方法</li><li>总结来说，sequential manner可能会丢失结构信息，而global manner一次性产生整个图，对大图不友好。</li></ol><h2 id="STGNN"><a href="#STGNN" class="headerlink" title="STGNN"></a>STGNN</h2><p>暂时用不到，先留着后面再看</p><h2 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h2><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><ol><li>引文网络</li><li>生物图网络</li><li>社交网络</li><li>其他</li></ol><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/gpXhMJ.png" alt="alt"></p><h3 id="Evaluation-and-Implementations"><a href="#Evaluation-and-Implementations" class="headerlink" title="Evaluation and Implementations"></a>Evaluation and Implementations</h3><ol><li>node classification<ol><li>大多方法把benchmark data分为train、valid和test</li><li>用在test上的多次运行结果的acc和F1作为评价指标</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/sUzsrL.png" width="600"></li></ol></li><li>graph classification<ol><li>一般是hi十折交叉验证</li><li>很多实验的setting比较模棱两可，并且是很多在模型选择和模型评估的时候可能都有问题</li><li>常见的问题是，每折的外部测试集都用于模型选择和风险评估</li><li>可以用十折cv来进行模型选择，或者用两个cv，一个内部的k折进行模型选择一个外部的k折进行评估</li></ol></li><li>开源实现<ol><li>安利了<a href="https://github.com/rusty1s/pytorch_geometric" target="_blank" rel="external nofollow noopener noreferrer">pytorch geometric</a>和<a href="https://www.dgl.ai/" target="_blank" rel="external nofollow noopener noreferrer">DGL</a>两个图方面的python库</li></ol></li></ol><h3 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h3><ol><li>node classification</li><li>graph classification</li><li>network embedding</li><li>graph generation</li><li>spatial-temporal graph forecasting</li><li>node clustering</li><li>link prediction</li><li>graph partitioning</li></ol><h4 id="CV中应用"><a href="#CV中应用" class="headerlink" title="CV中应用"></a>CV中应用</h4><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/FOw2K1.png" alt="alt"></p><h4 id="NLP中应用"><a href="#NLP中应用" class="headerlink" title="NLP中应用"></a>NLP中应用</h4><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/nGQPPN.png" width="600"> <img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/nGzlcr.png" width="600"><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>还有在Traffic、Recommender system和Chemistry中的应用</p><h2 id="Future-Directions"><a href="#Future-Directions" class="headerlink" title="Future Directions"></a>Future Directions</h2><ol><li>Model depth<ol><li>现在的研究依赖深度网络结构</li><li>有研究表明gcn层数越来越高的时候，模型表现反而下降</li><li>在有限数量的gcn层里，所有节点的representation都会converg到一个单独的node</li><li>所以深度网络结构在gnn上是否是一个好的策略还需要验证</li></ol></li><li>scalability trade-off<ol><li>GNN的可伸缩性是以破坏图形完整性为代价的。</li><li>无论使用抽样还是聚类，模型都会丢失部分图形信息。抽样会丢失一些有影响的节点，聚类会损失一些独特的结构</li><li>需要在scalability和integrity之间trade off</li></ol></li><li><strong>Heterogenity异质性</strong><ol><li>当前的大多数GNN都采用同质图。很难将当前的GNN直接应用于异构图，该图形可能包含不同类型的节点和边，或不同形式的节点和边输入，例如图像和文本。 因此，应开发新的方法来处理异构图</li></ol></li><li><strong>Dynamicty动态性</strong><ol><li>实际场景中图应该是动态变化的，比如节点或者边会新增或者删减，节点输入和边输入也可能随时间变化。</li><li>需要新的graph convolutions来适应这种图的动态特性</li><li>STGNN可以解决部分动态图，但是很少有考虑如何在dynamic spatial relation中使用graph convolutions。</li></ol></li></ol><h2 id="Significant-Reference"><a href="#Significant-Reference" class="headerlink" title="Significant Reference"></a>Significant Reference</h2><p>待补充</p>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;ul&gt;&lt;li&gt;一篇关于GNN的综述，主要讲&lt;ul&gt;&lt;li&gt;namely recurrent graph neural networks,&lt;/li&gt;&lt;li&gt;convolutional graph neural networks,&lt;/li&gt;&lt;li&gt;graph autoencoders,&lt;/li&gt;&lt;li&gt;spatial-temporal graph neural networks&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;文章结构是&lt;ul&gt;&lt;li&gt;GNN发展&lt;/li&gt;&lt;li&gt;GNN分类&lt;/li&gt;&lt;li&gt;GNN模型的overview&lt;/li&gt;&lt;li&gt;GNN模型在各个领域的应用&lt;/li&gt;&lt;li&gt;总结了开源代码和benchmark dataset&lt;/li&gt;&lt;li&gt;当前的challenges和research directions&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Papers" scheme="https://hanielxx.com/categories/Papers/"/>
    
    
    <category term="Review" scheme="https://hanielxx.com/tags/Review/"/>
    
    <category term="GNN" scheme="https://hanielxx.com/tags/GNN/"/>
    
    <category term="Survey" scheme="https://hanielxx.com/tags/Survey/"/>
    
    <category term="IEEE-2021" scheme="https://hanielxx.com/tags/IEEE-2021/"/>
    
  </entry>
  
  <entry>
    <title>GNN和图聚类</title>
    <link href="https://hanielxx.com/MachineLearning/2021-05-09-gnn-graph-clustering"/>
    <id>https://hanielxx.com/MachineLearning/2021-05-09-gnn-graph-clustering</id>
    <published>2021-05-09T05:11:01.000Z</published>
    <updated>2021-05-27T15:30:56.301Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>遇到一个聚类问题，感觉可以建模到GNN上面。记录一下问题和思路。</p><p>包括基于Graph的聚类：Chinese Whisper、Spectral Cluster，基于GNN的聚类：基于GCN、CDP(ECCV2018)、DAEGC(IJCAI2019), AGC(IJCAI2019)</p><a id="more"></a><p><strong>特点：</strong></p><ol><li>无监督的多模态数据</li><li>聚类成N个类别，这个类别数目不确定</li><li>会有新节点加入</li><li>如果训练端到端的模型，成本可能很高，每次都得聚类一次</li><li>图的数量很大，并且大小不确定，从几十到大几百上千不等</li><li>node之间的link很稀疏并且是顺序连接，部分连边可能并不代表真实的联系</li><li>如果是自己设置图的连边，那阈值也都不同，如果不建模到图上，就不存在连边问题，直接是一堆样本进行聚类</li><li>或许可以用集数作为连边信息，只要是集数连着就有边，没有集数就没有边，semi-supervised</li></ol><p><strong>几个方向：</strong></p><ol><li><del>Link Prediction</del><ol><li>不适用，因为这个是在有命名实体的情况下预测连接</li><li>想想知识图谱</li></ol></li><li>Graph Clustering<ol><li>(DAEGC)《Attributed Graph Clustering via Adaptive Graph Convolution》</li><li>比较像</li></ol></li><li>Community Detection<ol><li>发现共享相似属性或功能的顶点组来了解网络数据</li><li>有很多基于顶点embedding的聚类算法</li><li>《CommunityGAN: Community Detection with Generative Adversarial Nets》</li><li>《GEMSEC: Graph Embedding with Self Clustering》这个不太行，nodes are clustered into a fixed number of groups，而且需要图结构。但是，它是在sequence based graph（拥有相似邻居的节点在空间上更接近）上做的</li></ol></li><li><del>Graph Reconstruction</del><ol><li>《Multi-View Self-Constructing Graph Convolutional Networks With Adaptive Class Weighting Loss for Semantic Segmentation》直接通过输入feature构建基础图，而不是依赖先验知识。这居然是用在图像的语义分割上…</li></ol></li><li><del>dynamic graph clustering</del><ol><li>《Interpretable Clustering on Dynamic Graphs with Recurrent Graph Neural Networks》用一个动态随机块模型来捕获这些变化，并提出一种基于衰减的简单聚类算法，该算法基于节点之间的加权连接对节点进行聚类，其中权重随时间以固定速率降低。还基于半监督图聚类提出了两种新的RNN-GCN结构</li><li>节点之间的连接以及节点的聚类成员身份可能会随时间而变化，但是并不是新加入节点，和合集的场景不太一样</li></ol></li><li>或许不应该focus在端到端的模型上面，应该看看传统的机器学习算法，毕竟可以得到确定性的结果同时迭代新的数据。</li><li>可以利用上正则的匹配结果，作为部分label进行semi-supervised learning，这样的目的其实是为了提高召回，而不是precision</li></ol></div><h2 id="基于Graph"><a href="#基于Graph" class="headerlink" title="基于Graph"></a>基于Graph</h2><h3 id="Chinese-Whisper"><a href="#Chinese-Whisper" class="headerlink" title="Chinese Whisper"></a>Chinese Whisper</h3><p>基于图的聚类算法，代表性的基础算法是Chinese Whisper</p><ul><li>初始化：将所有的样本点初始化为不同的类，自下而上的进行聚类</li><li>建图：<strong>根据样本点之间的距离建图</strong>，距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高。是否连边需要<strong>设置阈值</strong></li><li>迭代：<ol><li>随机选取一个节点i开始，在其相连节点中选取边权重最大者j, 并将i归为节点j类（若相连节点中有多个节点属于同一类，则将这些权重相加再做比较）</li><li>遍历所有节点后，重复迭代至满足迭代次数</li></ol></li><li>优点：不用设定k，只需指定相似度阈值</li><li>缺点：<ol><li>对向量的要求度较高，需要向量能够增大类间距离，减小类内距离</li><li>由于随机初始化，每次聚类的结果有可能不一致</li></ol></li><li>优化<ol><li>CW需要自行设置相似度阈值，且该阈值敏感度较高，后续优化方向是自动选择阈值</li><li>Linkage Based Face Clustering via GCN（CVPR2019）</li></ol></li></ul><h3 id="谱聚类spectral-clustering"><a href="#谱聚类spectral-clustering" class="headerlink" title="谱聚类spectral clustering"></a>谱聚类spectral clustering</h3><ul><li><a href="https://www.cnblogs.com/pinard/p/6221564.html" target="_blank" rel="external nofollow noopener noreferrer">刘建平-谱聚类原理</a></li><li>解决的问题是kmeans中无法对非欧式空间的分布进行聚类的问题，主要原理是对聚类数据进行变换，然后进行k-means聚类，之后再还原到原空间。</li><li>算法思路：它的主要思想是把所有的数据看做空间中的点，这些点之间可以用边连接起来。距离较远的两个点之间的边权重值较低，而距离较近的两个点之间的边权重值较高，通过对所有数据点组成的图进行切图，让切图后不同的<strong>子图间边权重和尽可能的低</strong>，而<strong>子图内的边权重和尽可能的高</strong>，从而达到聚类的目的。</li><li>度矩阵<ul><li>度<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.842ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 814 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>定义为和它相连的所有边的权重之和，对于没有边连接的两个点之间<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="5.768ex" height="1.692ex" role="img" focusable="false" viewBox="0 -666 2549.6 748"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2049.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>。权重可以自己输入</li><li>得到<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="4.009ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 1772 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(1172, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>的度矩阵𝐷，是个对角阵</li></ul></li><li>邻接矩阵<ul><li>所有点之间的权重值，我们可以得到图的邻接矩阵</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="4.009ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 1772 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(1172, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>，第i行的第j个值对应我们的权重 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.944ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 1301.3 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container></li><li>通过样本点距离度量的相似矩阵𝑆来获得邻接矩阵𝑊。构建邻接矩阵𝑊的方法有三类。<ul><li>𝜖-邻近法：<ul><li>用节点间欧式距离和阈值𝜖比，取 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="24.227ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 10708.4 999.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1579.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2634.8, 0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mtext" transform="translate(3040.8, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(3290.8, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3635.8, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mtext" transform="translate(4185.8, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(4435.8, 0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(4955.8, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5300.8, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mtext" transform="translate(5769.8, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mo" transform="translate(6297.6, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mtext" transform="translate(7353.4, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(7603.4, 0)"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"></path></g><g data-mml-node="mtext" transform="translate(8009.4, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(8259.4, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(8725.4, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(9023.4, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(9492.4, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mtext" transform="translate(9958.4, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mn" transform="translate(10208.4, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container></li><li>两点间的权重要不就是𝜖,要不就是0，距离远近度量很不精确，因此在实际应用中，我们很少使用𝜖-邻近法。</li></ul></li><li>K邻近法：<ul><li>KNN算法遍历所有的样本点，取每个样本最近的k个点作为近邻。只有距离最近的k个点之间的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="5.768ex" height="1.597ex" role="img" focusable="false" viewBox="0 -666 2549.6 706"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mo" transform="translate(993.8, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2049.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>，图中用的RBF高斯核</li><li>RBF高斯核，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.871ex" xmlns="http://www.w3.org/2000/svg" width="23.416ex" height="3.435ex" role="img" focusable="false" viewBox="0 -1133.3 10349.9 1518.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mo" transform="translate(889, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1278, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1850, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msup" transform="translate(2294.7, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(3111.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3777.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(4833.7, 0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444, 0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972, 0)"></path></g><g data-mml-node="mo" transform="translate(6528.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(7306.3, 0)"><g data-mml-node="mrow" transform="translate(220, 543.6) scale(0.707)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(278, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(556, 0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1128, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(1906, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(572, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(2722.5, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msubsup" transform="translate(3000.5, 0)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(278, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(278, -287.9) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(1000.5, -377.4) scale(0.707)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="msup" transform="translate(500, 0)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mn" transform="translate(571, 289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><rect width="2803.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container> 值随距离增大而减小，介于0-1之间。也会一种现成的相似性度量表示</li><li>重构之后的邻接矩阵W非对称，但算法需要对称邻接矩阵</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/0nS2HD.png" alt="0nS2HD"></li></ul></li><li>全连接法：<ul><li>所有的点之间的权重值都大于0，因此称之为全连接法</li><li>可以选择不同的核函数来定义边权重，常用的有多项式核函数，高斯核函数和Sigmoid核函数。最常用的是高斯核函数RBF，此时相似矩阵和邻接矩阵相同</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.871ex" xmlns="http://www.w3.org/2000/svg" width="19.067ex" height="3.445ex" role="img" focusable="false" viewBox="0 -1137.7 8427.4 1522.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1579.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2634.8, 0)"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444, 0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972, 0)"></path></g><g data-mml-node="mo" transform="translate(4329.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5107.5, 0)"><g data-mml-node="mrow" transform="translate(220, 548.1) scale(0.707)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(278, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msub" transform="translate(556, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1422, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2200, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(3113.3, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="msubsup" transform="translate(3391.3, 0)"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mn" transform="translate(278, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mn" transform="translate(278, -287.9) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(1138.6, -377.4) scale(0.707)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="msup" transform="translate(500, 0)"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mn" transform="translate(571, 289) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><rect width="3079.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li></ul></li></ul></li></ul></li><li>拉普拉斯矩阵<ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="11.568ex" height="1.731ex" role="img" focusable="false" viewBox="0 -683 5113 765"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(958.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2014.6, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mo" transform="translate(3064.8, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4065, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g></g></g></svg></mjx-container>，其中D是度矩阵对角阵， W是邻接矩阵</li><li>L是对称矩阵，这可以由𝐷和𝑊都是对称矩阵而得</li><li>L是对称矩阵，则它的所有的特征值都是实数</li><li>对于任意的向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.972ex" xmlns="http://www.w3.org/2000/svg" width="27.023ex" height="2.876ex" role="img" focusable="false" viewBox="0 -841.7 11944.1 1271.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(603, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(1150.8, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(1831.8, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2659.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(3715.4, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(623, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(1035, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1813, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(6623.6, 0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7924.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(8313.8, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(9320, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(10320.2, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(490, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="msup" transform="translate(11151.6, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container>。可以通过 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="23.112ex" height="2.368ex" role="img" focusable="false" viewBox="0 -841.7 10215.4 1046.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(603, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(1150.8, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(1831.8, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(2659.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(3715.4, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(603, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(4866.2, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(5694.2, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(6466.4, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msup" transform="translate(7466.6, 0)"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mi" transform="translate(603, 363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mi" transform="translate(8617.4, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(9665.4, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container>得到</li><li>L是半正定的，且对应的n个实数特征值都大于等于0</li><li>L是半正定的，且对应的n个实数特征值都大于等于0，即 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="18.422ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 8142.6 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(777.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1833.6, 0)"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="mn" transform="translate(583, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(3097.9, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="msub" transform="translate(4153.7, 0)"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="mn" transform="translate(583, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(5418, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(6196, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(6640.7, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="msub" transform="translate(7085.3, 0)"><g data-mml-node="mi"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>， 且最小的特征值为0</li></ul></li><li>子图<ul><li>点集<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.74ex" height="1.595ex" role="img" focusable="false" viewBox="0 -683 769 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container>的的一个子集<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.203ex" height="1.71ex" role="img" focusable="false" viewBox="0 -716 2741.6 756"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1027.8, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1972.6, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.71ex" xmlns="http://www.w3.org/2000/svg" width="17.512ex" height="2.406ex" role="img" focusable="false" viewBox="0 -750 7740.1 1063.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(485, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(970, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(1268, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1657, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2407, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3073.8, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z" transform="translate(278, 0)"></path></g><g data-mml-node="munder" transform="translate(4407.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1012, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g></g></g><g data-mml-node="msub" transform="translate(6926.1, 0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(520, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></li></ul></li><li>切图<ul><li>就是把 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.013ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3541.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(786, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1175, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(1944, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2388.7, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(3152.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>切成相互没有连接的k个子图，然后合起来就是完整的图G</li><li>任意两个切图AB之间权重是： <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.972ex" xmlns="http://www.w3.org/2000/svg" width="23.596ex" height="2.669ex" role="img" focusable="false" viewBox="0 -750 10429.3 1179.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1048, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1437, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(2187, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2631.7, 0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mo" transform="translate(3390.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4057.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munder" transform="translate(5113.2, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(1012, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mo" transform="translate(1762, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2040, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(2452, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mi" transform="translate(3119, 0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g></g></g><g data-mml-node="msub" transform="translate(9128, 0)"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="TeXAtom" transform="translate(716, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container></li><li>定义切图cut： <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="37.652ex" height="3.153ex" role="img" focusable="false" viewBox="0 -1048.5 16642.2 1393.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1005, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(1366, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(1755, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mn" transform="translate(750, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2908.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3353.2, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mn" transform="translate(750, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(4673.4, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="msub" transform="translate(6012.1, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(7180.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7847.3, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(8903.1, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(9863.3, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(12283.6, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(13331.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(13720.6, 0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(14764.5, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mover" transform="translate(15209.2, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 531.3) scale(0.707)"><svg width="1476.4" height="246" x="0" y="444" viewBox="369.1 444 1476.4 246"><path data-c="AF" d="M69 544V590H430V544H69Z" transform="scale(4.429, 1)"></path></svg></g></g><g data-mml-node="mo" transform="translate(16253.2, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.362ex" height="2.729ex" role="img" focusable="false" viewBox="0 -1048.5 1044 1206.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mover"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 531.3) scale(0.707)"><svg width="1476.4" height="246" x="0" y="444" viewBox="369.1 444 1476.4 246"><path data-c="AF" d="M69 544V590H430V544H69Z" transform="scale(4.429, 1)"></path></svg></g></g></g></g></svg></mjx-container>其实就是V中A的补集</li><li>直接最小化切图，不是最优的切图</li><li>谱聚类有两种切图方式：RatioCut和cut</li><li>RatioCut：<ul><li>除了最小化切图，还考虑最大化每个子图点的个数</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/px7w07.png" alt="px7w07"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/eTmcmA.png" alt="eTmcmA"></li></ul></li><li>NutCut：<ul><li>Ncut切图和RatioCut切图很类似，但是把Ratiocut的分母|𝐴𝑖|换成𝑣𝑜𝑙(𝐴𝑖). 由于子图样本的个数多并不一定权重就大，我们切图时基于权重也更合我们的目标，因此一般来说Ncut切图优于RatioCut切图</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/rpF8Yi.png" alt="rpF8Yi"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/8nzn56.png" alt="8nzn56"></li></ul></li><li>总结：<ul><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/J5TpAk.png" alt="J5TpAk"></li></ul></li></ul></li><li>流程<ul><li>输入：n个样本, 类别k</li><li>根据输入的相似度的衡量方式对样本建图，根据相似图建立邻接矩阵W</li><li>计算出拉普拉斯矩阵L, 其中L=D-W， D为度矩阵</li><li>计算L的最小的k个特征向量u1, u2,…,uk（此步相当于降维),将这些向量组成为n*k维的矩阵U</li><li>将U中的每一行作为一个样本，共n个样本，使用k-means对这n个样本进行聚类</li><li>得到簇划分C(c1,c2,…ck).</li></ul></li><li>缺点是要设置K，和k-means一样</li><li>谱聚类算法的主要优点有：<ul><li>谱聚类只需要数据之间的相似度矩阵，因此对于处理稀疏数据的聚类很有效。这点传统聚类算法比如K-Means很难做到</li><li>由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。</li></ul></li><li>算法的主要缺点有：<ul><li>如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。</li><li>聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。</li></ul></li></ul><h2 id="基于GNN"><a href="#基于GNN" class="headerlink" title="基于GNN"></a>基于GNN</h2><h3 id="基于GCN聚类"><a href="#基于GCN聚类" class="headerlink" title="基于GCN聚类"></a>基于GCN聚类</h3><p>paper： Learning to Cluster Faces on Affinity Graphy(CVPR2019)</p><p>流程：</p><ol><li>将样本点建图(如何建图可以参照前面图聚类算法，实际上有很多种构建方式，主要取决于相似度如何衡量以及如何建边）</li><li>Cluster Proposal: 从前面建好的图中选择出多个sub-graph，也就是proposal，就是子图的概念。主要是为了降低计算消耗，在子图上去做聚类。每一个cluster proposal里的向量数是比较少的。比如说一个包含1M向量的全图可以转化为50K包含20向量的proposal</li><li>Cluster Detection: 将上面的Cluster Proposal利用GCN提取特征后聚类，计算预测类别与gt的差异，评价指标为，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.821ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4340.8 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(989, 0)"><path data-c="1D448" d="M107 637Q73 637 71 641Q70 643 70 649Q70 673 81 682Q83 683 98 683Q139 681 234 681Q268 681 297 681T342 682T362 682Q378 682 378 672Q378 670 376 658Q371 641 366 638H364Q362 638 359 638T352 638T343 637T334 637Q295 636 284 634T266 623Q265 621 238 518T184 302T154 169Q152 155 152 140Q152 86 183 55T269 24Q336 24 403 69T501 205L552 406Q599 598 599 606Q599 633 535 637Q511 637 511 648Q511 650 513 660Q517 676 519 679T529 683Q532 683 561 682T645 680Q696 680 723 681T752 682Q767 682 767 672Q767 650 759 642Q756 637 737 637Q666 633 648 597Q646 592 598 404Q557 235 548 205Q515 105 433 42T263 -22Q171 -22 116 34T60 167V183Q60 201 115 421Q164 622 164 628Q164 635 107 637Z"></path></g><g data-mml-node="mo" transform="translate(1756, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2145, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(2896, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3562.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="7.396ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3269 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(989, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1740, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2129, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(2880, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>Cluster Segmentation: 上面那个模块是检测，这个模块是剔除负样本点，评价指标和上面那一步是一样的。使用的loss均为MSE，训练然后梯度下降</li></ol><h3 id="CDP-ECCV2018"><a href="#CDP-ECCV2018" class="headerlink" title="CDP(ECCV2018)"></a>CDP(ECCV2018)</h3><p>是针对人脸识别提出的优化算法，解决的是在大数据集下传统算法聚类性能过差的问题</p><ul><li>是<strong>半监督</strong>方式的</li><li>该算法有3个部分<ul><li>base model, 建一个大的<strong>KNN图</strong></li><li>committe model(决策者模型),多个model, 对大图每一条边，判断是否断开，得到多个子图</li><li>Mediator(融合模型), 根据多个committe的结果来判断两个节点之间的边是保留还是断开，多数投票，最后输出聚类结果</li></ul></li><li>各模块<strong>都用GCN训练</strong>，<strong>非设置阈值</strong></li><li>优点：<ul><li>只探索局部关系，因为它的主要计算量集中在两个节点组成的pairs的关系，而不是整个图的关系，计算效率较高，可以用于大规模数据集</li></ul></li></ul><blockquote><p>We first provide an overview of the proposed approach. Our approach consists of three stages:</p><ol><li><p>Supervised initialization - Given a small portion of labeled data, we separately train the base model and committee members in a fully-supervised manner. More precisely, the base model B and all the N committee members {Ci|i = 1, 2, . . . , N} learn a mapping from image space to feature space Z using labeled data Dl. For the base model, this process can be denoted as the mapping: FB : Dl 􏰉→ Z, and as for committee members: FCi : Dl 􏰉→ Z, i = 1,2,…,N.</p></li><li><p>Consensus-driven propagation - CDP is applied on unlabeled data to se- lect valuable samples and conjecture labels thereon. The framework is shown in Fig. 1. We use the trained models from the first stage to extract deep features for unlabeled data and create k-NN graphs. The “committee” ensures the diver- sity of the graphs. Then a “mediator” network is designed to aggregate diverse opinions in the local structure of k-NN graphs to select meaningful pairs. With the selected pairs, a consensus-driven graph is created on the unlabeled data and nodes are assigned with pseudo labels via our label propagation algorithm.</p></li><li><p>Joint training using labeled and unlabeled data - Finally, we re-train the base model with labeled data, and unlabeled data with pseudo labels, in a multi-task learning framework.</p></li></ol></blockquote><h3 id="DAEGC-IJCAI2019"><a href="#DAEGC-IJCAI2019" class="headerlink" title="DAEGC(IJCAI2019)"></a>DAEGC(IJCAI2019)</h3><p>《Attributed Graph Clustering: A Deep Attentional Embedding Approach》</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/MGUaZE.png" alt="MGUaZE"></p><p>强调的是goal-directed的embedding方法，用attention based网络捕捉相邻节点之间的importance，并且结合了structure和node content信息作为compact representation。通过graph embedding产生的soft labels进行自监督聚类. 自监督过程和graph embedding优化过程是joint learned and optimized。</p><p>大致上就是先用auto encoder训练，然后用k-means得到一个初始化的cluster center，然后再做自监督的embedding优化，同时更新迭代簇中心位置。</p><p>这篇文章会在后面单独写一个笔记。</p><ul><li>Require:<ul><li>Graph G with n nodes;</li><li>Number of clusters k;</li><li>Number of iterations Iter;</li><li>Target distribution update interval T;</li><li>Clustering Coefficient γ.</li></ul></li><li>Output:<ul><li>k disjoint groups</li></ul></li></ul><p>模型结构：<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/MwcbDs.png" alt="MwcbDs"></p><p>实验结果：<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/fYsPmB.png" alt="fYsPmB"></p><h3 id="AGC-IJCAI2019"><a href="#AGC-IJCAI2019" class="headerlink" title="AGC(IJCAI2019)"></a>AGC(IJCAI2019)</h3><p>graph-clustering SOTA论文</p><p>《Attributed Graph Clustering via Adaptive Graph Convolution》</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/in0VDA.png" alt="in0VDA"></p><p>在使用k-order GCN之后，用了谱聚类。问题是谱聚类也要指定k值，所以论文里面是通过实验找到最好的k值…</p><p>和上面的DAEGC挺像的。</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><ol><li>算法不能提前指定类别数</li><li>边的确定<ol><li>ep是merge了pid_ep和photo_ep</li><li>根据ep来建立连接关系，每个1都和2相连，每个2都和3相连</li><li>按照时间顺序连接所有pid</li><li>所有的-1，按照时间顺序，连接前后W个视频，W是窗口大小，设为3</li></ol></li><li>node content X：使用视频的hetu_embedding，128维</li><li>edge weight W: similarity between nodes</li><li>难点：<ol><li>K值无法确定，且会有新增视频进来，K值一直在增加</li><li>如果每次新加入一个节点就得重新train，会很慢</li><li>每次的输入是一整个graph，输出是不同的类，不同用户之间的cluster肯定是不同的。所以要是考虑所有用户，那类别数是没上限的，只能考虑单个用户的图。</li><li>每个用户是一个graph，是不是都得训模型，还是可以有通用的提取特征的模型，直接inference每个用户图</li><li>最后要得到的是node embedding给下游模型做cluster，但是涉及到不同cluster中的节点数不同，有的多有的少。如果在无监督的情况下，想用密度聚类聚在一起，存在1）节点多的cluster召回高的时候，节点少的cluster会被聚成一个类；2）节点少的cluster准确高的时候，节点多的cluster召回低</li></ol></li></ol>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;遇到一个聚类问题，感觉可以建模到GNN上面。记录一下问题和思路。&lt;/p&gt;&lt;p&gt;包括基于Graph的聚类：Chinese Whisper、Spectral Cluster，基于GNN的聚类：基于GCN、CDP(ECCV2018)、DAEGC(IJCAI2019), AGC(IJCAI2019)&lt;/p&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="Clustering" scheme="https://hanielxx.com/tags/Clustering/"/>
    
    <category term="Graph" scheme="https://hanielxx.com/tags/Graph/"/>
    
    <category term="GNN" scheme="https://hanielxx.com/tags/GNN/"/>
    
    <category term="UnstructuredData" scheme="https://hanielxx.com/tags/UnstructuredData/"/>
    
  </entry>
  
  <entry>
    <title>AllenNLP框架学习笔记（二）</title>
    <link href="https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2"/>
    <id>https://hanielxx.com/MachineLearning/2021-04-26-allennlp-notes-2</id>
    <published>2021-04-26T12:11:01.000Z</published>
    <updated>2021-05-25T15:36:53.752Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>进一步理解和使用AllenNLP这个框架，续前面的<a href="https://hanielxx.com/MachineLearning/2021-04-14-allennlp-notes">AllenNLP框架学习笔记（一）</a></p></div><a id="more"></a><h2 id="数据相关"><a href="#数据相关" class="headerlink" title="数据相关"></a>数据相关</h2><h3 id="Fields"><a href="#Fields" class="headerlink" title="Fields"></a>Fields</h3><p>所有的数据都被包装成<code>Fields</code>类，一个field包含一个数据样本，在模型中会被转换为tensor作为输入和输出。</p><p>有很多种类的fields类</p><ul><li>LabelField</li><li>MultiLabelField</li><li>SequenceLabelField</li><li>SpanField</li><li>ListField</li><li>ArrayField</li></ul><p><img src="https://i.loli.net/2021/04/26/cezXt9NnbB1foYd.png"></p><p>用法参考：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># To create fields, simply pass the data to constructor.</span></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> Don't worry about the token_indexers too much for now. We have a whole</span></span><br><span class="line"><span class="comment"># chapter on why TextFields are set up this way, and how they work.</span></span><br><span class="line">tokens = [Token(<span class="string">"The"</span>), Token(<span class="string">"best"</span>), Token(<span class="string">"movie"</span>), Token(<span class="string">"ever"</span>), Token(<span class="string">"!"</span>)]</span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers=token_indexers)</span><br><span class="line"></span><br><span class="line">label_field = LabelField(<span class="string">"pos"</span>)</span><br><span class="line"></span><br><span class="line">sequence_label_field = SequenceLabelField(</span><br><span class="line">    [<span class="string">"DET"</span>, <span class="string">"ADJ"</span>, <span class="string">"NOUN"</span>, <span class="string">"ADV"</span>, <span class="string">"PUNKT"</span>], text_field</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can use print() fields to see their content</span></span><br><span class="line">print(text_field)</span><br><span class="line">print(label_field)</span><br><span class="line">print(sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Many of the fields implement native python methods in intuitive ways</span></span><br><span class="line">print(len(sequence_label_field))</span><br><span class="line">print(label <span class="keyword">for</span> label <span class="keyword">in</span> sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to create empty fields of the same type</span></span><br><span class="line">print(text_field.empty_field())</span><br><span class="line">print(label_field.empty_field())</span><br><span class="line">print(sequence_label_field.empty_field())</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can count vocabulary items in fields</span></span><br><span class="line">counter: Dict[str, Dict[str, int]] = defaultdict(Counter)</span><br><span class="line">text_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line">label_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line">sequence_label_field.count_vocab_items(counter)</span><br><span class="line">print(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create Vocabulary for indexing fields</span></span><br><span class="line">vocab = Vocabulary(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to turn themselves into tensors</span></span><br><span class="line">text_field.index(vocab)</span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> in practice, we will batch together instances and use the maximum padding</span></span><br><span class="line"><span class="comment"># lengths, instead of getting them from a single instance.</span></span><br><span class="line"><span class="comment"># You can print this if you want to see what the padding_lengths dictionary looks</span></span><br><span class="line"><span class="comment"># like, but it can sometimes be a bit cryptic.</span></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line">print(text_field.as_tensor(padding_lengths))</span><br><span class="line"></span><br><span class="line">label_field.index(vocab)</span><br><span class="line">print(label_field.as_tensor(label_field.get_padding_lengths()))</span><br><span class="line"></span><br><span class="line">sequence_label_field.index(vocab)</span><br><span class="line">padding_lengths = sequence_label_field.get_padding_lengths()</span><br><span class="line">print(sequence_label_field.as_tensor(padding_lengths))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fields know how to batch tensors</span></span><br><span class="line">tensor1 = label_field.as_tensor(label_field.get_padding_lengths())</span><br><span class="line"></span><br><span class="line">label_field2 = LabelField(<span class="string">"pos"</span>)</span><br><span class="line">label_field2.index(vocab)</span><br><span class="line">tensor2 = label_field2.as_tensor(label_field2.get_padding_lengths())</span><br><span class="line"></span><br><span class="line">batched_tensors = label_field.batch_tensors([tensor1, tensor2])</span><br><span class="line">print(batched_tensors)</span><br></pre></td></tr></table></figure><h3 id="instances"><a href="#instances" class="headerlink" title="instances"></a>instances</h3><p>一个Instance是一个模型预测的原子单位，是Fields类实例的集合，也是dataset的组成单位。fields-&gt;instances-&gt;datasets。</p><p>Instances用dataset reader创建，然后可以从里面获得<code>Vocabulary</code>。vocab可以把<code>Fields</code>map到id上。然后这些instances会被转换为batch tensor输入到模型中。</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/others/I6x77E.png" alt="I6x77E"></p><p>通过把field name和对应fields的字典传进constructor，可以创建instances。instances可以转换为field name和对应tensors的字典，每个field name对应的tensor可以被batch组织进模型中。</p><p>而且每个Field 名字是很重要的，因为最后包含tensor的字典会和key一起传进模型，<strong>所以要和模型forward方法中的参数保持一致。</strong></p><p>示例代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create Fields</span></span><br><span class="line">tokens = [Token(<span class="string">"The"</span>), Token(<span class="string">"best"</span>), Token(<span class="string">"movie"</span>), Token(<span class="string">"ever"</span>), Token(<span class="string">"!"</span>)]</span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">text_field = TextField(tokens, token_indexers=token_indexers)</span><br><span class="line"></span><br><span class="line">label_field = LabelField(<span class="string">"pos"</span>)</span><br><span class="line"></span><br><span class="line">sequence_label_field = SequenceLabelField(</span><br><span class="line">    [<span class="string">"DET"</span>, <span class="string">"ADJ"</span>, <span class="string">"NOUN"</span>, <span class="string">"ADV"</span>, <span class="string">"PUNKT"</span>], text_field</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create an Instance</span></span><br><span class="line">fields: Dict[str, Field] = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: text_field,</span><br><span class="line">    <span class="string">"label"</span>: label_field,</span><br><span class="line">&#125;</span><br><span class="line">instance = Instance(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can add fields later</span></span><br><span class="line">instance.add_field(<span class="string">"label_seq"</span>, sequence_label_field)</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can simply use print() to see the instance's content</span></span><br><span class="line">print(instance)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a Vocabulary</span></span><br><span class="line">counter: Dict[str, Dict[str, int]] = defaultdict(Counter)</span><br><span class="line">instance.count_vocab_items(counter)</span><br><span class="line">vocab = Vocabulary(counter)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert all strings in all of the fields into integer IDs by calling index_fields()</span></span><br><span class="line">instance.index_fields(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instances know how to turn themselves into a dict of tensors.  When we call this</span></span><br><span class="line"><span class="comment"># method in our data code, we additionally give a `padding_lengths` argument.</span></span><br><span class="line"><span class="comment"># We will pass this dictionary to the model as **tensors, so be sure the keys</span></span><br><span class="line"><span class="comment"># match what the model expects.</span></span><br><span class="line">tensors = instance.as_tensor_dict()</span><br><span class="line">print(tensors)</span><br></pre></td></tr></table></figure><p>基本上是：token-&gt; token_indexers-&gt;text_field-&gt;label_field-&gt;fields-&gt;instance-&gt;counter-&gt;instance.count_vocab_items(counter)-&gt;vocab-&gt;instance.index_fields()</p><p>待补充</p><h2 id="模型相关"><a href="#模型相关" class="headerlink" title="模型相关"></a>模型相关</h2><p>中间待补充</p><h2 id="通用的架构"><a href="#通用的架构" class="headerlink" title="通用的架构"></a>通用的架构</h2><p>待补充</p><h2 id="特征表示相关"><a href="#特征表示相关" class="headerlink" title="特征表示相关"></a>特征表示相关</h2><h3 id="文本到特征"><a href="#文本到特征" class="headerlink" title="文本到特征"></a>文本到特征</h3><p>要将文本进行编码</p><ol><li><code>Tokenizer</code>将文本拆分成独立的<code>Tokens</code></li><li>用<code>TextField, TokenIndexer, and Vocabulary</code>将Tokens转换成index</li><li>用<code>TextFieldEmbedder</code>获得每个Token的编码，只有这一步的参数是learnable的</li></ol><p>即Text-&gt;Tokens-&gt;Ids-&gt;Vectors，前两步骤有<code>DatasetReader</code>负责，最后一步由<code>Model</code>负责。</p><p>编码方式很多，常见的Glove、Word2vec、字符级别的CNN、POS tag embedding、结合Glove和CNN，以及<strong>wordpieces级别的BERT</strong></p><h3 id="Tokenizer"><a href="#Tokenizer" class="headerlink" title="Tokenizer"></a>Tokenizer</h3><p>主要有三种方式tokenize</p><ul><li>字符（包括空格），Characters (“AllenNLP is great” → [“A”, “l”, “l”, “e”, “n”, “N”, “L”, “P”, “ “, “i”, “s”, “ “, “g”, “r”, “e”, “a”, “t”])</li><li>Wordpieces (“AllenNLP is great” → [“Allen”, “##NL”, “##P”, “is”, “great”])</li><li>Words (“AllenNLP is great” → [“AllenNLP”, “is”, “great”])</li></ul><p>常用：</p><ul><li>SpacyTokenizer</li><li>PretrainedTransformerTokenizer, which uses a tokenizer from Hugging Face’s transformers library</li><li>CharacterTokenizer, which splits a string up into individual characters, including whitespace.</li></ul><p>每个tokenizer都实现了<code>tokenize()</code>方法，会返回Tokens列表。一个Token是一个轻量级的dataclass</p><h3 id="TextFields"><a href="#TextFields" class="headerlink" title="TextFields"></a>TextFields</h3><p>A TextField takes a list of Tokens from a Tokenizer and represents each of them as an array that can be converted into a vector by the model</p><p>TextFields读入Tokens列表，然后把每个token表示成一个idx。</p><p>包括这些方法，主要是给TokenIndexers负责，</p><ul><li>counting vocabulary items</li><li>converting strings to integers and then tensors</li><li>batching together several tensors with proper padding</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = ...  <span class="comment"># Whatever tokenizer you want</span></span><br><span class="line">sentence = <span class="string">"We are learning about TextFields"</span></span><br><span class="line">tokens = tokenizer.tokenize(sentence)</span><br><span class="line">token_indexers = &#123;...&#125;  <span class="comment"># we'll talk about this in the next section</span></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">...</span><br><span class="line">instance = Instance(&#123;<span class="string">"sentence"</span>: text_field, ...&#125;)</span><br></pre></td></tr></table></figure><h3 id="TokenIndexer"><a href="#TokenIndexer" class="headerlink" title="TokenIndexer"></a>TokenIndexer</h3><p>所有的Token idx都从2开始，0是padding，1是unk</p><p>可以组合使用不同的TokenIndexer，然后进行embedding的融合。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splits text into words (instead of wordpieces or characters).</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with both an id from a vocabulary and a sequence of</span></span><br><span class="line"><span class="comment"># characters.</span></span><br><span class="line">token_indexers: Dict[str, TokenIndexer] = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text ."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The setup here is the same as what we saw above.</span></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line"><span class="comment"># Note now that we have two entries in this output dictionary,</span></span><br><span class="line"><span class="comment"># one for each indexer that we specified.</span></span><br><span class="line">print(<span class="string">"Combined tensor dictionary:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we split text into words with part-of-speech tags, using Spacy's POS tagger.</span></span><br><span class="line"><span class="comment"># This will result in the `tag_` variable being set on each `Token` object, which</span></span><br><span class="line"><span class="comment"># we will read in the indexer.</span></span><br><span class="line">tokenizer = SpacyTokenizer(pos_tags=<span class="literal">True</span>)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with (1) an id from a vocabulary, (2) a sequence of</span></span><br><span class="line"><span class="comment"># characters, and (3) part of speech tag ids.</span></span><br><span class="line">token_indexers = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">    <span class="string">"pos_tags"</span>: SingleIdTokenIndexer(namespace=<span class="string">"pos_tag_vocab"</span>, feature_name=<span class="string">"tag_"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Spacy tokens:"</span>, tokens)</span><br><span class="line">print(<span class="string">"POS tags:"</span>, [token.tag_ <span class="keyword">for</span> token <span class="keyword">in</span> tokens])</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"Tensor dict with POS tags:"</span>, tensor_dict)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Spacy models <span class="string">'en_core_web_sm'</span> <span class="keyword">not</span> found.  Downloading <span class="keyword">and</span> installing.</span><br><span class="line">Tokens: [This, <span class="keyword">is</span>, some, text, .]</span><br><span class="line">Combined tensor <span class="built_in">dictionary</span>: &#123;<span class="string">'tokens'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])&#125;, <span class="string">'token_characters'</span>: &#123;<span class="string">'token_characters'</span>: tensor([[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">12</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>]])&#125;&#125;</span><br><span class="line">✔ Download <span class="keyword">and</span> installation successful</span><br><span class="line">You can now load the package via spacy.load(<span class="string">'en_core_web_sm'</span>)</span><br><span class="line">Spacy tokens: [This, <span class="keyword">is</span>, some, text, .]</span><br><span class="line">POS tags: [<span class="string">'DT'</span>, <span class="string">'VBZ'</span>, <span class="string">'DT'</span>, <span class="string">'NN'</span>, <span class="string">'.'</span>]</span><br><span class="line">Tensor dict with POS tags: &#123;<span class="string">'tokens'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])&#125;, <span class="string">'token_characters'</span>: &#123;<span class="string">'token_characters'</span>: tensor([[ <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [<span class="number">10</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">10</span>],</span><br><span class="line">        [<span class="number">12</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>]])&#125;, <span class="string">'pos_tags'</span>: &#123;<span class="string">'tokens'</span>: tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="TextFieldEmbedders"><a href="#TextFieldEmbedders" class="headerlink" title="TextFieldEmbedders"></a>TextFieldEmbedders</h3><h4 id="单个Indexer"><a href="#单个Indexer" class="headerlink" title="单个Indexer"></a>单个Indexer</h4><p>allennlp数据处理的时候，会把instances中的TextFiled转换成TextFiledTensors数据结构，即：<code>Dict[str, Dict[str, torch.Tensor]]</code>，外围str对应每个TokenIndexers，里面对应TokenIndexer生成的idx。这个会被输入到TextFieldEmbedder中，用TokenEmbedder来embeds or encodes。</p><p>下面的例子是两个单独的embedding，分别是普通的embedding和cnn encoder。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer;</span></span><br><span class="line"><span class="comment"># Note that we added the batch dimension at the front.  You choose the 'indexer1'</span></span><br><span class="line"><span class="comment"># name when you configure your data processing code.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"indexer1"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor(</span><br><span class="line">            [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">9</span>, <span class="number">4</span>, <span class="number">3</span>]]</span><br><span class="line">         )</span><br><span class="line">     &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># You would typically get the number of embeddings here from the vocabulary;</span></span><br><span class="line"><span class="comment"># if you use `allennlp train`, there is a separate process for instantiating the</span></span><br><span class="line"><span class="comment"># Embedding object using the vocabulary that you don't need to worry about for</span></span><br><span class="line"><span class="comment"># now.</span></span><br><span class="line">embedding = Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This 'indexer1' key must match the 'indexer1' key in the `token_tensor` above.</span></span><br><span class="line"><span class="comment"># We use these names to align the TokenIndexers used in the data code with the</span></span><br><span class="line"><span class="comment"># TokenEmbedders that do the work on the model side.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"indexer1"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"Using the TextFieldEmbedder:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># As we've said a few times, what's going on inside is that we match keys between</span></span><br><span class="line"><span class="comment"># the token tensor and the token embedders, then pass the inner dictionary to the</span></span><br><span class="line"><span class="comment"># token embedder.  The above lines perform the following logic:</span></span><br><span class="line">embedded_tokens = embedding(**token_tensor[<span class="string">"indexer1"</span>])</span><br><span class="line">print(<span class="string">"Using the Embedding directly:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a TokenCharactersIndexer</span></span><br><span class="line"><span class="comment"># Note that we added the batch dimension at the front. Don't worry too much</span></span><br><span class="line"><span class="comment"># about the magic 'token_characters' key - that is hard-coded to be produced</span></span><br><span class="line"><span class="comment"># by the TokenCharactersIndexer, and accepted by TokenCharactersEncoder;</span></span><br><span class="line"><span class="comment"># you don't have to produce those yourself in normal settings, it's done for you.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"indexer2"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">1</span>, <span class="number">3</span>, <span class="number">0</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">9</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">0</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">character_embedding = Embedding(num_embeddings=<span class="number">10</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">3</span>, num_filters=<span class="number">4</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Again here, the 'indexer2' key is arbitrary. It just has to match whatever key</span></span><br><span class="line"><span class="comment"># you gave to the corresponding TokenIndexer in your data code, which ends up</span></span><br><span class="line"><span class="comment"># as the top-level key in the token_tensor dictionary.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"indexer2"</span>: token_encoder&#125;)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"With a character CNN:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Using the TextFieldEmbedder: tensor([[[ <span class="number">0.6184</span>,  <span class="number">0.3636</span>, <span class="number">-0.6774</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>],</span><br><span class="line">         [ <span class="number">0.2992</span>, <span class="number">-0.2631</span>, <span class="number">-0.4046</span>],</span><br><span class="line">         [ <span class="number">0.4240</span>,  <span class="number">0.2915</span>,  <span class="number">0.6677</span>],</span><br><span class="line">         [<span class="number">-0.6025</span>,  <span class="number">0.2038</span>, <span class="number">-0.0412</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br><span class="line">Using the Embedding directly: tensor([[[ <span class="number">0.6184</span>,  <span class="number">0.3636</span>, <span class="number">-0.6774</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>],</span><br><span class="line">         [ <span class="number">0.2992</span>, <span class="number">-0.2631</span>, <span class="number">-0.4046</span>],</span><br><span class="line">         [ <span class="number">0.4240</span>,  <span class="number">0.2915</span>,  <span class="number">0.6677</span>],</span><br><span class="line">         [<span class="number">-0.6025</span>,  <span class="number">0.2038</span>, <span class="number">-0.0412</span>],</span><br><span class="line">         [<span class="number">-0.0317</span>, <span class="number">-0.5588</span>,  <span class="number">0.6220</span>]]], grad_fn=&lt;EmbeddingBackward&gt;)</span><br><span class="line">With a character CNN: tensor([[[<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>],</span><br><span class="line">         [<span class="number">0.0000</span>, <span class="number">0.3550</span>, <span class="number">0.5636</span>, <span class="number">0.3409</span>],</span><br><span class="line">         [<span class="number">0.1199</span>, <span class="number">0.0000</span>, <span class="number">0.2336</span>, <span class="number">0.2741</span>],</span><br><span class="line">         [<span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>, <span class="number">0.0000</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><p>注意到上面的都是用<code>BasicTextFieldEmbedder(token_embedders={&quot;indexer_name&quot;: embedding or encoder})</code>。</p><h4 id="多个Indexer"><a href="#多个Indexer" class="headerlink" title="多个Indexer"></a>多个Indexer</h4><p>下面的例子是多个Indexer，其中一个是单个word，另一个是sequence of characters per token，最后一个是different single ID,对应不同的 speech tags。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer</span></span><br><span class="line"><span class="comment"># and a TokenCharactersIndexer; see the code snippet above. This time we're using</span></span><br><span class="line"><span class="comment"># more intuitive names for the indexers and embedders.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"token_characters"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding each token.</span></span><br><span class="line">embedding = Embedding(num_embeddings=<span class="number">6</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for encoding the characters in each token.</span></span><br><span class="line">character_embedding = Embedding(num_embeddings=<span class="number">6</span>, embedding_dim=<span class="number">3</span>)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">3</span>, num_filters=<span class="number">4</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用名字来表示不同的namespace</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(</span><br><span class="line">    token_embedders=&#123;<span class="string">"tokens"</span>: embedding, <span class="string">"token_characters"</span>: token_encoder&#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"没有 speech tag"</span>)</span><br><span class="line">print(embedded_tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is what gets created by TextField.as_tensor with a SingleIdTokenIndexer,</span></span><br><span class="line"><span class="comment"># a TokenCharactersIndexer, and another SingleIdTokenIndexer for PoS tags;</span></span><br><span class="line"><span class="comment"># see the code above.</span></span><br><span class="line">token_tensor = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"token_characters"</span>: &#123;</span><br><span class="line">        <span class="string">"token_characters"</span>: torch.LongTensor(</span><br><span class="line">            [[[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">2</span>, <span class="number">1</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">4</span>, <span class="number">0</span>]]]</span><br><span class="line">        )</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"pos_tag_tokens"</span>: &#123;</span><br><span class="line">        <span class="string">"tokens"</span>: torch.LongTensor([[<span class="number">2</span>, <span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice below how the 'vocab_namespace' parameter matches the name used above.</span></span><br><span class="line"><span class="comment"># We're showing here how the code works when we're constructing the Embedding from</span></span><br><span class="line"><span class="comment"># a configuration file, where the vocabulary object gets passed in behind the</span></span><br><span class="line"><span class="comment"># scenes (but the vocab_namespace parameter must be set in the config). If you are</span></span><br><span class="line"><span class="comment"># using a `build_model` method (see the quick start chapter) or instantiating the</span></span><br><span class="line"><span class="comment"># Embedding yourself directly, you can just grab the vocab size yourself and pass</span></span><br><span class="line"><span class="comment"># in num_embeddings, as we do above.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding each token.</span></span><br><span class="line"><span class="comment"># 这里用vocab_namespace的名字和对应的vocab匹配</span></span><br><span class="line">embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">3</span>, vocab_namespace=<span class="string">"token_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for encoding the characters in each token.</span></span><br><span class="line">character_embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">4</span>, vocab_namespace=<span class="string">"character_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line">cnn_encoder = CnnEncoder(embedding_dim=<span class="number">4</span>, num_filters=<span class="number">5</span>, ngram_filter_sizes=(<span class="number">3</span>,))</span><br><span class="line">token_encoder = TokenCharactersEncoder(character_embedding, cnn_encoder)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This is for embedding the part of speech tag of each token.</span></span><br><span class="line">pos_tag_embedding = Embedding(</span><br><span class="line">    embedding_dim=<span class="number">6</span>, vocab_namespace=<span class="string">"pos_tag_vocab"</span>, vocab=vocab</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Notice how these keys match the keys in the token_tensor dictionary above;</span></span><br><span class="line"><span class="comment"># these are the keys that you give to your TokenIndexers when constructing</span></span><br><span class="line"><span class="comment"># your TextFields in the DatasetReader.</span></span><br><span class="line">embedder = BasicTextFieldEmbedder(</span><br><span class="line">    token_embedders=&#123;</span><br><span class="line">        <span class="string">"tokens"</span>: embedding,</span><br><span class="line">        <span class="string">"token_characters"</span>: token_encoder,</span><br><span class="line">        <span class="string">"pos_tag_tokens"</span>: pos_tag_embedding,</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedded_tokens = embedder(token_tensor)</span><br><span class="line">print(<span class="string">"有 speech tag"</span>)</span><br><span class="line">print(embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">没有 speech tag</span><br><span class="line">tensor([[[ <span class="number">0.3090</span>,  <span class="number">0.0000</span>,  <span class="number">0.4446</span>,  <span class="number">0.0000</span>, <span class="number">-0.0025</span>,  <span class="number">0.4985</span>,  <span class="number">0.6270</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.3697</span>,  <span class="number">0.7020</span>, <span class="number">-0.6689</span>],</span><br><span class="line">         [ <span class="number">0.2016</span>,  <span class="number">0.0000</span>,  <span class="number">0.7770</span>,  <span class="number">0.0000</span>, <span class="number">-0.4487</span>, <span class="number">-0.6927</span>,  <span class="number">0.1282</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0455</span>,  <span class="number">0.0369</span>,  <span class="number">0.0783</span>]]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;)</span><br><span class="line">有 speech tag</span><br><span class="line">tensor([[[<span class="number">-0.5869</span>,  <span class="number">0.1731</span>,  <span class="number">0.4276</span>, <span class="number">-0.6160</span>,  <span class="number">0.5848</span>,  <span class="number">0.6462</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.2893</span>,  <span class="number">0.0415</span>,  <span class="number">0.0000</span>,  <span class="number">0.1478</span>,  <span class="number">0.1721</span>, <span class="number">-0.2290</span>],</span><br><span class="line">         [<span class="number">-0.2442</span>, <span class="number">-0.0461</span>, <span class="number">-0.3557</span>, <span class="number">-0.1157</span>, <span class="number">-0.0065</span>, <span class="number">-0.1078</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0331</span>, <span class="number">-0.4360</span>,  <span class="number">0.6749</span>],</span><br><span class="line">         [<span class="number">-0.0437</span>,  <span class="number">0.7053</span>, <span class="number">-0.5893</span>, <span class="number">-0.1253</span>, <span class="number">-0.4747</span>,  <span class="number">0.0396</span>,  <span class="number">0.0131</span>,</span><br><span class="line">           <span class="number">0.2123</span>,  <span class="number">0.2087</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.6189</span>,  <span class="number">0.3971</span>, <span class="number">-0.6693</span>],</span><br><span class="line">         [<span class="number">-0.2701</span>, <span class="number">-0.3194</span>,  <span class="number">0.0756</span>,  <span class="number">0.6921</span>,  <span class="number">0.4557</span>,  <span class="number">0.5086</span>,  <span class="number">0.0000</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.1958</span>,  <span class="number">0.4875</span>, <span class="number">-0.6018</span>]]],</span><br><span class="line">       grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><p>要注意，有两个地方的key要匹配：</p><ol><li>vocab 的 namespace，在TokenIndexers和TokenEmbedders中需要匹配，即<code>embedding = Embedding(embedding_dim=3, vocab_namespace=&quot;token_vocab&quot;, vocab=vocab)</code></li><li>TextField中用于TokenIndexer词典的key需要与BasicTextFieldEmbedder中用于TokenEmbedder词典的key匹配，就是<code>embedder = BasicTextFieldEmbedder(token_embedders={&quot;token_characters&quot;: token_encoder})</code>里面的token_characters。</li></ol><h3 id="Tokenizer-TokenIndexer-ToeknEmbedders的配合"><a href="#Tokenizer-TokenIndexer-ToeknEmbedders的配合" class="headerlink" title="Tokenizer, TokenIndexer, ToeknEmbedders的配合"></a>Tokenizer, TokenIndexer, ToeknEmbedders的配合</h3><p>你需要配置代码以选择要用作具体Tokenizer，TokenIndexers和TokenEmbedders。需要确保选择适合的组件，否则代码将无法正常工作。</p><p>比如，选择一个CharacterTokenizer和一个TokenCharactersIndexer并没有任何意义，因为Indexer假定您已将其标记为单词。</p><p>并且，TokenIndexer的输出会输入到TokenEmbedder中，通过使用key值。通常会有一对一的关系在 TokenIndexer 和 TokenEmbedder，并且一种 TokenIndexer 可能只对一个 Tokenizer 起作用</p><blockquote><p>Using a word-level tokenizer (such as SpacyTokenizer or WhitespaceTokenizer):</p><ul><li>SingleIdTokenIndexer → Embedding (for things like GloVe or other simple embeddings, including learned POS tag embeddings)</li><li>TokenCharactersIndexer → TokenCharactersEncoder (for things like a character CNN)</li><li>ElmoTokenIndexer → ElmoTokenEmbedder (for ELMo)</li><li>PretrainedTransformerMismatchedIndexer → PretrainedTransformerMismatchedEmbedder (for using a transformer like BERT when you really want to do modeling at the word level, e.g., for a tagging task; more on what this does below)</li></ul><p>Using a character-level tokenizer (such as CharacterTokenizer):</p><ul><li>SingleIdTokenIndexer → Embedding</li></ul><p>Using a wordpiece tokenizer (such as PretrainedTransformerTokenizer):</p><ul><li>PretrainedTransformerIndexer → PretrainedTransformerEmbedder</li><li>SingleIdTokenIndexer → Embedding (if you don’t want contextualized wordpieces for some reason)</li></ul></blockquote><h3 id="用预训练编码"><a href="#用预训练编码" class="headerlink" title="用预训练编码"></a>用预训练编码</h3><p>这里主要讲如何使用类似 ELMo, BERT 的预训练上下文编码，只要选择不同的Tokenizer、Indexer、Embedder.</p><h4 id="获取text-fields"><a href="#获取text-fields" class="headerlink" title="获取text_fields"></a>获取text_fields</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Splits text into words (instead of wordpieces or characters).  For ELMo, you can</span></span><br><span class="line"><span class="comment"># just use any word-level tokenizer that you like, though for best results you</span></span><br><span class="line"><span class="comment"># should use the same tokenizer that was used with ELMo, which is an older version</span></span><br><span class="line"><span class="comment"># of spacy.  We're using a whitespace tokenizer here for ease of demonstration</span></span><br><span class="line"><span class="comment"># with binder.</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with an array of characters in a way that ELMo expects.</span></span><br><span class="line">token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Both ELMo and BERT do their own thing with vocabularies, so we don't need to add</span></span><br><span class="line"><span class="comment"># anything, but we do need to construct the vocab object so we can use it below.</span></span><br><span class="line"><span class="comment"># (And if you have any labels in your data that need indexing, you'll still need</span></span><br><span class="line"><span class="comment"># this.)</span></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text ."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"ELMo tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"elmo_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We typically batch things together when making tensors, which requires some</span></span><br><span class="line"><span class="comment"># padding computation.  Don't worry too much about the padding for now.</span></span><br><span class="line">padding_lengths = text_field.get_padding_lengths()</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"ELMo tensors:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Any transformer model name that huggingface's transformers library supports will</span></span><br><span class="line"><span class="comment"># work here.  Under the hood, we're grabbing pieces from huggingface for this</span></span><br><span class="line"><span class="comment"># part.</span></span><br><span class="line">transformer_model = <span class="string">"bert-base-cased"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To do modeling with BERT correctly, we can't use just any tokenizer; we need to</span></span><br><span class="line"><span class="comment"># use BERT's tokenizer.</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each wordpiece with an id from BERT's vocabulary.</span></span><br><span class="line">token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"Some text with an extraordinarily long identifier."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"BERT tokens:"</span>, tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"BERT tensors:"</span>, tensor_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we'll do an example with paired text, to show the right way to handle [SEP]</span></span><br><span class="line"><span class="comment"># tokens in AllenNLP.  We have built-in ways of handling this for two text pieces.</span></span><br><span class="line"><span class="comment"># If you have more than two text pieces, you'll have to manually add the special</span></span><br><span class="line"><span class="comment"># tokens.  The way we're doing this requires that you use a</span></span><br><span class="line"><span class="comment"># PretrainedTransformerTokenizer, not the abstract Tokenizer class.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Splits text into wordpieces, but without adding special tokens.</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(</span><br><span class="line">    model_name=transformer_model,</span><br><span class="line">    add_special_tokens=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">context_text = <span class="string">"This context is frandibulous."</span></span><br><span class="line">question_text = <span class="string">"What is the context like?"</span></span><br><span class="line">context_tokens = tokenizer.tokenize(context_text)</span><br><span class="line">question_tokens = tokenizer.tokenize(question_text)</span><br><span class="line">print(<span class="string">"Context tokens:"</span>, context_tokens)</span><br><span class="line">print(<span class="string">"Question tokens:"</span>, question_tokens)</span><br><span class="line"></span><br><span class="line">combined_tokens = tokenizer.add_special_tokens(context_tokens, question_tokens)</span><br><span class="line">print(<span class="string">"Combined tokens:"</span>, combined_tokens)</span><br><span class="line"></span><br><span class="line">text_field = TextField(combined_tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"Combined BERT tensors:"</span>, tensor_dict)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">ELMo tokens:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">ELMo tensors:</span> <span class="string">&#123;'elmo_tokens':</span> <span class="string">&#123;'elmo_tokens':</span> <span class="string">tensor([[259,</span>  <span class="number">85</span><span class="string">,</span> <span class="number">105</span><span class="string">,</span> <span class="number">106</span><span class="string">,</span> <span class="number">116</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">106</span><span class="string">,</span> <span class="number">116</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">116</span><span class="string">,</span> <span class="number">112</span><span class="string">,</span> <span class="number">110</span><span class="string">,</span> <span class="number">102</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span> <span class="number">117</span><span class="string">,</span> <span class="number">102</span><span class="string">,</span> <span class="number">121</span><span class="string">,</span> <span class="number">117</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">],</span></span><br><span class="line">        <span class="string">[259,</span>  <span class="number">47</span><span class="string">,</span> <span class="number">260</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span></span><br><span class="line">         <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">,</span> <span class="number">261</span><span class="string">]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="number">570</span><span class="string">/570</span> <span class="string">[00:00&lt;00:00,</span> <span class="string">164kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="string">213k/213k</span> <span class="string">[00:01&lt;00:00,</span> <span class="string">203kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="string">436k/436k</span> <span class="string">[00:01&lt;00:00,</span> <span class="string">300kB/s]</span></span><br><span class="line"><span class="attr">Downloading:</span> <span class="number">100</span><span class="string">%|██████████|</span> <span class="number">29.0</span><span class="string">/29.0</span> <span class="string">[00:00&lt;00:00,</span> <span class="number">10.</span><span class="string">5kB/s]</span></span><br><span class="line"><span class="attr">BERT tokens:</span> <span class="string">[[CLS],</span> <span class="string">Some,</span> <span class="string">text,</span> <span class="string">with,</span> <span class="string">an,</span> <span class="string">extra,</span> <span class="comment">##ord, ##ina, ##rily, long, id, ##ent, ##ifier, ., [SEP]]</span></span><br><span class="line"><span class="attr">BERT tensors:</span> <span class="string">&#123;'bert_tokens':</span> <span class="string">&#123;'token_ids':</span> <span class="string">tensor([</span>  <span class="number">101</span><span class="string">,</span>  <span class="number">1789</span><span class="string">,</span>  <span class="number">3087</span><span class="string">,</span>  <span class="number">1114</span><span class="string">,</span>  <span class="number">1126</span><span class="string">,</span>  <span class="number">3908</span><span class="string">,</span>  <span class="number">6944</span><span class="string">,</span>  <span class="number">2983</span><span class="string">,</span> <span class="number">11486</span><span class="string">,</span>  <span class="number">1263</span><span class="string">,</span></span><br><span class="line">        <span class="number">25021</span><span class="string">,</span>  <span class="number">3452</span><span class="string">,</span> <span class="number">17792</span><span class="string">,</span>   <span class="number">119</span><span class="string">,</span>   <span class="number">102</span><span class="string">]),</span> <span class="attr">'mask':</span> <span class="string">tensor([True,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line">        <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">]),</span> <span class="attr">'type_ids':</span> <span class="string">tensor([0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Context tokens:</span> <span class="string">[This,</span> <span class="string">context,</span> <span class="string">is,</span> <span class="string">f,</span> <span class="comment">##rand, ##ib, ##ulous, .]</span></span><br><span class="line"><span class="attr">Question tokens:</span> <span class="string">[What,</span> <span class="string">is,</span> <span class="string">the,</span> <span class="string">context,</span> <span class="string">like,</span> <span class="string">?]</span></span><br><span class="line"><span class="attr">Combined tokens:</span> <span class="string">[[CLS],</span> <span class="string">This,</span> <span class="string">context,</span> <span class="string">is,</span> <span class="string">f,</span> <span class="comment">##rand, ##ib, ##ulous, ., [SEP], What, is, the, context, like, ?, [SEP]]</span></span><br><span class="line"><span class="attr">Combined BERT tensors:</span> <span class="string">&#123;'bert_tokens':</span> <span class="string">&#123;'token_ids':</span> <span class="string">tensor([</span>  <span class="number">101</span><span class="string">,</span>  <span class="number">1188</span><span class="string">,</span>  <span class="number">5618</span><span class="string">,</span>  <span class="number">1110</span><span class="string">,</span>   <span class="number">175</span><span class="string">,</span> <span class="number">13141</span><span class="string">,</span> <span class="number">13292</span><span class="string">,</span> <span class="number">14762</span><span class="string">,</span>   <span class="number">119</span><span class="string">,</span>   <span class="number">102</span><span class="string">,</span></span><br><span class="line">         <span class="number">1327</span><span class="string">,</span>  <span class="number">1110</span><span class="string">,</span>  <span class="number">1103</span><span class="string">,</span>  <span class="number">5618</span><span class="string">,</span>  <span class="number">1176</span><span class="string">,</span>   <span class="number">136</span><span class="string">,</span>   <span class="number">102</span><span class="string">]),</span> <span class="attr">'mask':</span> <span class="string">tensor([True,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span></span><br><span class="line">        <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">,</span> <span class="literal">True</span><span class="string">]),</span> <span class="attr">'type_ids':</span> <span class="string">tensor([0,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">])&#125;&#125;</span></span><br></pre></td></tr></table></figure><p>注意，<code>tokenizer.add_special_tokens()</code>这个方法，只能用于<code>PretrainedTransformerTokenizer</code>。并且，<code>TokenEmbedder</code>在这种情况下是作用于整个tokens序列的，而不是单个token，是在<code>TextFieldEmbedder</code>里面做self-attention算embedding，但是在代码上没啥影响。</p><h4 id="embedding"><a href="#embedding" class="headerlink" title="embedding"></a>embedding</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># It's easiest to get ELMo input by just running the data code.  See the</span></span><br><span class="line"><span class="comment"># exercise above for an explanation of this code.</span></span><br><span class="line">tokenizer: Tokenizer = WhitespaceTokenizer()</span><br><span class="line">token_indexer: TokenIndexer = ELMoTokenCharactersIndexer()</span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">text = <span class="string">"This is some text."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"ELMo tokens:"</span>, tokens)</span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"elmo_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"ELMo tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using a tiny, toy version of ELMo to demonstrate this.</span></span><br><span class="line">elmo_options_file = (</span><br><span class="line">    <span class="string">"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/options.json"</span></span><br><span class="line">)</span><br><span class="line">elmo_weight_file = (</span><br><span class="line">    <span class="string">"https://allennlp.s3.amazonaws.com/models/elmo/test_fixture/lm_weights.hdf5"</span></span><br><span class="line">)</span><br><span class="line">elmo_embedding = ElmoTokenEmbedder(</span><br><span class="line">    options_file=elmo_options_file, weight_file=elmo_weight_file</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"elmo_tokens"</span>: elmo_embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"ELMo embedded tokens:"</span>, embedded_tokens)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Again, it's easier to just run the data code to get the right output.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using the smallest transformer model we can here, so that it runs on</span></span><br><span class="line"><span class="comment"># binder.</span></span><br><span class="line">transformer_model = <span class="string">"google/reformer-crime-and-punishment"</span></span><br><span class="line">tokenizer = PretrainedTransformerTokenizer(model_name=transformer_model)</span><br><span class="line">token_indexer = PretrainedTransformerIndexer(model_name=transformer_model)</span><br><span class="line">text = <span class="string">"Some text with an extraordinarily long identifier."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">print(<span class="string">"Transformer tokens:"</span>, tokens)</span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"bert_tokens"</span>: token_indexer&#125;)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line">print(<span class="string">"Transformer tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line">embedding = PretrainedTransformerEmbedder(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"bert_tokens"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"Transformer embedded tokens:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ELMo tokens: [This, <span class="keyword">is</span>, some, text.]</span><br><span class="line">ELMo tensors: &#123;<span class="string">'elmo_tokens'</span>: &#123;<span class="string">'elmo_tokens'</span>: tensor([[<span class="number">259</span>,  <span class="number">85</span>, <span class="number">105</span>, <span class="number">106</span>, <span class="number">116</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">106</span>, <span class="number">116</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">116</span>, <span class="number">112</span>, <span class="number">110</span>, <span class="number">102</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>],</span><br><span class="line">        [<span class="number">259</span>, <span class="number">117</span>, <span class="number">102</span>, <span class="number">121</span>, <span class="number">117</span>,  <span class="number">47</span>, <span class="number">260</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>,</span><br><span class="line">         <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>, <span class="number">261</span>]])&#125;&#125;</span><br><span class="line">ELMo embedded tokens: tensor([[[ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.8512</span>, <span class="number">-0.0000</span>, <span class="number">-0.4313</span>,</span><br><span class="line">          <span class="number">-1.4576</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0271</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>, <span class="number">-0.1257</span>, <span class="number">-0.4585</span>,  <span class="number">0.0000</span>, <span class="number">-1.1158</span>, <span class="number">-0.2194</span>, <span class="number">-1.5000</span>,</span><br><span class="line">          <span class="number">-1.4474</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.2561</span>, <span class="number">-0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.0740</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>],</span><br><span class="line">         [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.8952</span>, <span class="number">-0.8725</span>,  <span class="number">0.3791</span>, <span class="number">-0.5978</span>,</span><br><span class="line">          <span class="number">-0.3816</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.1073</span>, <span class="number">-0.0000</span>,  <span class="number">0.2156</span>,</span><br><span class="line">           <span class="number">0.0582</span>,  <span class="number">0.0000</span>,  <span class="number">0.3820</span>,  <span class="number">0.5719</span>, <span class="number">-0.0000</span>, <span class="number">-0.6818</span>, <span class="number">-0.7399</span>,</span><br><span class="line">          <span class="number">-1.2560</span>, <span class="number">-1.4208</span>,  <span class="number">0.3838</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.7528</span>,  <span class="number">0.1510</span>,</span><br><span class="line">          <span class="number">-1.5196</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.6609</span>],</span><br><span class="line">         [ <span class="number">0.7935</span>,  <span class="number">1.2918</span>, <span class="number">-0.1949</span>, <span class="number">-0.7476</span>, <span class="number">-0.0000</span>, <span class="number">-0.2793</span>, <span class="number">-0.8381</span>,</span><br><span class="line">          <span class="number">-1.2474</span>,  <span class="number">0.1516</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.7108</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.4098</span>, <span class="number">-0.3095</span>,  <span class="number">0.0000</span>, <span class="number">-1.7565</span>,  <span class="number">0.4459</span>, <span class="number">-0.6707</span>,</span><br><span class="line">          <span class="number">-1.4732</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.6564</span>,  <span class="number">0.1276</span>,  <span class="number">0.0000</span>,</span><br><span class="line">          <span class="number">-0.0000</span>,  <span class="number">0.5801</span>,  <span class="number">0.0000</span>,  <span class="number">0.2791</span>],</span><br><span class="line">         [<span class="number">-0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-6.5606</span>,  <span class="number">2.1171</span>, <span class="number">-2.1024</span>,  <span class="number">2.5919</span>, <span class="number">-4.8008</span>,</span><br><span class="line">           <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">1.9269</span>,  <span class="number">0.0000</span>, <span class="number">-0.6810</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">3.7315</span>, <span class="number">-0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-3.3836</span>, <span class="number">-0.0000</span>, <span class="number">-0.0000</span>,</span><br><span class="line">           <span class="number">1.8097</span>, <span class="number">-7.0459</span>,  <span class="number">0.0000</span>,  <span class="number">2.7400</span>, <span class="number">-0.0000</span>, <span class="number">-1.6098</span>,  <span class="number">2.8753</span>,</span><br><span class="line">           <span class="number">0.0000</span>,  <span class="number">0.0000</span>, <span class="number">-0.0000</span>, <span class="number">-0.4789</span>]]], grad_fn=&lt;CatBackward&gt;)</span><br></pre></td></tr></table></figure><h3 id="word-level模型同时使用wordpiece的transformer"><a href="#word-level模型同时使用wordpiece的transformer" class="headerlink" title="word-level模型同时使用wordpiece的transformer"></a>word-level模型同时使用wordpiece的transformer</h3><p>在part-of-speech tagging或者named entity recognition中，数据集是在word level的，因此模型的loss、output都应该是word level，但是可能会需要用transformer，这是在wordpiece level的。</p><p>有两种主要的方式：</p><ol><li>在transformer运行之后，在wordpiece-level上做一些pooling</li><li>把label扩散到wordpiece-level上</li></ol><h4 id="pooling-over-Wordpieces"><a href="#pooling-over-Wordpieces" class="headerlink" title="pooling over Wordpieces"></a>pooling over Wordpieces</h4><blockquote><p>The first step is tokenization, and here we tokenize at the word level (typically the tokenization will be already given to you, so you don’t need to run a tokenizer at all).</p><p>In the second step (indexing), we need to further tokenize each word into subword units, getting a list of wordpieces that will be indexed and passed to the transformer in the third step (embedding).</p><p>The embedding step has to run the transformer, then perform pooling to undo the subword tokenization that was done in the indexing subwordtep, so that we end up with one vector per original token.</p></blockquote><p>只需要使用<code>PretrainedTransformerMismatchedIndexer</code>和<code>PretrainedTransformerMismatchedEmbedder</code>就可以使用任何 Hugging Face 的 transformers。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This pattern is typically used in cases where your input data is already</span></span><br><span class="line"><span class="comment"># tokenized, so we're showing that here.</span></span><br><span class="line">text_tokens = [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"frandibulous"</span>, <span class="string">"text"</span>, <span class="string">"."</span>]</span><br><span class="line">tokens = [Token(x) <span class="keyword">for</span> x <span class="keyword">in</span> text_tokens]</span><br><span class="line">print(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using a very small transformer here so that it runs quickly in binder. You</span></span><br><span class="line"><span class="comment"># can change this to any transformer model name supported by Hugging Face.</span></span><br><span class="line">transformer_model = <span class="string">"google/reformer-crime-and-punishment"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents the list of word tokens with a sequences of wordpieces as determined</span></span><br><span class="line"><span class="comment"># by the transformer's tokenizer.  This actually results in a pretty complex data</span></span><br><span class="line"><span class="comment"># type, which you can see by running this.  It's complicated because we need to</span></span><br><span class="line"><span class="comment"># know how to combine the wordpieces back into words after running the</span></span><br><span class="line"><span class="comment"># transformer.</span></span><br><span class="line">indexer = PretrainedTransformerMismatchedIndexer(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, &#123;<span class="string">"transformer"</span>: indexer&#125;)</span><br><span class="line">text_field.index(Vocabulary())</span><br><span class="line">token_tensor = text_field.as_tensor(text_field.get_padding_lengths())</span><br><span class="line"></span><br><span class="line"><span class="comment"># There are two key things to notice in this output.  First, there are two masks:</span></span><br><span class="line"><span class="comment"># `mask` is a word-level mask that gets used in the utility functions described in</span></span><br><span class="line"><span class="comment"># the last section of this chapter.  `wordpiece_mask` gets used by the `Embedder`</span></span><br><span class="line"><span class="comment"># itself.  Second, there is an `offsets` tensor that gives start and end wordpiece</span></span><br><span class="line"><span class="comment"># indices for the original tokens.  In the embedder, we grab these, average all of</span></span><br><span class="line"><span class="comment"># the wordpieces for each token, and return the result.</span></span><br><span class="line">print(<span class="string">"Indexed tensors:"</span>, token_tensor)</span><br><span class="line"></span><br><span class="line">embedding = PretrainedTransformerMismatchedEmbedder(model_name=transformer_model)</span><br><span class="line"></span><br><span class="line">embedder = BasicTextFieldEmbedder(token_embedders=&#123;<span class="string">"transformer"</span>: embedding&#125;)</span><br><span class="line"></span><br><span class="line">tensor_dict = text_field.batch_tensors([token_tensor])</span><br><span class="line">embedded_tokens = embedder(tensor_dict)</span><br><span class="line">print(<span class="string">"Embedded tokens size:"</span>, embedded_tokens.size())</span><br><span class="line">print(<span class="string">"Embedded tokens:"</span>, embedded_tokens)</span><br></pre></td></tr></table></figure><h4 id="扩散标签到wordpiece"><a href="#扩散标签到wordpiece" class="headerlink" title="扩散标签到wordpiece"></a>扩散标签到wordpiece</h4><p>没有直接的函数可以实现这个功能，但是直接自己写个方法，把word的label转换为wordpiece的labels也比较简单。做完之后，就可以用<code>PretrainedTransformerMismatchedIndexer</code>和<code>PretrainedTransformerMismatchedEmbedder</code>来处理word-piece level的数据。</p><p>另一种选择是为非初始单词提供空标签，并为任何带有空标签的单词掩盖损失计算。但是，从建模角度来看，这是有问题的，因为它打破了CRF局部性假设（您不会从CRF转换概率中获得任何用处），并且通常会使建模更加困难。我们不推荐这种方法。</p><h3 id="padding和mask"><a href="#padding和mask" class="headerlink" title="padding和mask"></a>padding和mask</h3><p>由于要batch computation，所以不一样长的序列需要padding，allennlp里面用的是<code>text_field.get_padding_lengths()</code>。<code>collate_function</code>方法会在batch中找到最长的dimension，然后将这个最大值传给<code>text_field.as_tensor()</code>，因此每个相同维度的tensor在被创建之前就会被batched。</p><p>mask的过程在<code>TextFieldEmbedder</code>中，但是也要确保模型代码做了对应的masking computation。</p><p><code>allennlp.nn.util</code>中提供了很多masked版本的pytorch的工具类方法，比如<code>masked_softmax</code>和<code>masked_log_softmax</code>和<code>masked_topk</code></p><h3 id="使用TextField输出的TextFieldTensors"><a href="#使用TextField输出的TextFieldTensors" class="headerlink" title="使用TextField输出的TextFieldTensors"></a>使用TextField输出的TextFieldTensors</h3><p>TextField会返回一个TextFieldTensors对象，是一个复杂的字典结构。</p><p>不要直接编写访问TextfieldTensors对象内部的代码。allennlp中有几个方法可以访问textfieldtensors。TextFieldEmbedder对象会把TextFieldTensors对象转换为每个输入token对应一个embedding。一般会通过mask，并获得token id来把他们转换为字符串。allennlp.nn.util中提供了get_text_field_mask和get_token_ids_from_text_field_tesors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># We're following the logic from the "Combining multiple TokenIndexers" example</span></span><br><span class="line"><span class="comment"># above.</span></span><br><span class="line">tokenizer = SpacyTokenizer(pos_tags=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">vocab = Vocabulary()</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"This"</span>, <span class="string">"is"</span>, <span class="string">"some"</span>, <span class="string">"text"</span>, <span class="string">"."</span>], namespace=<span class="string">"token_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace(</span><br><span class="line">    [<span class="string">"T"</span>, <span class="string">"h"</span>, <span class="string">"i"</span>, <span class="string">"s"</span>, <span class="string">" "</span>, <span class="string">"o"</span>, <span class="string">"m"</span>, <span class="string">"e"</span>, <span class="string">"t"</span>, <span class="string">"x"</span>, <span class="string">"."</span>], namespace=<span class="string">"character_vocab"</span></span><br><span class="line">)</span><br><span class="line">vocab.add_tokens_to_namespace([<span class="string">"DT"</span>, <span class="string">"VBZ"</span>, <span class="string">"NN"</span>, <span class="string">"."</span>], namespace=<span class="string">"pos_tag_vocab"</span>)</span><br><span class="line"></span><br><span class="line">text = <span class="string">"This is some text."</span></span><br><span class="line">text2 = <span class="string">"This is some text with more tokens."</span></span><br><span class="line">tokens = tokenizer.tokenize(text)</span><br><span class="line">tokens2 = tokenizer.tokenize(text2)</span><br><span class="line">print(<span class="string">"Tokens:"</span>, tokens)</span><br><span class="line">print(<span class="string">"Tokens 2:"</span>, tokens2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Represents each token with (1) an id from a vocabulary, (2) a sequence of</span></span><br><span class="line"><span class="comment"># characters, and (3) part of speech tag ids.</span></span><br><span class="line">token_indexers = &#123;</span><br><span class="line">    <span class="string">"tokens"</span>: SingleIdTokenIndexer(namespace=<span class="string">"token_vocab"</span>),</span><br><span class="line">    <span class="string">"token_characters"</span>: TokenCharactersIndexer(namespace=<span class="string">"character_vocab"</span>),</span><br><span class="line">    <span class="string">"pos_tags"</span>: SingleIdTokenIndexer(namespace=<span class="string">"pos_tag_vocab"</span>, feature_name=<span class="string">"tag_"</span>),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">text_field = TextField(tokens, token_indexers)</span><br><span class="line">text_field.index(vocab)</span><br><span class="line">text_field2 = TextField(tokens2, token_indexers)</span><br><span class="line">text_field2.index(vocab)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We're using the longer padding lengths here; we'd typically be relying on our</span></span><br><span class="line"><span class="comment"># collate function to figure out what the longest values are to use.</span></span><br><span class="line">padding_lengths = text_field2.get_padding_lengths()</span><br><span class="line">tensor_dict = text_field.as_tensor(padding_lengths)</span><br><span class="line">tensor_dict2 = text_field2.as_tensor(padding_lengths)</span><br><span class="line">print(<span class="string">"Combined tensor dictionary:"</span>, tensor_dict)</span><br><span class="line">print(<span class="string">"Combined tensor dictionary 2:"</span>, tensor_dict2)</span><br><span class="line"></span><br><span class="line">text_field_tensors = text_field.batch_tensors([tensor_dict, tensor_dict2])</span><br><span class="line">print(<span class="string">"Batched tensor dictionary:"</span>, text_field_tensors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We've seen plenty of examples of using a TextFieldEmbedder, so we'll just show</span></span><br><span class="line"><span class="comment"># the utility methods here.</span></span><br><span class="line">mask = nn_util.get_text_field_mask(text_field_tensors)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask)</span><br><span class="line">print(<span class="string">"Mask size:"</span>, mask.size())</span><br><span class="line">token_ids = nn_util.get_token_ids_from_text_field_tensors(text_field_tensors)</span><br><span class="line">print(<span class="string">"Token ids:"</span>, token_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can also handle getting masks when you have lists of TextFields, but there's</span></span><br><span class="line"><span class="comment"># an important parameter that you need to pass, which we'll show here.  The</span></span><br><span class="line"><span class="comment"># difference in output that you see between here and above is just that there's an</span></span><br><span class="line"><span class="comment"># extra dimension in this output.  Where shapes used to be (batch_size=2, ...),</span></span><br><span class="line"><span class="comment"># now they are (batch_size=1, list_length=2, ...).</span></span><br><span class="line">list_field = ListField([text_field, text_field2])</span><br><span class="line">tensor_dict = list_field.as_tensor(list_field.get_padding_lengths())</span><br><span class="line">text_field_tensors = list_field.batch_tensors([tensor_dict])</span><br><span class="line">print(<span class="string">"Batched tensors for ListField[TextField]:"</span>, text_field_tensors)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The num_wrapping_dims argument tells get_text_field_mask how many nested lists</span></span><br><span class="line"><span class="comment"># there are around the TextField, which we need for our heuristics that guess</span></span><br><span class="line"><span class="comment"># which tensor to use when computing a mask.</span></span><br><span class="line">mask = nn_util.get_text_field_mask(text_field_tensors, num_wrapping_dims=<span class="number">1</span>)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask)</span><br><span class="line">print(<span class="string">"Mask:"</span>, mask.size())</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Tokens:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">Tokens 2:</span> <span class="string">[This,</span> <span class="string">is,</span> <span class="string">some,</span> <span class="string">text,</span> <span class="string">with,</span> <span class="string">more,</span> <span class="string">tokens,</span> <span class="string">.]</span></span><br><span class="line"><span class="attr">Combined tensor dictionary:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Combined tensor dictionary 2:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">        <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">])&#125;&#125;</span></span><br><span class="line"><span class="attr">Batched tensor dictionary:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]],</span></span><br><span class="line"></span><br><span class="line">        <span class="string">[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">         <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">tensor([[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">],</span></span><br><span class="line">        <span class="string">[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">]])</span></span><br><span class="line"><span class="attr">Mask size:</span> <span class="string">torch.Size([2,</span> <span class="number">8</span><span class="string">])</span></span><br><span class="line"><span class="attr">Token ids:</span> <span class="string">tensor([[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">        <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]])</span></span><br><span class="line"><span class="string">Batched</span> <span class="string">tensors</span> <span class="string">for</span> <span class="string">ListField[TextField]:</span> <span class="string">&#123;'tokens':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">6</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">6</span><span class="string">]]])&#125;,</span> <span class="attr">'token_characters':</span> <span class="string">&#123;'token_characters':</span> <span class="string">tensor([[[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]],</span></span><br><span class="line"></span><br><span class="line">         <span class="string">[[</span> <span class="number">2</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">4</span><span class="string">,</span>  <span class="number">5</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">5</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">8</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">9</span><span class="string">,</span> <span class="number">11</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">1</span><span class="string">,</span>  <span class="number">4</span><span class="string">,</span> <span class="number">10</span><span class="string">,</span>  <span class="number">3</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[</span> <span class="number">8</span><span class="string">,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">],</span></span><br><span class="line">          <span class="string">[10,</span>  <span class="number">7</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">9</span><span class="string">,</span>  <span class="number">1</span><span class="string">,</span>  <span class="number">5</span><span class="string">],</span></span><br><span class="line">          <span class="string">[12,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">,</span>  <span class="number">0</span><span class="string">]]]])&#125;,</span> <span class="attr">'pos_tags':</span> <span class="string">&#123;'tokens':</span> <span class="string">tensor([[[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">5</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">,</span> <span class="number">0</span><span class="string">],</span></span><br><span class="line">         <span class="string">[2,</span> <span class="number">3</span><span class="string">,</span> <span class="number">2</span><span class="string">,</span> <span class="number">4</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">1</span><span class="string">,</span> <span class="number">5</span><span class="string">]]])&#125;&#125;</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">tensor([[[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">,</span> <span class="literal">False</span><span class="string">],</span></span><br><span class="line">         <span class="string">[</span> <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">,</span>  <span class="literal">True</span><span class="string">]]])</span></span><br><span class="line"><span class="attr">Mask:</span> <span class="string">torch.Size([1,</span> <span class="number">2</span><span class="string">,</span> <span class="number">8</span><span class="string">])</span></span><br></pre></td></tr></table></figure><h2 id="未完"><a href="#未完" class="headerlink" title="未完"></a>未完</h2><p>剩下的懒得记了，赶时间，直接看很快地过了一遍guide剩下内容。笔记自己参考下官方文档吧，也不多了。</p>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;进一步理解和使用AllenNLP这个框架，续前面的&lt;a href=&quot;https://hanielxx.com/MachineLearning/2021-04-14-allennlp-notes&quot;&gt;AllenNLP框架学习笔记（一）&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="AllenNLP" scheme="https://hanielxx.com/tags/AllenNLP/"/>
    
    <category term="NLP" scheme="https://hanielxx.com/tags/NLP/"/>
    
    <category term="OpenLibrary" scheme="https://hanielxx.com/tags/OpenLibrary/"/>
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="Pytorch" scheme="https://hanielxx.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>AllenNLP框架学习笔记（一）</title>
    <link href="https://hanielxx.com/MachineLearning/2021-04-14-allennlp-notes"/>
    <id>https://hanielxx.com/MachineLearning/2021-04-14-allennlp-notes</id>
    <published>2021-04-14T02:11:01.000Z</published>
    <updated>2021-05-25T15:36:53.751Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>刚开始学习使用AllenNLP框架，记录一些笔记供参考。</p><p>感觉这个框架用起来比较优雅，从基类的抽象，到模块间的配合。但缺点是官方的guide又臭又长，讲的也不是很清楚感觉，很多东西还是得边看源码边写。这个笔记是看官方guide记录，只能做到脑子里大概有个框架，要熟悉AllenNLP还是得看源码。</p></div><a id="more"></a><h2 id="Github-README"><a href="#Github-README" class="headerlink" title="Github-README"></a>Github-README</h2><ul><li><p>官方仓库：<a href="https://github.com/allenai/allennlp" target="_blank" rel="external nofollow noopener noreferrer">https://github.com/allenai/allennlp</a></p></li><li><p>当前版本：<code>v2.2.0</code></p></li><li><p>Guide: <a href="https://guide.allennlp.org/" target="_blank" rel="external nofollow noopener noreferrer">AllenNLP Guide</a></p></li><li><p>需要train-config files的配置，参考 此<a href="https://github.com/allenai/allennlp-template-config-files" target="_blank" rel="external nofollow noopener noreferrer">模板</a></p></li><li><p>直接code配置并且自己train，参考 此<a href="https://github.com/allenai/allennlp-template-python-script" target="_blank" rel="external nofollow noopener noreferrer">模板</a></p></li><li><p>allennlp的插件是动态加载的，会自动识别官方维护的包，其他的需要写一个本地插件配置文件<code>.allennlp_plugins</code>，存放在运行allennlp命令的目录下，或者全局配置<code>~/.allennlp/plugins</code>。一行一个。</p></li><li><p>用<code>allennlp test-install</code>测试是否存在插件</p></li><li><p>Install Requirement：</p><ul><li>python 3.6.1+</li><li>Pytorch</li></ul></li><li><p>推荐用pip安装：<code>pip install allennlp</code></p></li><li><p>如果python版本高于3.7，要注意不能安装pypi的dataclasses，使用<code>pip freeze | grep dataclasses</code>，如果有的话直接<code>pip uninstall -y dataclasses</code></p></li><li><p><code>allennlp --help</code>查看命令</p></li></ul><h2 id="大体框架"><a href="#大体框架" class="headerlink" title="大体框架"></a>大体框架</h2><h3 id="定义输入输出"><a href="#定义输入输出" class="headerlink" title="定义输入输出"></a>定义输入输出</h3><ol><li>用<code>Instance</code>表示一个example对象</li><li>每个<code>Instance</code>对象包含几个<code>Fields</code>，这些<code>Fields</code>后面会被转换为tensor</li><li>对于文本分类，输入为<code>text: TextField</code>，输出为<code>label: LabelField</code>，都是类对象</li></ol><h3 id="读数据"><a href="#读数据" class="headerlink" title="读数据"></a>读数据</h3><ol><li><p>使用<code>DatasetReader</code>类</p></li><li><p>会将原始数据转换为<code>Instances</code></p></li><li><p>上面的文本分类，对应的dataset文件，每行一句话，后面是label，中间用<code>[TAB]</code>分隔</p></li><li><p>数据样例：<code>I like this movie a lot! [TAB] positive</code></p></li><li><p>可以通过继承<code>DatasetReader</code>类实现自己的reader，但是<strong>必须重写<code>_read()</code>方法</strong></p></li><li><p>对于prediction，label不用写，代码也是一样的</p></li><li><p>代码样例如下</p><ol><li>在调用<code>reader.read(file)</code>时，</li><li>会读入input file，split每一行text，</li><li>用tokenizer对text分词，得到vacab中对应的id。</li><li>要注意的是，<code>fields</code>中的keys，后面会作为参数名传入<code>Model</code>中。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register('classification-tsv')</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.tokenizer = SpacyTokenizer()</span><br><span class="line">    self.token_indexers = &#123;<span class="string">'tokens'</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">    <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> lines:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            text, label = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">            text_field = TextField(self.tokenizer.tokenize(text),</span><br><span class="line">                                    self.token_indexers)</span><br><span class="line">            label_field = LabelField(label)</span><br><span class="line">            fields = &#123;<span class="string">'text'</span>: text_field, <span class="string">'label'</span>: label_field&#125;</span><br><span class="line">            <span class="keyword">yield</span> Instance(fields)</span><br></pre></td></tr></table></figure></li></ol><h3 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h3><ol><li><p>ALLenNLP的模型是Pytorch中的<code>Module</code>，实现<code>forward()</code>方法，<strong>要求输出是dictionary</strong>，输出中会包含一个<code>loss</code> key，用于后面的optimization</p></li><li><p>batch instances-&gt;<code>Model.forward()</code>-&gt;get <code>loss</code>-&gt;backprop phase-&gt;update parameters，这个training loop在框架中已经实现了，只有在需要自定义的时候再改</p></li><li><p>建议在model的<code>__init__</code>方法中，将所有的参数都作为构造器参数，可以方便更改模型而不需要改代码。</p></li><li><p>建议在参数后面写注释说明，方便理解，而且会有一些神奇的用法，比如在构造模型的时候，会读取配置文件，自动创建对应的对象。通过装饰器实现的。就像<code>ClassyVision</code>中一样。</p></li><li><p>模型样例如下，</p><ol><li>通过vocab传vocabulary，</li><li><code>TextFieldEmbedder</code>会将<code>TextField</code>创建的tensors编码，输出<code>(batch_size, num_tokens, embedding_dim)</code>的embedding，</li><li>通过<code>Seq2VecEncoder</code>将一个序列的tensor转换为一个tensor，即<code>(batch_size, encoding_dim)</code>。</li><li>最后的分类器是一个fc层，输入的shape可以通过encoder的<code>get_output_dim()</code>获得，输出的shape就是label的个数，通过<code>vocab.get_vocab_size(&quot;labels&quot;)</code>得到。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Model.register('simple_classifier')</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                vocab: Vocabulary,</span></span></span><br><span class="line"><span class="function"><span class="params">                embedder: TextFieldEmbedder,</span></span></span><br><span class="line"><span class="function"><span class="params">                encoder: Seq2VecEncoder)</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        num_labels = vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</span><br></pre></td></tr></table></figure></li></ol><h3 id="前向传播过程"><a href="#前向传播过程" class="headerlink" title="前向传播过程"></a>前向传播过程</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                text: TextFieldTensors,</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        <span class="comment"># Shape: (1,)</span></span><br><span class="line">        loss = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">'loss'</span>: loss, <span class="string">'probs'</span>: probs&#125;</span><br></pre></td></tr></table></figure><ol><li>DatasetReader中使用的字段名称，这里是<code>text</code>和<code>label</code>，DatasetReader会生成batch具有相同字段名称的Instances，所以这里要使<strong>用同样的方式命名参数</strong></li><li>要注意输入输出都是<code>batch torch.Tensor</code></li><li>流程比较简单，embed-&gt;mask padding-&gt;encoder-&gt;classifier-&gt;softmax-&gt;loss</li><li>最后的fc层输出一个分数，一般叫<code>logit</code>，需要用softmax将它转换为在label上的概率分布</li></ol><h2 id="训练和预测"><a href="#训练和预测" class="headerlink" title="训练和预测"></a>训练和预测</h2><p>这里接上面的分类模型，讲的是怎么训练模型和做prediction。</p><p>有两种方式实现：</p><ol><li>自己写代码来构建dataset reader和model，然后进行training loop</li><li>通过配置文件，使用<code>allennlp train</code>命令训练</li></ol><h3 id="用自己的代码训练"><a href="#用自己的代码训练" class="headerlink" title="用自己的代码训练"></a>用自己的代码训练</h3><p>大多数情况下可以直接使用<code>allennlp train</code>来训练，也可以自定义代码。</p><p>下面使用的dataset是：<a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/" target="_blank" rel="external nofollow noopener noreferrer">Movie Review Data</a>，是用户在IMDb上的评论，label是positive和negative二分类。</p><h4 id="测试dataset-reader"><a href="#测试dataset-reader" class="headerlink" title="测试dataset reader"></a>测试dataset reader</h4><p>在使用自己的dataset reader之前，建议写一写测试脚本来验证一下代码是否正确。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict, Iterable, List</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> DatasetReader, Instance</span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> Field, LabelField, TextField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token, Tokenizer, WhitespaceTokenizer</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self,</span></span></span><br><span class="line"><span class="function"><span class="params">        tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        token_indexers: Dict[str, TokenIndexer] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        max_tokens: int = None,</span></span></span><br><span class="line"><span class="function"><span class="params">        **kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super().__init__(**kwargs)</span><br><span class="line">        self.tokenizer = tokenizer <span class="keyword">or</span> WhitespaceTokenizer()</span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> &#123;<span class="string">"tokens"</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line">        self.max_tokens = max_tokens</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, sentiment = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">                tokens = self.tokenizer.tokenize(text)</span><br><span class="line">                <span class="keyword">if</span> self.max_tokens:</span><br><span class="line">                    tokens = tokens[: self.max_tokens]</span><br><span class="line">                text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">                label_field = LabelField(sentiment)</span><br><span class="line">                fields: Dict[str, Field] = &#123;<span class="string">"text"</span>: text_field, <span class="string">"label"</span>: label_field&#125;</span><br><span class="line">                <span class="keyword">yield</span> Instance(fields)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset_reader = ClassificationTsvReader(max_tokens=<span class="number">64</span>)</span><br><span class="line">instances = list(dataset_reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> instance <span class="keyword">in</span> instances[:<span class="number">10</span>]:</span><br><span class="line">    print(instance)</span><br></pre></td></tr></table></figure><h4 id="给模型输入Instances"><a href="#给模型输入Instances" class="headerlink" title="给模型输入Instances"></a>给模型输入Instances</h4><p>训练过程会被分为几个简单的function来分别实例化一写依赖。模型需要从数据中建立<code>Vocabulary</code>，创建的过程作为一个单独的function。为了不把模型创建的过程放在训练流程中，模型的创建也会作为一个单独的function。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> allennlp.modules <span class="keyword">import</span> TextFieldEmbedder, Seq2VecEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.text_field_embedders <span class="keyword">import</span> BasicTextFieldEmbedder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.token_embedders <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.seq2vec_encoders <span class="keyword">import</span> BagOfEmbeddingsEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.nn <span class="keyword">import</span> util</span><br><span class="line"><span class="keyword">from</span> allennlp.training.metrics <span class="keyword">import</span> CategoricalAccuracy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line">        super().__init__(vocab)</span><br><span class="line">        self.embedder = embedder</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        num_labels = vocab.get_vocab_size(<span class="string">"labels"</span>)</span><br><span class="line">        self.classifier = torch.nn.Linear(encoder.get_output_dim(), num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, text: TextFieldTensors, label: torch.Tensor</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits, dim=<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># Shape: (1,)</span></span><br><span class="line">        loss = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">"loss"</span>: loss, <span class="string">"probs"</span>: probs&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_training_loop</span><span class="params">()</span>:</span></span><br><span class="line">    dataset_reader = ClassificationTsvReader(max_tokens=<span class="number">64</span>)</span><br><span class="line">    print(<span class="string">"Reading data"</span>)</span><br><span class="line">    instances = list(dataset_reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>))</span><br><span class="line"></span><br><span class="line">    vocab = build_vocab(instances)</span><br><span class="line">    model = build_model(vocab)</span><br><span class="line"></span><br><span class="line">    outputs = model.forward_on_instances(instances[:<span class="number">4</span>])</span><br><span class="line">    print(outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(instances: Iterable[Instance])</span> -&gt; Vocabulary:</span></span><br><span class="line">    print(<span class="string">"Building the vocabulary"</span>)</span><br><span class="line">    <span class="keyword">return</span> Vocabulary.from_instances(instances)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(vocab: Vocabulary)</span> -&gt; Model:</span></span><br><span class="line">    print(<span class="string">"Building the model"</span>)</span><br><span class="line">    vocab_size = vocab.get_vocab_size(<span class="string">"tokens"</span>)</span><br><span class="line">    embedder = BasicTextFieldEmbedder(</span><br><span class="line">        &#123;<span class="string">"tokens"</span>: Embedding(embedding_dim=<span class="number">10</span>, num_embeddings=vocab_size)&#125;</span><br><span class="line">    )</span><br><span class="line">    encoder = BagOfEmbeddingsEncoder(embedding_dim=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> SimpleClassifier(vocab, embedder, encoder)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">run_training_loop()</span><br></pre></td></tr></table></figure><h4 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h4><p>AllenNLP使用<code>Trainer</code>来训练模型。Trainer会负责关联你的model、optimizer、instances、dataloder，执行training loop等。所有的function几乎都是<code>build_</code>格式。</p><h5 id="依赖setup"><a href="#依赖setup" class="headerlink" title="依赖setup"></a>依赖setup</h5><ul><li>中间实现和上面的一样，所以省略了一些代码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tempfile</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Dict, Iterable, List, Tuple</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> allennlp</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> allennlp.data <span class="keyword">import</span> (</span><br><span class="line">    DataLoader,</span><br><span class="line">    DatasetReader,</span><br><span class="line">    Instance,</span><br><span class="line">    Vocabulary,</span><br><span class="line">    TextFieldTensors,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> allennlp.data.data_loaders <span class="keyword">import</span> SimpleDataLoader</span><br><span class="line"><span class="keyword">from</span> allennlp.data.fields <span class="keyword">import</span> LabelField, TextField</span><br><span class="line"><span class="keyword">from</span> allennlp.data.token_indexers <span class="keyword">import</span> TokenIndexer, SingleIdTokenIndexer</span><br><span class="line"><span class="keyword">from</span> allennlp.data.tokenizers <span class="keyword">import</span> Token, Tokenizer, WhitespaceTokenizer</span><br><span class="line"><span class="keyword">from</span> allennlp.models <span class="keyword">import</span> Model</span><br><span class="line"><span class="keyword">from</span> allennlp.modules <span class="keyword">import</span> TextFieldEmbedder, Seq2VecEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.seq2vec_encoders <span class="keyword">import</span> BagOfEmbeddingsEncoder</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.token_embedders <span class="keyword">import</span> Embedding</span><br><span class="line"><span class="keyword">from</span> allennlp.modules.text_field_embedders <span class="keyword">import</span> BasicTextFieldEmbedder</span><br><span class="line"><span class="keyword">from</span> allennlp.nn <span class="keyword">import</span> util</span><br><span class="line"><span class="keyword">from</span> allennlp.training.trainer <span class="keyword">import</span> GradientDescentTrainer, Trainer</span><br><span class="line"><span class="keyword">from</span> allennlp.training.optimizers <span class="keyword">import</span> AdamOptimizer</span><br><span class="line"><span class="keyword">from</span> allennlp.training.metrics <span class="keyword">import</span> CategoricalAccuracy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    self,</span></span></span><br><span class="line"><span class="function"><span class="params">    tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">    token_indexers: Dict[str, TokenIndexer] = None,</span></span></span><br><span class="line"><span class="function"><span class="params">    max_tokens: int = None,</span></span></span><br><span class="line"><span class="function"><span class="params">    **kwargs</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, vocab: Vocabulary, embedder: TextFieldEmbedder, encoder: Seq2VecEncoder</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">        self, text: TextFieldTensors, label: torch.Tensor</span></span></span><br><span class="line"><span class="function"><span class="params">    )</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dataset_reader</span><span class="params">()</span> -&gt; DatasetReader:</span></span><br><span class="line">    <span class="comment"># ....</span></span><br><span class="line">    <span class="keyword">return</span> ClassificationTsvReader()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(reader: DatasetReader)</span> -&gt; Tuple[List[Instance], List[Instance]]:</span></span><br><span class="line">    print(<span class="string">"Reading data"</span>)</span><br><span class="line">    training_data = list(reader.read(<span class="string">"quick_start/data/movie_review/train.tsv"</span>))</span><br><span class="line">    validation_data = list(reader.read(<span class="string">"quick_start/data/movie_review/dev.tsv"</span>))</span><br><span class="line">    <span class="keyword">return</span> training_data, validation_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(instances: Iterable[Instance])</span> -&gt; Vocabulary:</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">return</span> Vocabulary.from_instances(instances)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(vocab: Vocabulary)</span> -&gt; Model:</span></span><br><span class="line">    <span class="comment"># ...</span></span><br><span class="line">    <span class="keyword">return</span> SimpleClassifier(vocab, embedder, encoder)</span><br></pre></td></tr></table></figure><h5 id="run-training-loop和Trainer构建"><a href="#run-training-loop和Trainer构建" class="headerlink" title="run_training_loop和Trainer构建"></a>run_training_loop和Trainer构建</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> allennlp.training.trainer <span class="keyword">import</span> GradientDescentTrainer, Trainer</span><br><span class="line"><span class="keyword">from</span> allennlp.training.optimizers <span class="keyword">import</span> AdamOptimizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_training_loop</span><span class="params">()</span>:</span></span><br><span class="line">    dataset_reader = build_dataset_reader()</span><br><span class="line"></span><br><span class="line">    train_data, dev_data = read_data(dataset_reader)</span><br><span class="line"></span><br><span class="line">    vocab = build_vocab(train_data + dev_data)</span><br><span class="line">    model = build_model(vocab)</span><br><span class="line"></span><br><span class="line">    train_loader, dev_loader = build_data_loaders(train_data, dev_data)</span><br><span class="line">    train_loader.index_with(vocab)</span><br><span class="line">    dev_loader.index_with(vocab)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># You obviously won't want to create a temporary file for your training</span></span><br><span class="line">    <span class="comment"># results, but for execution in binder for this guide, we need to do this.</span></span><br><span class="line">    <span class="keyword">with</span> tempfile.TemporaryDirectory() <span class="keyword">as</span> serialization_dir:</span><br><span class="line">        trainer = build_trainer(model, serialization_dir, train_loader, dev_loader)</span><br><span class="line">        print(<span class="string">"Starting training"</span>)</span><br><span class="line">        trainer.train()</span><br><span class="line">        print(<span class="string">"Finished training"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model, dataset_reader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_data_loaders</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    train_data: List[Instance],</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_data: List[Instance],</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Tuple[DataLoader, DataLoader]:</span></span><br><span class="line">    train_loader = SimpleDataLoader(train_data, <span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">    dev_loader = SimpleDataLoader(dev_data, <span class="number">8</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> train_loader, dev_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_trainer</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    model: Model,</span></span></span><br><span class="line"><span class="function"><span class="params">    serialization_dir: str,</span></span></span><br><span class="line"><span class="function"><span class="params">    train_loader: DataLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">    dev_loader: DataLoader,</span></span></span><br><span class="line"><span class="function"><span class="params">)</span> -&gt; Trainer:</span></span><br><span class="line">    parameters = [(n, p) <span class="keyword">for</span> n, p <span class="keyword">in</span> model.named_parameters() <span class="keyword">if</span> p.requires_grad]</span><br><span class="line">    optimizer = AdamOptimizer(parameters)  <span class="comment"># type: ignore</span></span><br><span class="line">    trainer = GradientDescentTrainer(</span><br><span class="line">        model=model,</span><br><span class="line">        serialization_dir=serialization_dir,</span><br><span class="line">        data_loader=train_loader,</span><br><span class="line">        validation_data_loader=dev_loader,</span><br><span class="line">        num_epochs=<span class="number">5</span>,</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> trainer</span><br><span class="line"></span><br><span class="line">run_training_loop()</span><br></pre></td></tr></table></figure><h3 id="用allennlp内置的训练框架"><a href="#用allennlp内置的训练框架" class="headerlink" title="用allennlp内置的训练框架"></a>用allennlp内置的训练框架</h3><p>上面的所有的<code>build_*</code>方法，allennlp都有对应实现，可以直接使用，然后自定义自己的<code>DatasetReader</code>和<code>Model</code>类。</p><p>这个方法通过<code>json</code>配置文件来指定所有的参数，然后框架会创建对应的对象，然后进行training loop。</p><p>AllenNLP依赖模型构造器中的类型注释，来正确地构造这些对象。</p><p>举例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_model</span><span class="params">(vocab: Vocabulary)</span> -&gt; Model:</span></span><br><span class="line">    print(<span class="string">"Building the model"</span>)</span><br><span class="line">    vocab_size = vocab.get_vocab_size(<span class="string">"tokens"</span>)</span><br><span class="line">    embedder = BasicTextFieldEmbedder(</span><br><span class="line">        &#123;<span class="string">"tokens"</span>: Embedding(embedding_dim=<span class="number">10</span>, num_embeddings=vocab_size)&#125;)</span><br><span class="line">    encoder = BagOfEmbeddingsEncoder(embedding_dim=<span class="number">10</span>)</span><br><span class="line">    <span class="keyword">return</span> SimpleClassifier(vocab, embedder, encoder)</span><br></pre></td></tr></table></figure><p>对应的<code>JSON</code>字典应该像下面这个：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">"model": &#123;</span><br><span class="line">    "type": "simple_classifier",</span><br><span class="line">    "embedder": &#123;</span><br><span class="line">        "token_embedders": &#123;</span><br><span class="line">            "tokens": &#123;</span><br><span class="line">                "type": "embedding",</span><br><span class="line">                "embedding_dim": 10</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    "encoder": &#123;</span><br><span class="line">        "type": "bag_of_embeddings",</span><br><span class="line">        "embedding_dim": 10</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>There are two special things to note: first, to select a particular subclass of a base type (e.g., SimpleClassifier as a subclass of Model,<br>or BagOfEmbeddingsEncoder as a subclass of Seq2VecEncoder) we need an additional “type”: “simple_classifier” key.<br>The string “simple_classifier” comes from the call to Model.register that we saw in the previous chapter.</p></blockquote><p>这里的vocab没有配置在json中，因为它通过data构造出来后，直接pass进model。</p><p>通常，显示为 <code>build_ *</code> 方法的参数的对象之间的顺序依赖关系被排除在配置文件之外，因为它们以不同的方式处理。</p><p>上面的方式同样应用于dataset reader, dataloader, trainer等所有对象。</p><p>实际上，allennlp用的是<code>Jsonnet</code>，是<code>json</code>的superset，支持更优雅的特征比如变量和imports，但是能像JSON文件一样使用。</p><p>上面的classifier的配置可以像下面这样：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"dataset_reader"</span> : &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"classification-tsv"</span>,</span><br><span class="line">        <span class="attr">"token_indexers"</span>: &#123;</span><br><span class="line">            <span class="attr">"tokens"</span>: &#123;</span><br><span class="line">                <span class="attr">"type"</span>: <span class="string">"single_id"</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"train_data_path"</span>: <span class="string">"quick_start/data/movie_review/train.tsv"</span>,</span><br><span class="line">    <span class="attr">"validation_data_path"</span>: <span class="string">"quick_start/data/movie_review/dev.tsv"</span>,</span><br><span class="line">    <span class="attr">"model"</span>: &#123;</span><br><span class="line">        <span class="attr">"type"</span>: <span class="string">"simple_classifier"</span>,</span><br><span class="line">        <span class="attr">"embedder"</span>: &#123;</span><br><span class="line">            <span class="attr">"token_embedders"</span>: &#123;</span><br><span class="line">                <span class="attr">"tokens"</span>: &#123;</span><br><span class="line">                    <span class="attr">"type"</span>: <span class="string">"embedding"</span>,</span><br><span class="line">                    <span class="attr">"embedding_dim"</span>: <span class="number">10</span></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="attr">"encoder"</span>: &#123;</span><br><span class="line">            <span class="attr">"type"</span>: <span class="string">"bag_of_embeddings"</span>,</span><br><span class="line">            <span class="attr">"embedding_dim"</span>: <span class="number">10</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"data_loader"</span>: &#123;</span><br><span class="line">        <span class="attr">"batch_size"</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="attr">"shuffle"</span>: <span class="literal">true</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="attr">"trainer"</span>: &#123;</span><br><span class="line">        <span class="attr">"optimizer"</span>: <span class="string">"adam"</span>,</span><br><span class="line">        <span class="attr">"num_epochs"</span>: <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置文件中可以看到所有在build_ *方法参数对应的条目（词汇表除外，我们将其省略，因为我们仅使用默认参数）。 通过按名称将JSON对象中的键与构造函数参数进行匹配来读取配置文件。如果keys不匹配，会有<code>ConfigurationError</code>。</p><p>这种训练方式使用<code>allennlp train [config.json] -s [serialization_dir]</code>。</p><p>allennlp通过<code>.register()</code>方法来导入类，和其他框架差不多，都是用装饰器。</p><h3 id="evaluate过程"><a href="#evaluate过程" class="headerlink" title="evaluate过程"></a>evaluate过程</h3><p>allennlp用<code>Metric</code>来追踪一些训练过程中的指标。</p><p>下面以accuracy指标为例。</p><p>只需要在模型的初始化中加入：<code>self.accuracy = CategoricalAccuracy()</code>，然后在forward path中更新metric，<code>self.accuracy(logits, label)</code>。</p><p>AllenNLP中每个metric都会维护<code>counts</code>的计数，来计算这个metric。对于上面的acc，维护的counts就是预测对的数量。每次调用acc实例的时候都会更新这个变量。通过<code>get_metrics()</code>方法可以得到。这个方法需要自己在模型中实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="comment"># ....</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_metrics</span><span class="params">(self, reset: bool = False)</span> -&gt; Dict[str, float]:</span></span><br><span class="line">        <span class="keyword">return</span> &#123;<span class="string">"accuracy"</span>: self.accuracy.get_metric(reset)&#125;</span><br></pre></td></tr></table></figure><p>命令行就直接<code>allennlp evaluate</code>就可以，它读入<code>allennlp train</code>命令生成的模型文件路径，以及包含test instances的文件路径，最后返回计算好的metric。</p><h3 id="prediction过程"><a href="#prediction过程" class="headerlink" title="prediction过程"></a>prediction过程</h3><p>train、evaluate过程中的样本都是有标签的，而perdition没有。因此如果想共用代码，只需要对label这个参数设置为optional。训练阶段会包含<code>LabelFields</code>，预测阶段没有。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DatasetReader.register('classification-tsv')</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ClassificationTsvReader</span><span class="params">(DatasetReader)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                 lazy: bool = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                 tokenizer: Tokenizer = None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 token_indexers: Dict[str, TokenIndexer] = None)</span>:</span></span><br><span class="line">        super().__init__(lazy)</span><br><span class="line">        self.tokenizer = tokenizer <span class="keyword">or</span> WhitespaceTokenizer()</span><br><span class="line">        self.token_indexers = token_indexers <span class="keyword">or</span> &#123;<span class="string">'tokens'</span>: SingleIdTokenIndexer()&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">text_to_instance</span><span class="params">(self, text: str, label: str = None)</span> -&gt; Instance:</span></span><br><span class="line">        tokens = self.tokenizer.tokenize(text)</span><br><span class="line">        text_field = TextField(tokens, self.token_indexers)</span><br><span class="line">        fields = &#123;<span class="string">'text'</span>: text_field&#125;</span><br><span class="line">        <span class="keyword">if</span> label:</span><br><span class="line">            fields[<span class="string">'label'</span>] = LabelField(label)</span><br><span class="line">        <span class="keyword">return</span> Instance(fields)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_read</span><span class="params">(self, file_path: str)</span> -&gt; Iterable[Instance]:</span></span><br><span class="line">        <span class="keyword">with</span> open(file_path, <span class="string">'r'</span>) <span class="keyword">as</span> lines:</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">                text, sentiment = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">                <span class="keyword">yield</span> self.text_to_instance(text, sentiment)</span><br></pre></td></tr></table></figure><p>同样，在model的<code>forward</code>方法中，也需要进行调整，因为没有label就没法算loss，也不需要算，只要返回结果。需要在参数中设置默认值<code>None</code>，然后在计算metrics和loss的外面加一个<code>if</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SimpleClassifier</span><span class="params">(Model)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">                text: Dict[str, torch.Tensor],</span></span></span><br><span class="line"><span class="function"><span class="params">                label: torch.Tensor = None)</span> -&gt; Dict[str, torch.Tensor]:</span></span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens, embedding_dim)</span></span><br><span class="line">        embedded_text = self.embedder(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_tokens)</span></span><br><span class="line">        mask = util.get_text_field_mask(text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, encoding_dim)</span></span><br><span class="line">        encoded_text = self.encoder(embedded_text, mask)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        logits = self.classifier(encoded_text)</span><br><span class="line">        <span class="comment"># Shape: (batch_size, num_labels)</span></span><br><span class="line">        probs = torch.nn.functional.softmax(logits)</span><br><span class="line">        output = &#123;<span class="string">'probs'</span>: probs&#125;</span><br><span class="line">        <span class="keyword">if</span> label <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.accuracy(logits, label)</span><br><span class="line">            <span class="comment"># Shape: (1,)</span></span><br><span class="line">            output[<span class="string">'loss'</span>] = torch.nn.functional.cross_entropy(logits, label)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>AllenNLPshiyong <code>predictors</code>来进行预测，它是一个在训练模型外面的包装类。主要工作是接受<code>json</code>格式的instances输入，转换为<code>instance</code>对象，传进模型，返回<code>json</code>格式的结果。</p><p><code>Predictor</code>需要继承基类<code>Predictor</code>并且实现<code>predict()</code>和<code>_json_to_instances()</code>方法。其他的由基类负责。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Predictor.register("sentence_classifier")</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceClassifierPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sentence: str)</span> -&gt; JsonDict:</span></span><br><span class="line">        <span class="comment"># This method is implemented in the base class.</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_json(&#123;<span class="string">"sentence"</span>: sentence&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_json_to_instance</span><span class="params">(self, json_dict: JsonDict)</span> -&gt; Instance:</span></span><br><span class="line">        sentence = json_dict[<span class="string">"sentence"</span>]</span><br><span class="line">        <span class="keyword">return</span> self._dataset_reader.text_to_instance(sentence)</span><br></pre></td></tr></table></figure><p>日常的一些任务都已经有内置的<code>Predictor</code>了，所以先检查一下有没有再自己写。</p><p>如果要自己写</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentenceClassifierPredictor</span><span class="params">(Predictor)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, sentence: str)</span> -&gt; JsonDict:</span></span><br><span class="line">        <span class="keyword">return</span> self.predict_json(&#123;<span class="string">"sentence"</span>: sentence&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_json_to_instance</span><span class="params">(self, json_dict: JsonDict)</span> -&gt; Instance:</span></span><br><span class="line">        sentence = json_dict[<span class="string">"sentence"</span>]</span><br><span class="line">        <span class="keyword">return</span> self._dataset_reader.text_to_instance(sentence)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># We've copied the training loop from an earlier example, with updated model</span></span><br><span class="line"><span class="comment"># code, above in the Setup section. We run the training loop to get a trained</span></span><br><span class="line"><span class="comment"># model.</span></span><br><span class="line">model, dataset_reader = run_training_loop()</span><br><span class="line">vocab = model.vocab</span><br><span class="line">predictor = SentenceClassifierPredictor(model, dataset_reader)</span><br><span class="line"></span><br><span class="line">output = predictor.predict(<span class="string">"A good movie!"</span>)</span><br><span class="line">print(</span><br><span class="line">    [</span><br><span class="line">        (vocab.get_token_from_index(label_id, <span class="string">"labels"</span>), prob)</span><br><span class="line">        <span class="keyword">for</span> label_id, prob <span class="keyword">in</span> enumerate(output[<span class="string">"probs"</span>])</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">output = predictor.predict(<span class="string">"This was a monstrous waste of time."</span>)</span><br><span class="line">print(</span><br><span class="line">    [</span><br><span class="line">        (vocab.get_token_from_index(label_id, <span class="string">"labels"</span>), prob)</span><br><span class="line">        <span class="keyword">for</span> label_id, prob <span class="keyword">in</span> enumerate(output[<span class="string">"probs"</span>])</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>会得到类似下面的输出：</p><blockquote><p>[(‘neg’, 0.48853254318237305), (‘pos’, 0.511467456817627)]<br>[(‘neg’, 0.5346643924713135), (‘pos’, 0.4653356373310089)]</p></blockquote><p>命令行和evaluate很像，接受模型的路径，和包含预测样本instances的json文件路径。</p><p>在<a href="https://github.com/allenai/allennlp-guide" target="_blank" rel="external nofollow noopener noreferrer">对应的repo</a>中，切换到敖<code>quick_start</code>目录可以直接执行下面的命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ allennlp train \</span><br><span class="line">    my_text_classifier.jsonnet \</span><br><span class="line">    --serialization-dir model \</span><br><span class="line">    --include-package my_text_classifier</span><br><span class="line"></span><br><span class="line">$ allennlp evaluate \</span><br><span class="line">    model/model.tar.gz \</span><br><span class="line">    data/movie_review/test.tsv \</span><br><span class="line">    --include-package my_text_classifier</span><br><span class="line"></span><br><span class="line">$ allennlp predict \</span><br><span class="line">    model/model.tar.gz \</span><br><span class="line">    data/movie_review/test.jsonl \</span><br><span class="line">    --include-package my_text_classifier \</span><br><span class="line">    --predictor sentence_classifier</span><br></pre></td></tr></table></figure><h2 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h2><h3 id="预训练情景"><a href="#预训练情景" class="headerlink" title="预训练情景"></a>预训练情景</h3><p>AllenNLP把很多模块抽象化，你只需要实现内部的细节，整个流程如何连接则不需要自己管。比如<code>TextFieldEmbedder</code>和<code>Seq2Vector</code>，只要接口和他们相同，就可以使用它。</p><p><code>SeqVevEncoder</code>可以是任何，只要是输入为一个序列向量<code>(batch_size, num_tokens, embedding_dim)</code>，输出为<code>(batch_size, encoding_dim)</code>，都可以使用它。比如bag of embedding, CNN, RNN, Transformer等。</p><p>使用bert的一个demo</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">local bert_model = <span class="string">"bert-base-uncased"</span>;</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">"dataset_reader"</span> : &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"classification-tsv"</span>,</span><br><span class="line">        <span class="string">"tokenizer"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"pretrained_transformer"</span>,</span><br><span class="line">            <span class="string">"model_name"</span>: bert_model,</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"token_indexers"</span>: &#123;</span><br><span class="line">            <span class="string">"bert"</span>: &#123;</span><br><span class="line">                <span class="string">"type"</span>: <span class="string">"pretrained_transformer"</span>,</span><br><span class="line">                <span class="string">"model_name"</span>: bert_model,</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"max_tokens"</span>: <span class="number">512</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"train_data_path"</span>: <span class="string">"quick_start/data/movie_review/train.tsv"</span>,</span><br><span class="line">    <span class="string">"validation_data_path"</span>: <span class="string">"quick_start/data/movie_review/dev.tsv"</span>,</span><br><span class="line">    <span class="string">"model"</span>: &#123;</span><br><span class="line">        <span class="string">"type"</span>: <span class="string">"simple_classifier"</span>,</span><br><span class="line">        <span class="string">"embedder"</span>: &#123;</span><br><span class="line">            <span class="string">"token_embedders"</span>: &#123;</span><br><span class="line">                <span class="string">"bert"</span>: &#123;</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"pretrained_transformer"</span>,</span><br><span class="line">                    <span class="string">"model_name"</span>: bert_model</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"encoder"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"bert_pooler"</span>,</span><br><span class="line">            <span class="string">"pretrained_model"</span>: bert_model</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"data_loader"</span>: &#123;</span><br><span class="line">        <span class="string">"batch_size"</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">"shuffle"</span>: true</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">"trainer"</span>: &#123;</span><br><span class="line">        <span class="string">"optimizer"</span>: &#123;</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"huggingface_adamw"</span>,</span><br><span class="line">            <span class="string">"lr"</span>: <span class="number">1.0e-5</span></span><br><span class="line">        &#125;,</span><br><span class="line">        <span class="string">"num_epochs"</span>: <span class="number">5</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下面直接复制了一段</p><blockquote><p>Use a PretrainedTransformerTokenizer (“pretrained_transformer”), which tokenizes the string into wordpieces and adds special tokens like [CLS] and [SEP]</p><p>Use a PretrainedTransformerIndexer (“pretrained_transformer”), which converts those wordpieces into ids using BERT’s vocabulary</p><p>Replace the embedder layer with a PretrainedTransformerEmbedder (“pretrained_transformer”), which uses a pretrained BERT model to embed the tokens, returning the top layer from BERT</p><p>Replace the encoder with a BertPooler (“bert_pooler”), which adds another (pretrained) linear layer on top of the [CLS] token and returns the result</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ allennlp train \</span><br><span class="line">    my_text_classifier.jsonnet \</span><br><span class="line">    --serialization-dir model-bert \</span><br><span class="line">    --include-package my_text_classifier</span><br></pre></td></tr></table></figure><p>修改不同的transformer架构如BERT，RoBERTa，XLNet，都只需要改<code>model_name</code>就可以。上面的修改也只是改了配置文件，没有改模型代码。</p><h3 id="部署网页展示demo"><a href="#部署网页展示demo" class="headerlink" title="部署网页展示demo"></a>部署网页展示demo</h3><p>还可以直接部署一个网页展示，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip install allennlp-server</span><br><span class="line">python allennlp-server/server_simple.py \</span><br><span class="line">    --archive-path model/model.tar.gz \</span><br><span class="line">    --predictor sentence_classifier \</span><br><span class="line">    --field-name sentence</span><br><span class="line">    --include-package my_text_classifier</span><br></pre></td></tr></table></figure><p>具体的需要再查。</p><h3 id="GPU相关"><a href="#GPU相关" class="headerlink" title="GPU相关"></a>GPU相关</h3><p>用allennlp不需要自己去让模型兼容gpu，或者手动把参数，模型移到gpu上。直接添加<code>cuda_device</code>选项，指定GPU的device id。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">"trainer": &#123;</span><br><span class="line">    ...</span><br><span class="line">    "cuda_device": 0</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者用分布式多gpu，使用pytorch的<code>DistributedDataParallel</code>包。eval和predict的时候可以直接在命令行指定cuda device。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">"trainer": &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;,</span><br><span class="line">"distributed": &#123;</span><br><span class="line">    "cuda_devices": [0, 1, 2, 3]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;刚开始学习使用AllenNLP框架，记录一些笔记供参考。&lt;/p&gt;&lt;p&gt;感觉这个框架用起来比较优雅，从基类的抽象，到模块间的配合。但缺点是官方的guide又臭又长，讲的也不是很清楚感觉，很多东西还是得边看源码边写。这个笔记是看官方guide记录，只能做到脑子里大概有个框架，要熟悉AllenNLP还是得看源码。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="AllenNLP" scheme="https://hanielxx.com/tags/AllenNLP/"/>
    
    <category term="NLP" scheme="https://hanielxx.com/tags/NLP/"/>
    
    <category term="OpenLibrary" scheme="https://hanielxx.com/tags/OpenLibrary/"/>
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="Pytorch" scheme="https://hanielxx.com/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>ClassyVision框架和多模态视频分类流程</title>
    <link href="https://hanielxx.com/MachineLearning/2021-04-03-classy-vision-and-multimodal-video-classification"/>
    <id>https://hanielxx.com/MachineLearning/2021-04-03-classy-vision-and-multimodal-video-classification</id>
    <published>2021-04-03T08:11:01.000Z</published>
    <updated>2021-07-02T09:15:34.703Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>记录对 ClassyVision 框架的理解，以及数据处理 pipeline。中间有用到 pytorch 的分布式训练。</p></div><a id="more"></a><h2 id="拉数据和处理"><a href="#拉数据和处理" class="headerlink" title="拉数据和处理"></a>拉数据和处理</h2><ol><li>先处理数据得到<code>videos.txt</code>，每行一个 pid</li><li>然后得到 pid-label 的类似<code>saishi_train_data_v3.txt</code>的文件。</li><li>然后得到<code>视频ID,概率值,标签,描述</code>的数据回查文件。</li><li>然后拉取数据，就是<code>run.sh</code></li><li>然后生成</li></ol><h3 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h3><ul><li>登录拉数据 webserver<ul><li><code>ssh xudi06@relay.corp.kuaishou.com</code></li><li><code>ssh web_server@bjpg-rs8553.yz02 -p22</code></li></ul></li><li><code>df -h</code>找到 mmu 项目路径<code>/mnt/mmu_ssd/share/yangfan/yuanwei/xingshisenlin_saishi</code></li><li>查看<code>run.sh</code></li><li>如果提示文件不存在类似信息，查看是否切换到webserver用户，是否有权限</li><li>如果执行出错，可能存在隐藏字符无法执行，自己手打试试</li></ul><h3 id="run-sh-脚本说明"><a href="#run-sh-脚本说明" class="headerlink" title="run.sh 脚本说明"></a>run.sh 脚本说明</h3><ul><li><p><code>run.sh</code>脚本文件</p></li><li><p>命令：webserver 下<code>bash run.sh</code></p></li><li><p>功能：拉取视频相关计数、特征、文本、视频封面和指定帧、用户特征</p></li><li><p>生成：<code>users.txt，author_2tower_embedding_by_photo_id，text.txt，cover目录，users_list.txt，users_list_out.txt</code></p></li><li><p>users_list 需要自己手动通过<code>users.txt</code>生成</p></li><li><p>准备说明：</p><ul><li>videos.txt: 每行一个 pid</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup sh /data/project/export_photo_info_count.sh -f `<span class="built_in">pwd</span>`/videos.txt -o `<span class="built_in">pwd</span>`/users.txt &amp;</span><br><span class="line">nohup sh /data/project/export_features.sh -i `<span class="built_in">pwd</span>`/videos.txt -o `<span class="built_in">pwd</span>` -t 10 -d <span class="string">"9"</span> &amp;</span><br><span class="line">nohup java -Dfile.encoding=UTF-8 -cp /data/project/chenxiaohui/kuaishou-mmu-data/*:kuaishou-mmu-data-1.0-SNAPSHOT.jar com.kuaishou.runner.BaseRunner -r  ExportTextSer    vice -i `<span class="built_in">pwd</span>`/videos.txt -o `<span class="built_in">pwd</span>`/text.txt -t 10 &amp;</span><br><span class="line">nohup sh  /data/project/export_frames.sh  `<span class="built_in">pwd</span>`/videos.txt `<span class="built_in">pwd</span>`/cover 1 100 30 &amp;</span><br></pre></td></tr></table></figure><h3 id="run-sh-导出视频相关计数"><a href="#run-sh-导出视频相关计数" class="headerlink" title="run.sh 导出视频相关计数"></a>run.sh 导出视频相关计数</h3><ul><li>bash 脚本：<code>export_photo_info_count.sh</code></li><li>功能：导出视频相关计数</li><li>输入：videos.txt</li><li>输出：users.txt</li><li>格式：photoId \t viewCount \t likeCount \t unlikeCount \t commentCount \t forwardCount \t authorId</li><li>样例：36853120853 77026 496 0 9 0 1337316458</li><li>wiki: <a href="https://docs.corp.kuaishou.com/k/home/VEPkMOB4UCl4/fcACxteH0QO2VqgPQNA1h5NOx" target="_blank" rel="external nofollow noopener noreferrer">https://docs.corp.kuaishou.com/k/home/VEPkMOB4UCl4/fcACxteH0QO2VqgPQNA1h5NOx</a></li><li>使用：<code>nohup sh /data/project/export_photo_info_count.sh -f `pwd`/videos.txt -o `pwd`/users.txt &amp;</code></li><li>程序运行参数说明：<ul><li>-f 输入文件，其中每一行是一个 photo id</li><li>-o 指定的输出文件，结果：photoId \t viewCount \t likeCount \t unlikeCount \t commentCount \t forwardCount \t authorId</li></ul></li></ul><h3 id="导出所有视频"><a href="#导出所有视频" class="headerlink" title="导出所有视频"></a>导出所有视频</h3><ul><li>webserver下，对777目录的</li><li>执行：<code>nohup sh /data/project/export_author_videos.sh -i "users.txt" -o  user_pids.txt" &amp;</code></li><li>输出：uid \t pid,pid,pid</li></ul><h3 id="run-sh-导出用户特征向量"><a href="#run-sh-导出用户特征向量" class="headerlink" title="run.sh 导出用户特征向量"></a>run.sh 导出用户特征向量</h3><ul><li>bash 脚本：<code>export_features.sh</code></li><li>功能：导出特征，-d9 是导出用户特征向量</li><li>输入：videos.txt</li><li>输出：author_2tower_embedding_by_photo_id</li><li>格式：pid \t 64 维的 embedding 向量，向量也可能为空</li><li>wiki：<a href="https://docs.corp.kuaishou.com/k/home/VBbdL4rQHmz4/fcABg6qainvICo6XTlhYqNAQN" target="_blank" rel="external nofollow noopener noreferrer">https://docs.corp.kuaishou.com/k/home/VBbdL4rQHmz4/fcABg6qainvICo6XTlhYqNAQN</a></li><li>使用：<code>nohup sh /data/project/export_features.sh -i `pwd`/videos.txt -o `pwd` -t 10 -d "9" &amp;</code></li><li>参数<ul><li>-i 指定将要传入的 photo id 数据文件，每一行一个 photo id，需要传入绝对路径，导出文件会自动生成</li><li>-o 指定输出文件的存放目录，绝对路径, 不能以”/“结尾</li><li>-t 线程个数</li><li>-d 想要的算法结果，以逗号分隔开，比如 -d “1,2”</li></ul></li><li>上述的<code>-d 9</code>表示输出<code>author_2tower_embedding_by_photo_id</code>，是推荐那边的双塔模型输出的作者信息编码，作用可以理解成这个作者偏向于发什么类型的视频。</li><li>这个后续会使用<code>generate_rec_for_raw_user_embeding_from_hive_file()</code>方法处理得到 rec</li><li>关于<code>-d</code>更多的信息看 wiki</li></ul><h3 id="run-sh-导出文本"><a href="#run-sh-导出文本" class="headerlink" title="run.sh 导出文本"></a>run.sh 导出文本</h3><ul><li>功能：导出视频相关文本，包括视频第一帧的 caption、标题 title、内容 text、OCR 结果、作者添加的视频字幕</li><li>wiki: <a href="https://docs.corp.kuaishou.com/k/home/VaBp46x_CVEw/fcABSVDkEsfNzNYd5emmlRsye" target="_blank" rel="external nofollow noopener noreferrer">https://docs.corp.kuaishou.com/k/home/VaBp46x_CVEw/fcABSVDkEsfNzNYd5emmlRsye</a></li><li>输入：videos.txt</li><li>输出：text.txt</li><li>格式：pid + caption + “\t” + title + “\t” + text + “\t” + ocr + “\t” + speech</li><li>先chmod 777 dir</li><li>命令：<code>nohup java -Dfile.encoding=UTF-8 -cp /data/project/chenxiaohui/kuaishou-mmu-data/*:kuaishou-mmu-data-1.0-SNAPSHOT.jar com.kuaishou.runner.BaseRunner -r ExportTextService -i `pwd`/videos.txt -o `pwd`/text.txt -t 10 &amp;</code></li><li>输出：<code>caption + "\t" + title + "\t" + text + "\t" + ocr + "\t" + speech</code></li><li>参数：<ul><li>-t 线程个数</li><li>-o 指定输出文件的存放目录，绝对路径, 不能以”/“结尾</li><li>输入方式一、输入指定 id 范围<ul><li>-s 开始 id</li><li>-e 结束 id</li></ul></li><li>输入方式二、输入指定 id<ul><li>-i 指定将要传入的 photo id 数据文件，每一行一个 photo id，需要传入绝对路径</li></ul></li></ul></li></ul><h3 id="run-sh-导出视频封面和指定帧"><a href="#run-sh-导出视频封面和指定帧" class="headerlink" title="run.sh 导出视频封面和指定帧"></a>run.sh 导出视频封面和指定帧</h3><ul><li>wiki: <a href="https://docs.corp.kuaishou.com/k/home/VPLKPDK8Fv-k/fcACJiIC2EN2KOKRYwvDciV0g" target="_blank" rel="external nofollow noopener noreferrer">https://docs.corp.kuaishou.com/k/home/VPLKPDK8Fv-k/fcACJiIC2EN2KOKRYwvDciV0g</a></li><li>命令：<code>nohup sh /data/project/export_frames.sh `pwd`/videos.txt `pwd`/cover 1 100 30 &amp;</code></li><li>输入：videos.txt</li><li>输出：cover 目录下的视频封面和指定帧</li><li>参数：<ul><li>第一个参数，photo id 列表文件，每一行一个 photo id，需要传入绝对路径</li><li>第二个参数，图片存储到的目录，绝对路径</li><li>第三个参数，运行类型。如果导出指定帧(type=2)，photo id 列表文件输入应该是 photo_id\t1,2,3 类型，帧 id 用逗号隔开，photo_id 和帧 id 用\t 隔开. 注意：\t 是不可见的符号，也可以用单个空格进行分割</li><li>第四个参数，线程数，最大 200</li><li>第五个参数，文件夹数量，一般为 10 (数据是按照”photo_id%文件夹数量“的方式下载存放的，请注意)，每个目录最多存放 10w 个</li></ul></li></ul><h3 id="get-user-list"><a href="#get-user-list" class="headerlink" title="get_user_list"></a>get_user_list</h3><p>准备好 author id，和<code>cut -f7 -d $'\t' users.txt | sort | uniq &gt; users_list.txt</code>效果一样</p><h3 id="run-sh-导出用户特征"><a href="#run-sh-导出用户特征" class="headerlink" title="run.sh 导出用户特征"></a>run.sh 导出用户特征</h3><ul><li>wiki: <a href="https://docs.corp.kuaishou.com/k/home/VdeW2QONo4Qw/fcAB7R1aarXIFGzBDBwTvkI0D" target="_blank" rel="external nofollow noopener noreferrer">https://docs.corp.kuaishou.com/k/home/VdeW2QONo4Qw/fcAB7R1aarXIFGzBDBwTvkI0D</a></li><li>命令：<code>sh /data/project/user_info_export.sh -i `pwd`/users_list.txt</code></li><li>输入：<code>users_list.txt</code></li><li>输出：<code>users_list_out.txt</code></li><li>格式：userId  用户名 用户简介</li><li>参数：<ul><li>-i 输入文件，其中每一行是一个 author id</li></ul></li><li>需要先准备好 author id：<code>cut -f7 -d $'\t' users.txt | sort | uniq &gt; users_list.txt</code></li><li>输出：运行成功会在输入文件的文件夹产生一个新的文件，文件名与输入文件名前缀相同，比如  author.txt 会生成  author_out.txt<br>      结果格式为 userId  用户名 用户简介</li></ul><h3 id="mk-train-org"><a href="#mk-train-org" class="headerlink" title="mk_train_org"></a>mk_train_org</h3><p>把拉取下来的，存放在 cover 目录中的数据，打上对应的 label。输出<code>train_org.txt</code>文件。</p><p>label 文件是人工标注的结果 或者自己清洗得到的</p><p>关键代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取label和对应的类别编号</span></span><br><span class="line"><span class="comment"># 0 n 其他</span></span><br><span class="line"><span class="comment"># 1 n 舞蹈</span></span><br><span class="line">labelindex={}</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'/share/yuanwei05/xingshisenlin/saishi/data/saishi_class.label'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> fin:</span><br><span class="line">      l=l.strip().split()</span><br><span class="line">      labelindex[l[<span class="number">2</span>]]=int(l[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 给每个拉取下来存放在cover的pic，打上对应的label，结果如下</span></span><br><span class="line"><span class="comment"># /share/yangfan/yuanwei/xingshisenlin_saishi/cover/0/36872762760.jpg 0</span></span><br><span class="line"><span class="comment"># /share/yangfan/yuanwei/xingshisenlin_saishi/cover/0/36873258930.jpg 0</span></span><br><span class="line">pidlabel={}</span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./saishi_train_data_v3.txt'</span>,<span class="string">'r'</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> fin:</span><br><span class="line">      l=l.strip().split()</span><br><span class="line">      pid=l[<span class="number">0</span>]</span><br><span class="line">      label=l[<span class="number">1</span>]</span><br><span class="line">      pidlabel[pid]=label</span><br><span class="line">    <span class="comment">#with open('./coarse_train_v3/train_org.txt','w') as fout:</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./train_data_v3/train_org.txt'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> fout:</span><br><span class="line">      image_dir=<span class="string">'./cover'</span></span><br><span class="line">      <span class="keyword">for</span> target <span class="keyword">in</span> sorted(os.listdir(image_dir)):</span><br><span class="line">        d = os.path.join(image_dir, target)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(d):</span><br><span class="line">          <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">for</span> root, _, fnames <span class="keyword">in</span> sorted(os.walk(d,followlinks=<span class="literal">True</span>)):</span><br><span class="line">          <span class="keyword">for</span> fname <span class="keyword">in</span> sorted(fnames):</span><br><span class="line">            <span class="keyword">if</span> has_file_allowed_extension(fname, IMG_EXTENSIONS):</span><br><span class="line">              path = os.path.join(root, fname)</span><br><span class="line">              image_id = fname.split(<span class="string">"."</span>)[<span class="number">0</span>]</span><br><span class="line">              <span class="keyword">if</span> <span class="string">'_'</span> <span class="keyword">in</span> image_id:</span><br><span class="line">                image_id=image_id.split(<span class="string">'_'</span>)[<span class="number">0</span>]</span><br><span class="line">              <span class="keyword">if</span> image_id <span class="keyword">in</span> pidlabel:</span><br><span class="line">                <span class="comment">#if pidlabel[image_id]=='婴儿否':</span></span><br><span class="line">                <span class="comment">#    continue</span></span><br><span class="line">                fout.write(<span class="string">'%s\t%d\n'</span>%(os.path.abspath(path),labelindex[pidlabel[image_id]]))</span><br></pre></td></tr></table></figure><h3 id="transfer-user-pid"><a href="#transfer-user-pid" class="headerlink" title="transfer_user_pid"></a>transfer_user_pid</h3><ul><li>输入<code>users_list_out.txt</code>文件</li><li>输出<code>users_profile.txt</code></li><li>输出文件格式是<code>pid 用户名 用户简介</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transfer_user_pid</span><span class="params">()</span>:</span></span><br><span class="line">    lines = open(<span class="string">'users_list_out.txt'</span>).readlines()</span><br><span class="line">    dic = {}</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        s = line.strip(<span class="string">'\n'</span>).split(<span class="string">'\t'</span>)</span><br><span class="line">        dic[s[<span class="number">0</span>]] = <span class="string">'\t'</span>.join(s[<span class="number">1</span>:])</span><br><span class="line"></span><br><span class="line">    lines = open(<span class="string">'users.txt'</span>).readlines()</span><br><span class="line">    output_f = open(<span class="string">'users_profile.txt'</span>, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        s = line.strip(<span class="string">'\n'</span>).split(<span class="string">'\t'</span>)</span><br><span class="line">        pid = s[<span class="number">0</span>]</span><br><span class="line">        info = dic[s[<span class="number">-1</span>]]</span><br><span class="line">        output_f.write(pid + <span class="string">'\t'</span> + info + <span class="string">'\n'</span>)</span><br></pre></td></tr></table></figure><h3 id="split-train-val"><a href="#split-train-val" class="headerlink" title="split_train_val"></a>split_train_val</h3><ul><li>输入：<code>train_org.txt</code></li><li>输出：<code>train_data.txt</code>和<code>val_data.txt</code></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_train_val</span><span class="params">(data_path, save_path)</span>:</span></span><br><span class="line">    f = open(os.path.join(data_path, <span class="string">"train_org.txt"</span>), <span class="string">"r"</span>)</span><br><span class="line">    lines = f.readlines()</span><br><span class="line">    f.close()</span><br><span class="line">    random.seed(<span class="number">123</span>)</span><br><span class="line">    random.shuffle(lines)</span><br><span class="line">    pos = int(len(lines) * <span class="number">0.95</span>)</span><br><span class="line">    train_f = open(os.path.join(save_path, <span class="string">"train_data.txt"</span>), <span class="string">"w"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines[:pos]:</span><br><span class="line">        train_f.write(line)</span><br><span class="line">    train_f.close()</span><br><span class="line">    val_f = open(os.path.join(save_path,<span class="string">"val_data.txt"</span>), <span class="string">"w"</span>)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines[pos:]:</span><br><span class="line">        val_f.write(line)</span><br><span class="line">    val_f.close()</span><br></pre></td></tr></table></figure><h3 id="generate-rec-for-raw-user-embeding"><a href="#generate-rec-for-raw-user-embeding" class="headerlink" title="generate_rec_for_raw_user_embeding"></a>generate_rec_for_raw_user_embeding</h3><ul><li>输入<code>text.txt, author_2tower_embedding_by_photo_id_path, users_profile.txt</code></li><li>输出<code>train.txt, val.txt, train.idx, train.rec, val.idx, val.rec</code></li><li>依赖<code>mxnet</code>和<code>encode</code>函数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(record, index, label, data)</span>:</span></span><br><span class="line">    <span class="comment">#idx_file = 'tmp.idx'</span></span><br><span class="line">    <span class="comment">#rec_file = 'tmp.rec'</span></span><br><span class="line">    <span class="comment">#record = mx.recordio.MXIndexedRecordIO(idx_file, rec_file, 'w')</span></span><br><span class="line">    header = mx.recordio.IRHeader(<span class="number">0</span>, label, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="comment">#mes = json.dumps([1,2,3])</span></span><br><span class="line">    s = mx.recordio.pack(header, data)</span><br><span class="line">    record.write_idx(index, s)</span><br></pre></td></tr></table></figure><h2 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h2><h3 id="bash-运行脚本"><a href="#bash-运行脚本" class="headerlink" title="bash 运行脚本"></a>bash 运行脚本</h3><p>路径：<code>/share/yuanwei/multimodal/run_saishi.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/opt/conda/envs/rapids/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=/opt/conda/envs/rapids:<span class="variable">$PYTHONPATH</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># export CUDA_VISIBLE_DEVICES=0,1,2,3,5,6,7</span></span><br><span class="line"><span class="comment"># python -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 --master_addr=localhost --master_port=29600 --use_env \</span></span><br><span class="line"><span class="comment">#                classy_train.py --skip_tensorboard --device=gpu --config=/share/yuanwei/xingshisenlin/saishi/configs/config_attention_ywcode_0301_data_v3.json \</span></span><br><span class="line"><span class="comment"># --num_workers=4 --log_freq=50 --distributed_backend=ddp --checkpoint_folder /share/yuanwei05/xingshisenlin/saishi/models/models_20210301 &gt; /share/yuanwei05/xingshisenlin/saishi/20210301.log 2&gt;&amp;1</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># mkdir /share/yuanwei/xingshisenlin/saishi/clean_data_iter/models_20210225_focus_sub_val</span></span><br><span class="line"><span class="comment"># mv /share/yuanwei/model_test_tmp/*.txt /share/yuanwei/xingshisenlin/saishi/clean_data_iter/models_20210225_focus_sub_val</span></span><br><span class="line"><span class="comment"># wait</span></span><br><span class="line"><span class="comment"># python -m torch.distributed.launch --nnodes=1 --nproc_per_node=8 --master_addr=localhost --master_port=29600 --use_env \</span></span><br><span class="line"><span class="comment">#                classy_train.py --skip_tensorboard --device=gpu --config=/share/yuanwei/xingshisenlin/saishi/configs/config_attention_ywcode_testonly_data_v2.json \</span></span><br><span class="line"><span class="comment"># --num_workers=4 --log_freq=50 --distributed_backend=ddp --checkpoint_folder /share/yuanwei05/xingshisenlin/saishi/models/models_20210225_focus_sub &gt; /share/yuanwei05/xingshisenlin/saishi/testonly.log 2&gt;&amp;1</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># mkdir /share/yuanwei/xingshisenlin/saishi/clean_data_iter/models_20210225_focus_sub_train</span></span><br><span class="line"><span class="comment"># mv /share/yuanwei/model_test_tmp/*.txt /share/yuanwei/xingshisenlin/saishi/clean_data_iter/models_20210225_focus_sub_train</span></span><br><span class="line"></span><br><span class="line">sh predict_saishi_eval.sh /share/yuanwei05/xingshisenlin/saishi/<span class="built_in">eval</span>/usemodel_0301_eval_0121-0128_prob_tmp /share/yuanwei05/xingshisenlin/saishi/models/models_20210301/checkpoint.torch 8 1</span><br><span class="line"></span><br><span class="line"><span class="built_in">wait</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#sh run_saishi_clean_data.sh</span></span><br></pre></td></tr></table></figure><h4 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h4><ul><li><p>–nnodes=1: 当前 job 包含一个节点</p></li><li><p>–nproc_per_node=8: 每个节点 8 个任务</p></li><li><p>–master_addr: master 节点的 ip</p></li><li><p>–master_port=29600: master 节点的 port</p></li><li><p>–use_env: 如果为 TRUE，用 LOCAL_RANK 环境变量给子进程传递 local_rank，这时候程 y 序不会传递–local_rank</p></li><li><p>–config: classy_train 的配置文件</p></li><li><p>num_workers: classy_vision.trainer 多线程的线程数</p></li><li><p>–distributed_backend: 用于设置 trainer_class 为 DistributedTrainer</p></li></ul><h3 id="文件说明"><a href="#文件说明" class="headerlink" title="文件说明"></a>文件说明</h3><ol><li>运行<code>/share/yuanwei/multimodal/classy_train.py</code>训练</li><li>配置文件为<code>/share/yuanwei/xingshisenlin/saishi/configs/config_attention_ywcode_0301_data_v3.json</code></li><li>保存 checkpoint 的 checkpoint_folder 为<code>/share/yuanwei05/xingshisenlin/saishi/models/models_20210301</code></li><li>预训练 ckp 为<code>/share/wuxiangyu/multimodal/models_ft/finetune.pt</code></li><li>task 文件为<code>fine_tuning_task_pretrain.py</code>，继承了<code>ClassificationTask(ClassyTask)</code>，通过<code>classy_train.py</code>中的<code>build_task(config)</code>得到</li><li>loss 文件为<code>class_multimodal_rawuser_noisetxt_noiseauthor_outtarget.py</code></li><li>原始 dataset 说明<ol><li>会直接<code>self.lines = open(meta_file).readlines()</code>读取 meta 文件</li><li>MXNet 通过索引记录文件和相应的索引文件读取<ol><li><code>self.record = mx.recordio.MXIndexedRecordIO(self.idx_file, self.rec_file, 'r')</code></li><li><code>self.record.read_idx(index)</code></li></ol></li><li>从 rec 文件中获得一些特征，text_feature，user_profile_feature，user_feature</li><li><code>__getitem__</code>返回的是<code>image, int(target), text_feature, user_profile_feature, torch.tensor(user_feature),int(index)</code></li></ol></li><li>classy_dataset 文件为<code>class_multimodal_rawuser_noisetxt_noiseauthor_outtarget.py</code>，<ol><li>其中训练文件为<code>/share/yangfan/yuanwei/xingshisenlin_saishi/train_data_v3/</code></li><li>其中 train 数据样例为<code>/share/yangfan/yuanwei/xingshisenlin_saishi/cover/11/38989219271.jpg 38989219271 38989219271 38989219271 0</code></li><li>dataset 类文件为<code>multimodal_dataset_raw_user.py</code></li><li>dataset同时会读取<code>rec</code>文件和<code>idx</code>文件</li><li>还会在其中创建 bert-tokenizer 和 transform</li><li>还会生成 noise words 和 drop words 和 target map</li><li>会将 dataset 中的<code>target</code>通过<code>"target_file": "/share/yuanwei/xingshisenlin/saishi/data/saishi_class.target</code>转换成 one-hot 编码的类别</li><li>会给<code>user_feature</code>添加 noise</li><li>会给<code>text_feature</code>按照 drop file 中的词进行 drop，就是将 drop file 中的词都删掉，然后在<code>randint(0,min(len(s),self.token_max_length-10))</code>位置随机加入一个 noise file 中的词。后面还会做 padding</li><li><code>user_profile_feature</code>和<code>text_feature</code>操作相同，但是不会 drop word 和 noise</li><li>最后会将<code>sample[0:-1]</code>进行 transform，然后加上<code>index</code>返回</li></ol></li><li>meters 文件为<code>accuracy_meter_allownegative_dict_3.py</code>，只看了基本的函数，其他还没具体看，后面遇到了再回来看</li><li>optimizer: SGD</li><li>models 文件为<code>classy_multimodal_yw_3.py</code>，其中<ol><li>image_model：resnet，内嵌</li><li>image_fc_model：mlp</li><li>text_model：nlp_new,<code>/home/xudi06/multimodal/classy_vision/models/nlp_new.py</code><ol><li>bert: bert-base-chinese</li><li>mlp</li></ol></li><li>user_profile_model：nlp_new</li><li>user_model：mlp</li><li>fuse_model: multi_head_attention_new</li><li>heads: FullyConnectedHeadFeaDict</li></ol></li></ol><h2 id="模型测试和评估"><a href="#模型测试和评估" class="headerlink" title="模型测试和评估"></a>模型测试和评估</h2><p>测试的过程大体如下：</p><ol><li>取一批新数据，没有标注，作为测试集（非验证集）</li><li>获取这些样本的data、特征等，和训练的特征一样</li><li>有两种方式喂给模型<ol><li>和训练方式相同：gputest，流程是：<ol><li>修改gputest-config，将drop和noise比例置0</li><li>和模型训练相同，只是epoch为1</li><li>在模型训练的代码中有一段代码会保存结果在一个临时目录<blockquote><p>classification_task:812<br>self.write_test[local_rank]=open(‘/share/yuanwei05/model_test_tmp/%s.txt’%(local_rank),’w’)</p></blockquote></li></ol></li><li>使用<code>run_.sh-&gt;predict_.sh-&gt;test_.sh</code>，加载checkpoint，然后一条一条读入，进行预测，得到结果。</li></ol></li><li>如果是gputest方式，手动将临时结果目录下的文件，copy到自己的目录下</li><li>使用<code>/share/yuanwei05/wudao/eval/get_shuf_202104_strategy.py</code>随机采样test结果，然后进行数据回查</li><li>在数据回查的过程中，设置一个阈值，高于这个阈值的就为真</li><li>通过阈值，每个类采样一两百给人工评，可以算出precision，近似整个测试集的precision</li><li>对于训练的模型，每个phase都会保存model，checkpoint会保存最后一个phase的model，如果模型没有过拟合，那么最后一个phase即checkpoint的结果和之前最好的model，效果差不会很大，一个点左右无所谓。</li></ol><p>最后的生成文件如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">&gt; ll -t /share/yangfan/yuanwei/wudao_eval_2021_0401-0403</span><br><span class="line">total 34G</span><br><span class="line">-rw-r--r-- 1 1001 1001  19K Apr  7 12:16 data_format_gputest.py</span><br><span class="line">-rw-r--r-- 1 root root  16M Apr  7 12:14 train.idx</span><br><span class="line">-rw-r--r-- 1 root root 6.6G Apr  7 12:14 train.rec</span><br><span class="line">-rw-r--r-- 1 root root  91M Apr  7 12:14 train.txt</span><br><span class="line">-rw-r--r-- 1 root root  51M Apr  7 12:07 train_data.txt</span><br><span class="line">-rw-r--r-- 1 root root  51M Apr  7 12:04 train_org.txt</span><br><span class="line">-rw-r--r-- 1 root root 1.7G Apr  7 12:03 audio_128.txt</span><br><span class="line">-rw-r--r-- 1 root root  780 Apr  7 11:55 needRunVideoEmbedding.txt</span><br><span class="line">-rw-r--r-- 1 root root 4.1G Apr  7 11:55 video_embedding.txt</span><br><span class="line">-rw-r--r-- 1 root root  866 Apr  7 11:51 get_videoembedding.py</span><br><span class="line">-rw-rw-r-- 1 1001 1001  59M Apr  7 11:09 users_profile.txt</span><br><span class="line">-rw-rw-r-- 1 1001 1001  21G Apr  7 11:08 audio</span><br><span class="line">-rw-rw-r-- 1 1001 1001 395M Apr  7 11:08 author_2tower_embedding_by_photo_id</span><br><span class="line">-rw------- 1 1001 1001 416K Apr  7 11:08 nohup.out</span><br><span class="line">-rw-rw-r-- 1 1001 1001  33M Apr  7 11:08 users_list_out.txt</span><br><span class="line">-rw-rw-r-- 1 1001 1001 411M Apr  7 11:06 text.txt</span><br><span class="line">drwxrwxr-x 1 1001 1001   30 Apr  7 11:04 cover/</span><br><span class="line">-rw-rw-r-- 1 1001 1001 4.2M Apr  7 11:01 users_list.txt</span><br><span class="line">-rw-r--r-- 1 1001 1001  701 Apr  7 11:01 run.sh</span><br><span class="line">-rw-rw-r-- 1 1001 1001  24M Apr  7 10:55 users.txt</span><br><span class="line">-rw-r--r-- 1 root root 8.0M Apr  7 10:51 videos.txt</span><br><span class="line">-rw-r--r-- 1 root root 1.2K Apr  7 10:30 mk_train_org.py</span><br><span class="line">-rw-r--r-- 1 1001 1001 5.0K Apr  7 10:28 main.py</span><br><span class="line">-rw-r--r-- 1 1001 1001  863 Apr  7 10:28 find_noframe0.py</span><br><span class="line">-rw-r--r-- 1 1001 1001  748 Apr  7 10:28 convert_to_128.py</span><br><span class="line">-rwxr-xr-x 1 1001 1001  350 Apr  7 10:28 get_user_by_photo.sh*</span><br></pre></td></tr></table></figure><h2 id="推荐类别项目"><a href="#推荐类别项目" class="headerlink" title="推荐类别项目"></a>推荐类别项目</h2><h3 id="关键词收集"><a href="#关键词收集" class="headerlink" title="关键词收集"></a>关键词收集</h3><ul><li>推荐</li><li>必备</li><li>分享</li><li>适合</li><li>好物</li><li>推广</li></ul><h3 id="数据量"><a href="#数据量" class="headerlink" title="数据量"></a>数据量</h3><table><thead><tr><th>类别</th><th>已标注</th><th>未标注</th><th>共计</th></tr></thead><tbody><tr><td>穿搭</td><td>9993</td><td>5007</td><td>15000</td></tr><tr><td>旅游</td><td>2845</td><td>8622</td><td>11467</td></tr><tr><td>美食</td><td>8498</td><td>6520</td><td>15018</td></tr><tr><td>美妆</td><td>9027</td><td>5973</td><td>15000</td></tr><tr><td>汽车</td><td>3522</td><td>11478</td><td>15000</td></tr><tr><td>亲子</td><td>2898</td><td>10514</td><td>13412</td></tr><tr><td>游戏</td><td>3334</td><td>11666</td><td>15000</td></tr></tbody></table>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;记录对 ClassyVision 框架的理解，以及数据处理 pipeline。中间有用到 pytorch 的分布式训练。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="Private" scheme="https://hanielxx.com/tags/Private/"/>
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="Pytorch" scheme="https://hanielxx.com/tags/Pytorch/"/>
    
    <category term="ClassyVision" scheme="https://hanielxx.com/tags/ClassyVision/"/>
    
    <category term="MultiModal" scheme="https://hanielxx.com/tags/MultiModal/"/>
    
    <category term="VideoClassification" scheme="https://hanielxx.com/tags/VideoClassification/"/>
    
    <category term="DistributedTraining" scheme="https://hanielxx.com/tags/DistributedTraining/"/>
    
  </entry>
  
  <entry>
    <title>BERT-FineTune源码理解</title>
    <link href="https://hanielxx.com/MachineLearning/2021-03-31-bert-finetune-analysis"/>
    <id>https://hanielxx.com/MachineLearning/2021-03-31-bert-finetune-analysis</id>
    <published>2021-03-31T08:11:01.000Z</published>
    <updated>2021-05-25T15:36:53.752Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>之前有分析了BERT预训练的源码，即<code>create_pretraining_data.py</code>和<code>run_pretrain.py</code>两个文件，参见<a href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis">BERT-预训练源码理解</a>。</p><p>现在对<code>run_classifier.py</code>进行分析。</p><p>待更新上传</p></div><a id="more"></a>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;之前有分析了BERT预训练的源码，即&lt;code&gt;create_pretraining_data.py&lt;/code&gt;和&lt;code&gt;run_pretrain.py&lt;/code&gt;两个文件，参见&lt;a href=&quot;https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis&quot;&gt;BERT-预训练源码理解&lt;/a&gt;。&lt;/p&gt;&lt;p&gt;现在对&lt;code&gt;run_classifier.py&lt;/code&gt;进行分析。&lt;/p&gt;&lt;p&gt;待更新上传&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="Private" scheme="https://hanielxx.com/tags/Private/"/>
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="BERT" scheme="https://hanielxx.com/tags/BERT/"/>
    
    <category term="Pretrain" scheme="https://hanielxx.com/tags/Pretrain/"/>
    
  </entry>
  
  <entry>
    <title>2021-生活札记</title>
    <link href="https://hanielxx.com/Daily/2021-03-30-lift-notes"/>
    <id>https://hanielxx.com/Daily/2021-03-30-lift-notes</id>
    <published>2021-03-30T14:53:30.000Z</published>
    <updated>2021-07-02T07:09:12.792Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>从<code>2021.3.31</code>起，记录每日生活札记。</p><p>2021 年 3 月 30 日晚，11：00 睡，希望能开个好头~</p></div><a id="more"></a><h2 id="2021-年-4-月"><a href="#2021-年-4-月" class="headerlink" title="2021 年 4 月"></a>2021 年 4 月</h2><h3 id="第一周-4-1-4-7"><a href="#第一周-4-1-4-7" class="headerlink" title="第一周 4.1-4.7"></a>第一周 4.1-4.7</h3><table><thead><tr><th>日期</th><th>起床</th><th>睡觉</th><th>学习 和 工作</th><th>备注</th></tr></thead><tbody><tr><td>3.31</td><td>7:10</td><td>11:40</td><td>1. 上午入职<br>2. 下午看各种材料申请各种权限<br>3. 开始看多模框架代码</td><td>太累了，早上 5 点多醒，早起去拿核酸报告<br>中午不知道午休时间没休息<br>下午开始头晕，腿酸麻</td></tr><tr><td>4.1</td><td>8:10</td><td>1:45</td><td>1. 配置服务器和 mac 环境<br>2. 看<a href="https://classyvision.ai/tutorials" target="_blank" rel="external nofollow noopener noreferrer">classyvision-tutorials</a><br>3. 快手入职相关</td><td>比第一天好一点<br>但还是很累<br>知道了自己面试的面评失眠了</td></tr><tr><td>4.2</td><td>8:15</td><td>1:30</td><td>1. 看完<a href="https://classyvision.ai/tutorials" target="_blank" rel="external nofollow noopener noreferrer">classyvision-tutorials</a>并做了思维导图<br>2. 和 mentor 汇报<br>配置服务器和 mac 环境并 debug<br>3. 办了张招商银行工资卡</td><td>1. 状态一般，用了 降噪耳机白天舒服多了<br>2. 后面的任务感觉压力还是挺大的，要加油<br>3. 后面三天清明假期，要好好补课程作业了</td></tr><tr><td>4.3 -4.7</td><td></td><td></td><td>1. 熟悉了数据处理源码<br>2. 理解数据处理的源码<br>3. 跑通了赛事任务</td><td></td></tr></tbody></table><h3 id="第二周-4-8-4-14"><a href="#第二周-4-8-4-14" class="headerlink" title="第二周 4.8-4.14"></a>第二周 4.8-4.14</h3><table><thead><tr><th>日期</th><th>起床</th><th>睡觉</th><th>学习 和 工作</th><th>备注</th></tr></thead><tbody><tr><td>4.8</td><td>8:32</td><td>1:12</td><td>1. 开始接触推荐类别任务<br>2. 了解hive表拉取数据到模型迭代的流程<br>3. 写数据处理data_format pipeline，重新定义接口</td><td></td></tr><tr><td>4.9</td><td>8:28</td><td>1:34</td><td>1. 对推荐已有的数据进行部分回查<br>2. 修改推荐的配置，初步跑通得到一个base<br>3. 看源码</td><td>代码看多了看的很晕</td></tr><tr><td>4.10</td><td>8:38</td><td></td><td>1. 学习hive sql<br>2. 理解模型如何计算评价指标<br>3. 看源码了解训练过程</td><td>理解的还不是特别清楚</td></tr></tbody></table><div class="note info"><p>每天好忙。。日常十一点多到宿舍，只想躺着歇会。。</p><p>每天好枯燥。。重复性劳动真的好没意思。。</p><p>懒得写了。。七月底离职吧。。诶</p></div>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;从&lt;code&gt;2021.3.31&lt;/code&gt;起，记录每日生活札记。&lt;/p&gt;&lt;p&gt;2021 年 3 月 30 日晚，11：00 睡，希望能开个好头~&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Daily" scheme="https://hanielxx.com/categories/Daily/"/>
    
    
    <category term="Daily" scheme="https://hanielxx.com/tags/Daily/"/>
    
    <category term="LifeNotes" scheme="https://hanielxx.com/tags/LifeNotes/"/>
    
    <category term="Private" scheme="https://hanielxx.com/tags/Private/"/>
    
  </entry>
  
  <entry>
    <title>实习准备和总结</title>
    <link href="https://hanielxx.com/Notes/2021-03-22-jobhunting-interview-notes"/>
    <id>https://hanielxx.com/Notes/2021-03-22-jobhunting-interview-notes</id>
    <published>2021-03-22T13:11:35.000Z</published>
    <updated>2021-05-17T15:34:23.117Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>近两周的实习总结。</p></div><a id="more"></a><h2 id="目前研究内容"><a href="#目前研究内容" class="headerlink" title="目前研究内容"></a>目前研究内容</h2><ol><li>文本分类等方法？<ol><li>这里其实主要是二分类，效率高的和效率低的。在分类的基础上，希望能量化这个沉默效率，因此预测 siRNA 对靶基因的一个沉默效率，可以抽象从靶基因沉默的后验概率。</li><li>模型方面尝试了一些机器学习模型，比如随机森林、GBDT 等，也有用了 word2vec、BiLSTM 等模型，以及有用过 Attention 机制。最近在研究是用 BERT 预训练所有已知的靶基因 RNA 序列。</li><li>对 siRNA 进行特征提取和挖掘占据了比较多时间，因为生物背景太强了，除了已知的序列特征、热力学特征以及 GC 碱基含量之外，其他的如空间结构信息、多模 motif 的影响、和靶基因的结合位点等等，这些特征在生物学上的影响还不是很清楚。需要花很多时间去找论文和实验。</li></ol></li></ol><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="BiLSTM-模型"><a href="#BiLSTM-模型" class="headerlink" title="BiLSTM 模型"></a>BiLSTM 模型</h3><ol><li>BiLSTM 就是双向的 LSTM 模型。</li><li>LSTM 就是一个比较基本的 RNN 模型，每个 LSTM cell 有三个门控，遗忘门、输入门、输出门。分别控制要遗忘的信息多少、对输入信息进行限制以及 cell 的输出控制。它们的权重其实就是输入和上一次的输出 ht 拼接在一起，然后分别乘以权重向量，再通过 sigmoid 激活函数得到。</li><li>三个门但是有四个 MLP，因为还有一个是通过上一次的输出和这次的输入计算当前 cell 的输入，用 tanh 激活函数</li><li>当前 cell 状态的计算是上一次的单元状态*遗忘门+当前单元状态*输入门</li><li>梯度消失：<ol><li>如果 t-j 很大，也就是 j 距离 t 时间步很远，当<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="9.524ex" height="2.022ex" role="img" focusable="false" viewBox="0 -853.7 4209.5 893.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="msup" transform="translate(815.5, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1103.2, 363) scale(0.707)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g><g data-mml-node="mo" transform="translate(2653.7, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(3709.5, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>时，会产生梯度爆炸问题，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="9.524ex" height="2.022ex" role="img" focusable="false" viewBox="0 -853.7 4209.5 893.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(571, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="msup" transform="translate(815.5, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1103.2, 363) scale(0.707)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g><g data-mml-node="mo" transform="translate(2653.7, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mn" transform="translate(3709.5, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>时，会产生梯度消失问题。</li><li>t 是当前时间步，j 是历史时间步，求 loss 损失偏导的时候是累乘的。</li></ol></li><li>写 LSTM 的公式<ol><li>输入：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="21.162ex" height="2.388ex" role="img" focusable="false" viewBox="0 -805.6 9353.4 1055.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mo" transform="translate(742.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(1798.6, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(2159.6, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2688.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3288.6, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(3864.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4253.6, 0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(5301.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msup" transform="translate(5579.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(6456.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6901.5, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8686.4, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mo" transform="translate(8964.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>输入门控向量：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.234ex" height="2.444ex" role="img" focusable="false" viewBox="0 -830.4 8501.5 1080.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1036.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2092.5, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2663.5, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3052.5, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1103.2, 363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4449.7, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msup" transform="translate(4727.7, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5604.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6049.6, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7834.5, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mo" transform="translate(8112.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>输出门控向量：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.682ex" height="2.388ex" role="img" focusable="false" viewBox="0 -805.6 8699.5 1055.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mo" transform="translate(1135.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2191.5, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2762.5, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3151.5, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1103.2, 363) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mo" transform="translate(4647.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msup" transform="translate(4925.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5802.9, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6247.6, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8032.5, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mo" transform="translate(8310.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>遗忘门控向量：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="19.89ex" height="2.515ex" role="img" focusable="false" viewBox="0 -861.5 8791.5 1111.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g><g data-mml-node="mo" transform="translate(1181.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2237.5, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2808.5, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3197.5, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mi" transform="translate(1103.2, 363) scale(0.707)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g><g data-mml-node="mo" transform="translate(4739.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msup" transform="translate(5017.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5894.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6339.5, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(576, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8124.5, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g><g data-mml-node="mo" transform="translate(8402.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>当前 cell state：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="19.251ex" height="2.135ex" role="img" focusable="false" viewBox="0 -861.5 8509 943.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1016, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(2071.8, 0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g><g data-mml-node="mo" transform="translate(3198, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msup" transform="translate(3698.2, 0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="TeXAtom" transform="translate(433, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(361, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1139, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(5562.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msup" transform="translate(6562.6, 0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7543.7, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(8044, 0)"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g></g></g></svg></mjx-container></li><li>隐藏状态 hidden state：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="16.691ex" height="2.388ex" role="img" focusable="false" viewBox="0 -805.6 7377.5 1055.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(576, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1159, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(2214.8, 0)"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"></path></g><g data-mml-node="mi" transform="translate(465, 363) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g><g data-mml-node="mo" transform="translate(3295, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(3795.2, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(4156.2, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(4685.2, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5285.2, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(5861.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(6250.2, 0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(433, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(6988.5, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>输出 output：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="12.911ex" height="2.388ex" role="img" focusable="false" viewBox="0 -805.6 5706.7 1055.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(490, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(1073, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2128.8, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(2699.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(3088.8, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="mo" transform="translate(1103.2, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="msup" transform="translate(4436.5, 0)"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mi" transform="translate(576, 363) scale(0.707)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g></g><g data-mml-node="mo" transform="translate(5317.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol></li><li>LSTM 的优缺点<ol><li>优点：相比简单 RNN 可以记住更长的时间步骤，避免了无休止的连乘，而是边加边乘，一定程度上解决了梯度消失问题</li><li>缺点：RNN 的梯度问题在 LSTM 及其变种里面得到了一定程度的解决，但还是不够。它可以处理 100 个量级的序列，而对于 1000 个量级，或者更长的序列则依然会显得很棘手。</li><li>缺点 2：计算费时。每一个 LSTM 的 cell 里面都意味着有 4 个全连接层(MLP),如果 LSTM 的时间跨度很大，并且网络又很深，这个计算量会很大，很耗时。</li><li>缺点 3：参数量过多就会存在过拟合的风险。</li></ol></li></ol><h3 id="BERT-模型"><a href="#BERT-模型" class="headerlink" title="BERT 模型"></a>BERT 模型</h3><ol><li>BERT 主要是使用 transformer 的 encoder 模型堆叠起来。</li><li>BERT 的输入<ol><li>包含了 token embedding、position embedding 还有 segment embedding。</li><li>token embedding 是模型中关于词最主要的信息</li><li>segment embedding 是因为 BERT 里面的下一句的预测任务，所以会有两句拼接起来。比如前一句的 segment emebdding 为 1，后一句为 0。句子末尾都有加[SEP]结尾符，两句拼接开头有[CLS]符</li><li>position embedding 是表示文本不同位置的字/词所携带的语义信息存在差异，可以通过学习得到，也可以通过设置一个跟位置或者时序相关的函数得到，比如设置一个正弦或者余弦函数</li></ol></li><li>特点是 MLM 和 NSP 机制。<ol><li>MLM 就是 Masked Language Model 简单来说就是讲一句话盖住几个词，然后来预测它们。bert 中是将 15%的词 mask 掉，但是这 15%中又分为 80%用 mask 表示、10%随机表示、10%不变。</li><li>NSP 机制就是预测两句话是不是连续的。bert 在创建输入的时候就是设置 50%为正确的顺序关系，50%是随机语料中提取的。</li></ol></li><li>BERT 相比 LSTM 的优点：<ol><li>包含词的信息和距离无关：LSTM 相比 RNN 能一定程度解决梯度消失问题，但依然无法解决太长的距离。而 BERT 基于 Transformer 的 encoder，使用多头自注意力，不管当前词和其他词的空间距离有多远，<strong>包含其他词的信息不取决于距离，而是取决于两者的相关性</strong>，这是 Transformer 的第一个优势</li><li>用到后面的词：对于 Transformer 这样的结构来说，在对当前词进行计算的时候，不仅可以用到前面的词，也可以用到后面的词。而 RNN 只能用到前面的词，这并不是个严重的问题，因为这可以通过双向 RNN 来解决。</li><li>计算效率：RNN 和 LSTM 都是顺序结构，必须算出一个隐向量才能计算后一个，那么这就意味着隐向量无法同时并行计算，导致计算效率低。而 Transformer 一次输入一整个句子的所有 embedding 计算，不存在这个问题。</li></ol></li></ol><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><ol><li>看博客<a href="https://hanielxx.com/MachineLearning/2020-08-15-attention-is-all-you-need">Attention is all you need</a></li><li>self-attention 的计算</li><li>残差连接</li><li>输入的 position encoding</li><li>多头注意力</li><li>Batch Normalization Normalization</li></ol><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><ol><li>word2vec 有两种方式，cbow 和 skip gram，前者是用周围词预测中心词，后者是用中心词预测周围词。实现方式上可以使用层次化 softmax 和负采样方式，这样对计算量来说和大规模词表的情况都比较好。</li><li>参考博客<ol><li><a href="http://www.cnblogs.com/pinard/p/7160330.html" target="_blank" rel="external nofollow noopener noreferrer">刘建平-word2vec 原理（一）</a></li><li><a href="http://www.cnblogs.com/pinard/p/7243513.html" target="_blank" rel="external nofollow noopener noreferrer">刘建平-word2vec 原理（二）</a></li><li><a href="https://www.cnblogs.com/pinard/p/7249903.html" target="_blank" rel="external nofollow noopener noreferrer">刘建平-word2vec 原理（三）</a></li></ol></li><li>负采样：采样的过程需要指定总体的概率分布，根据词的出现频率采样</li><li>损失函数：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.117ex" xmlns="http://www.w3.org/2000/svg" width="41.524ex" height="3.022ex" role="img" focusable="false" viewBox="0 -841.7 18353.4 1335.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mo" transform="translate(1041.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2097.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(2875.6, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(3173.6, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3658.6, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(4135.6, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(4706.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msubsup" transform="translate(5095.6, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mrow" transform="translate(828.3, 363) scale(0.707)"><g data-mml-node="mo"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g><g data-mml-node="mi" transform="translate(275, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(583, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(6666.1, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(7242.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(7853.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="munder" transform="translate(8853.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1057.3, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msub" transform="translate(1724.3, 0)"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(944, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1066, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g></g></g></g></g><g data-mml-node="mi" transform="translate(12819.9, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(13117.9, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(13602.9, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(14079.9, 0)"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"></path></g><g data-mml-node="mo" transform="translate(14650.9, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(15039.9, 0)"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msubsup" transform="translate(778, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mrow" transform="translate(828.3, 363) scale(0.707)"><g data-mml-node="mo"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g><g data-mml-node="mi" transform="translate(275, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(583, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g><g data-mml-node="mi" transform="translate(2348.6, 0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g><g data-mml-node="mo" transform="translate(17964.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.509ex" height="1.359ex" role="img" focusable="false" viewBox="0 -443 1108.9 600.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g></g></g></svg></mjx-container>是输出单词，正样本</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.576ex" xmlns="http://www.w3.org/2000/svg" width="3.132ex" height="2.294ex" role="img" focusable="false" viewBox="0 -759 1384.2 1013.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(485, 363) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g><g data-mml-node="TeXAtom" transform="translate(485, -247) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g></g></g></g></g></svg></mjx-container>是它的词向量</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.303ex" height="1.595ex" role="img" focusable="false" viewBox="0 -694 576 705"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></svg></mjx-container>是隐藏层输出，在 CBOW 中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="15.574ex" height="3.023ex" role="img" focusable="false" viewBox="0 -975.6 6883.6 1336.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(853.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(1909.6, 0)"><g data-mml-node="mn" transform="translate(311.9, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><rect width="737.4" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(3053.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(433, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1211, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msub" transform="translate(5536.1, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></g></g></svg></mjx-container>，在 Skip-gram 中，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.592ex" xmlns="http://www.w3.org/2000/svg" width="7.146ex" height="2.162ex" role="img" focusable="false" viewBox="0 -694 3158.7 955.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(853.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(1909.6, 0)"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="TeXAtom" transform="translate(485, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></g></svg></mjx-container>就是中心词</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex" xmlns="http://www.w3.org/2000/svg" width="20.111ex" height="2.363ex" role="img" focusable="false" viewBox="0 -749.5 8888.9 1044.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"></path></g><g data-mml-node="TeXAtom" transform="translate(944, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1066, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2362.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(3418.6, 0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g><g data-mml-node="mi" transform="translate(716, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1057.3, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(1335.3, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(2025.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(3080.9, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3803.1, 0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(4303.3, 0)"><path data-c="B7" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mi" transform="translate(4581.3, 0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g></g></g></svg></mjx-container>是负例采样的集合</li><li>所以 skip-gram 的损失就是输入的中心词和整个正样本做 bmm，然后再和所有负采样得到的负样本做 bmm，然后做 logsigmoid，再求 batch 的平均</li></ol></li></ol><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><h3 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h3><h3 id="HMM"><a href="#HMM" class="headerlink" title="HMM"></a>HMM</h3><h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><h3 id="Attention-机制"><a href="#Attention-机制" class="headerlink" title="Attention 机制"></a>Attention 机制</h3><ol><li>siRNA 那个项目中的 Attention 主要是使用最后一层 LSTM 中的 hidden 向量进行 attention 计算，实验了简单的 dot attention、concat attention 和 general attention 几种方式，最后还是 dot attention 效果最好。</li><li>dot attention 是直接 encode output 和 hidden 点乘求和得到权重</li><li>concat attention 是 encoder output 和 hidden 拼接后使用一个带有 tanh 激活函数的全连接层处理，然后进行线性变换得到的</li><li>general attention 是对 encoder output 进行线性变换后，再与 hidden 进行点乘得到权重。</li><li>Transformer 中的 Attention 是 Self Attention，因为是通过对当前层的输入直接得到，因此叫 self attention</li></ol><h3 id="GBDT-模型"><a href="#GBDT-模型" class="headerlink" title="GBDT 模型"></a>GBDT 模型</h3><ol><li>参考<a href="https://hanielxx.com/Notes/2020-12-08-ucas-big-data-analysis-review.html#Gradient-Boost-Decision-Tree-GBDT">笔记</a></li></ol><h3 id="随机森林"><a href="#随机森林" class="headerlink" title="随机森林"></a>随机森林</h3><ol><li><p>首先，RF 使用了<strong>CART 决策树</strong>作为弱学习器，这让我们想到了梯度提升树 GBDT。</p></li><li><p>第二，在使用决策树的基础上，RF 对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的 n 个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是 RF 通过<strong>随机选择节点上的一部分样本特征</strong>，这个数字小于 n，假设为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.822ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1689.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(600, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1041, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></g></g></svg></mjx-container>，然后在这些随机选择的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.822ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 1689.4 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(600, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(469, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1041, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g></g></g></g></g></svg></mjx-container> 个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。</p></li><li><p>RF 在数据采样上用的 booststrap 采样，即有放回采样。</p></li><li><p>RF 的主要优点有：</p><ol><li>训练可以高度并行化，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。</li><li>由于可以随机选择决策树节点划分特征，这样在样本特征维度很高的时候，仍然能高效的训练模型。</li><li>在训练后，可以给出各个特征对于输出的重要性</li><li>由于采用了随机采样，训练出的模型的方差小，泛化能力强。</li><li>相对于 Boosting 系列的 Adaboost 和 GBDT， RF 实现比较简单。</li><li>对部分特征缺失不敏感。</li></ol></li><li><p>RF 的主要缺点有：</p><ul><li>在某些噪音比较大的样本集上，RF 模型容易陷入过拟合。</li><li>取值划分比较多的特征容易对 RF 的决策产生更大的影响，从而影响拟合的模型的效果。</li></ul></li></ol><h3 id="Bagging-和-Boosting-的区别"><a href="#Bagging-和-Boosting-的区别" class="headerlink" title="Bagging 和 Boosting 的区别"></a>Bagging 和 Boosting 的区别</h3><ol><li>偏差—方差<ul><li>Boosting：从偏差—方差分解角度看，降低偏差。</li><li>Bagging：从偏差—方差分解角度看，降低方差。</li></ul></li><li>样本选择：<ul><li>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化.而权值是根据上一轮的分类结果进行调整。</li><li>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</li></ul></li><li>样例权重：<ul><li>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</li><li>Bagging：使用均匀取样，每个样例的权重相等</li></ul></li><li>基学习器权重：<ul><li>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重.</li><li>Bagging：所有弱分类器的权重相等.</li></ul></li><li>串、并行计算：<ul><li>Boosting：串行，各个及学习器只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</li><li>Bagging：各个预测函数可以并行生成。</li></ul></li></ol><h3 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h3><ol><li>和 GBDT 区别和联系：<ol><li>同样是 CART 回归树</li><li>和 GBDT 比较像，GBDT 使用平方误差作为损失函数，选取特征切分点的依据是最小化每个节点的损失函数，GBDT 拟合的是当前模型的残差。单独使用 GBDT 模型容易出现过拟合的现象，因为先前被选取的特征权重较大，留给后面特征的拟合是相对较小的残差。实际应用中会选择 GBDT+LR 的方式做模型训练。</li><li>GBDT 和 XGBoost 都是逐个添加树，属于 Boosting 算法。因此使用 Additive Traning 方式优化</li></ol></li><li>XGBoost 的优化目标：<ol><li>除了拟合数据的损失，还有正则化项用于控制模型复杂度。这个正则化项和叶子节点数 T 以及每个叶子大小<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.407ex" height="1.027ex" role="img" focusable="false" viewBox="0 -443 622 454"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g></g></g></svg></mjx-container>有关，要让他们尽可能小。</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.972ex" xmlns="http://www.w3.org/2000/svg" width="24.637ex" height="3.134ex" role="img" focusable="false" viewBox="0 -955.8 10889.5 1385.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="3A9" d="M55 454Q55 503 75 546T127 617T197 665T272 695T337 704H352Q396 704 404 703Q527 687 596 615T666 454Q666 392 635 330T559 200T499 83V80H543Q589 81 600 83T617 93Q622 102 629 135T636 172L637 177H677V175L660 89Q645 3 644 2V0H552H488Q461 0 456 3T451 20Q451 89 499 235T548 455Q548 512 530 555T483 622T424 656T361 668Q332 668 303 658T243 626T193 560T174 456Q174 380 222 233T270 20Q270 7 263 0H77V2Q76 3 61 89L44 175V177H84L85 172Q85 171 88 155T96 119T104 93Q109 86 120 84T178 80H222V83Q206 132 162 199T87 329T55 454Z"></path></g><g data-mml-node="mo" transform="translate(722, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1111, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(1661, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2327.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(3383.6, 0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mi" transform="translate(3926.6, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(4852.8, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mfrac" transform="translate(5853, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mi" transform="translate(6646.6, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g><g data-mml-node="munderover" transform="translate(7396.2, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(412, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1190, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="msubsup" transform="translate(9863.9, 0)"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mn" transform="translate(622, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(622, -284.4) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container></li></ol></li><li>XGBoost 寻找最优节点值：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.251ex" xmlns="http://www.w3.org/2000/svg" width="12.146ex" height="3.619ex" role="img" focusable="false" viewBox="0 -1046.6 5368.4 1599.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mo" transform="translate(622, 363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(622, -259.6) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1303.3, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2359.1, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(3137.1, 0)"><g data-mml-node="msub" transform="translate(717.1, 548.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1172.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(1950.3, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="1991.3" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>此时目标函数为：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.251ex" xmlns="http://www.w3.org/2000/svg" width="26.797ex" height="4.04ex" role="img" focusable="false" viewBox="0 -1232.8 11844.2 1785.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mi" transform="translate(763, 0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(1192, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(1881.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2937.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(3715.6, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="munderover" transform="translate(4675.8, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(412, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1190, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mfrac" transform="translate(7143.5, 0)"><g data-mml-node="msubsup" transform="translate(695.1, 643.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mn" transform="translate(786, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(786, -284.4) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1172.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(1950.3, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="1991.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(9597, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(10597.2, 0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g><g data-mml-node="mi" transform="translate(11140.2, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>。</li><li>其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.551ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 1127.3 999.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>是第 j 个叶子节点中所有样本在损失函数对当前模型的一阶偏导数的求和。</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.652ex" height="2.211ex" role="img" focusable="false" viewBox="0 -683 1172.3 977.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>是二阶偏导数的求和。这个是通过泰勒二阶展开后，计算目标函数的二次项极值得到。</li><li>这时候的目标函数是关于 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.179ex" height="1.668ex" role="img" focusable="false" viewBox="0 -443 963.3 737.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D714" d="M495 384Q495 406 514 424T555 443Q574 443 589 425T604 364Q604 334 592 278T555 155T483 38T377 -11Q297 -11 267 66Q266 68 260 61Q201 -11 125 -11Q15 -11 15 139Q15 230 56 325T123 434Q135 441 147 436Q160 429 160 418Q160 406 140 379T94 306T62 208Q61 202 61 187Q61 124 85 100T143 76Q201 76 245 129L253 137V156Q258 297 317 297Q348 297 348 261Q348 243 338 213T318 158L308 135Q309 133 310 129T318 115T334 97T358 83T393 76Q456 76 501 148T546 274Q546 305 533 325T508 357T495 384Z"></path></g><g data-mml-node="mi" transform="translate(622, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="1.593ex" height="1.532ex" role="img" focusable="false" viewBox="0 -677 704 677"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></svg></mjx-container>的二次函数。</li></ol></li><li>XGBoost 的分裂：<ol><li>从第一个节点分裂使用贪心算法，遍历所有特征的所有划分点选出增益 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.251ex" xmlns="http://www.w3.org/2000/svg" width="5.048ex" height="3.619ex" role="img" focusable="false" viewBox="0 -1046.6 2231.3 1599.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="msub" transform="translate(717.1, 548.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1172.3, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(1950.3, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="1991.3" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>最大的点。</li><li>为了限制增长可以设置（1）分裂的增益阈值（2）最大深度（3）最小样本权重和（分裂后任意叶子节点值不能低于这个值）防止过拟合。</li><li>分裂的增益定义为： <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.469ex" xmlns="http://www.w3.org/2000/svg" width="41.246ex" height="4.076ex" role="img" focusable="false" viewBox="0 -1152.3 18230.5 1801.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1315, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1660, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2537.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3593.6, 0)"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g><g data-mml-node="mrow" transform="translate(4387.1, 0)"><g data-mml-node="mo"><path data-c="5B" d="M224 -649V1150H455V1099H275V-598H455V-649H224Z"></path></g><g data-mml-node="mfrac" transform="translate(472, 0)"><g data-mml-node="msubsup" transform="translate(717.1, 552.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mn" transform="translate(786, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(786, -300) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mo" transform="translate(1362.5, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2140.5, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="2125.8" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(3060.1, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mfrac" transform="translate(4060.3, 0)"><g data-mml-node="msubsup" transform="translate(717.1, 562.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mn" transform="translate(786, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(786, -300) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mo" transform="translate(1417.7, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2195.7, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="2164.8" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(6687.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(7687.6, 0)"><g data-mml-node="mrow" transform="translate(315.3, 516.8) scale(0.707)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(389, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mo" transform="translate(1706.5, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(2484.5, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="msup" transform="translate(3857.2, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g></g><g data-mml-node="mo" transform="translate(1362.5, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(2140.5, 0)"><g data-mml-node="mi"><path data-c="1D43B" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 219 683Q260 681 355 681Q389 681 418 681T463 682T483 682Q499 682 499 672Q499 670 497 658Q492 641 487 638H485Q483 638 480 638T473 638T464 637T455 637Q416 636 405 634T387 623Q384 619 355 500Q348 474 340 442T328 395L324 380Q324 378 469 378H614L615 381Q615 384 646 504Q674 619 674 627T617 637Q594 637 587 639T580 648Q580 650 582 660Q586 677 588 679T604 682Q609 682 646 681T740 680Q802 680 835 681T871 682Q888 682 888 672Q888 645 876 638H874Q872 638 869 638T862 638T853 637T844 637Q805 636 794 634T776 623Q773 618 704 340T634 58Q634 51 638 51Q646 48 692 46H723Q729 38 729 37T726 19Q722 6 716 0H701Q664 2 567 2Q533 2 504 2T458 2T437 1Q420 1 420 10Q420 15 423 24Q428 43 433 45Q437 46 448 46H454Q481 46 514 49Q520 50 522 50T528 55T534 64T540 82T547 110T558 153Q565 181 569 198Q602 330 602 331T457 332H312L279 197Q245 63 245 58Q245 51 253 49T303 46H334Q340 38 340 37T337 19Q333 6 327 0H312Q275 2 178 2Q144 2 115 2T69 2T48 1Q31 1 31 10Q31 12 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mi" transform="translate(831, -150) scale(0.707)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g></g><g data-mml-node="mo" transform="translate(3558.2, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(4336.2, 0)"><path data-c="1D706" d="M166 673Q166 685 183 694H202Q292 691 316 644Q322 629 373 486T474 207T524 67Q531 47 537 34T546 15T551 6T555 2T556 -2T550 -11H482Q457 3 450 18T399 152L354 277L340 262Q327 246 293 207T236 141Q211 112 174 69Q123 9 111 -1T83 -12Q47 -12 47 20Q47 37 61 52T199 187Q229 216 266 252T321 306L338 322Q338 323 288 462T234 612Q214 657 183 657Q166 657 166 673Z"></path></g></g><rect width="3678.4" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(11606, 0)"><path data-c="5D" d="M16 1099V1150H247V-649H16V-598H196V1099H16Z"></path></g></g><g data-mml-node="mo" transform="translate(16687.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(17687.5, 0)"><path data-c="1D6FE" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path></g></g></g></svg></mjx-container>，其实就是上一次的损失减去分裂后的损失差值。</li></ol></li><li>加快分裂：<ol><li>分裂的时候贪心遍历分割点，称为全局扫描法。</li><li>当数据量过大导致内存无法一次载入或者在分布式情况下，贪心算法的效率就会变得很低，全局扫描法不再适用。</li><li>基于此，XGBoost 提出了一系列<strong>加快寻找最佳分裂点</strong>的方案：<ol><li>特征预排序+缓存：XGBoost 在训练之前，预先对每个特征按照特征值大小进行<strong>排序</strong>，然后保存为 block 结构，后的迭代中会重复地使用这个结构，使计算量大大减小。</li><li>分位点近似法：对每个特征按照特征值排序后，采用类似<strong>分位点</strong>选取的方式，仅仅<strong>选出常数个特征值作为该特征的候选分割点</strong>，在寻找该特征的最佳分割点时，从候选分割点中选出最优的一个。</li><li>并行查找：由于各个特性已预先存储为 block 结构，XGBoost 支持利用<strong>多个线程并行地计算每个特征的最佳分割点</strong>，这不仅大大提升了结点的分裂速度，也极利于大规模训练集的适应性扩展。</li></ol></li></ol></li><li>防止过拟合：<ol><li>Shrinkage：每次迭代中对树的<strong>每个叶子结点</strong>的分数乘上一个缩减权重 η，这可以使得每一棵树的影响力不会太大，留下更大的空间给后面生成的树去优化模型</li><li>Column subsampling：选取部分特征进行建树。其可分为两种，一种是按层随机采样，在对同一层内每个结点分裂之前，先随机选择一部分特征，然后只需要遍历这部分的特征，来确定最优的分割点。另一种是随机选择特征，则建树前随机选择一部分特征然后分裂就只遍历这些特征。一般情况下前者效果更好。</li></ol></li><li>可并行的近似算法：<ol><li>对于连续型特征值，当样本数量非常大，该特征取值过多时，遍历所有取值会花费很多时间，且容易过拟合。因此 XGBoost 思想是对特征进行分桶，即找到 l 个划分点，将位于相邻分位点之间的样本分在一个桶中。在遍历该特征的时候，只需要遍历各个分位点，从而计算最优划分。和 LGBM 不一样的地方是，直方图算法是在统计了频次之后找分割点。</li><li>从算法伪代码中该流程还可以分为两种，全局的近似是在新生成一棵树之前就对各个特征计算分位点并划分样本，之后在每次分裂过程中都采用近似划分，而局部近似就是在具体的某一次分裂节点的过程中采用近似算法。</li></ol></li><li>缺失值处理：<ol><li>当样本的第<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="0.781ex" height="1.52ex" role="img" focusable="false" viewBox="0 -661 345 672"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>个特征值缺失时，无法利用该特征进行划分时，XGBoost 的想法是将该样本分别划分到左结点和右结点，然后计算其增益，哪个大就划分到哪边。</li></ol></li><li>优点：<ol><li>GBDT 以 CART 为基分类器，XGBoost 还支持线性分类器。</li><li>传统的 GBDT 只用到了一阶导数，XGBoost 还用到了二阶导数信息。XGBoost 工具还可以自定义代价函数，只要支持一阶和二阶求导。</li><li>代价函数中加入了正则项，控制叶子节点数和叶子节点上的权重。降低了模型的复杂度，防止过拟合。</li><li>有 Shrinkage 缩减的特性，在每次迭代完之后可以将叶子节点乘上一个权重，削弱每棵树的影响，让后面有更大的学习空间。</li><li>column subsampling 特征抽样，防止过拟合减少计算</li><li>对缺失值的处理，XGBoost 可以自动学习出它的分裂方向。</li><li>支持并行，是在特征粒度上的并行，没有改变 boost 的串行结构。决策树最耗时的是对特征的值排序，而 XGboost 使用了特征预排序，后面就可以在节点分裂的时候对每个特征使用多线程并行计算。</li><li>可以并行的近似直方图算法。在数据量比较大的时候，贪心算法效率会很低，XGBoost 提出了可并行的近似直方图算法，能高效的生成候选分割点。</li></ol></li><li>缺点：<ol><li>空间消耗大：预排序要保存数据的特征值，以及特征排序后的结果，需要消耗训练数据两倍的内存。</li><li>时间上的开销大：遍历每个分割点的时候，都需要计算分裂增益。</li><li>对 cache 的优化不友好：在预排序后，特征的访问是一种随机访问，并且不同的特征访问的顺序不一样，无法对 cache 进行优化。在每一层长树的时候，需要随机访问一个行索引到叶子索引的数组，并且不同特征访问的顺序也不一样，也会造成较大的 cache miss</li><li>分裂的时候是贪心算法，不适合处理稀疏数据，会产生很多的子树，训练特别慢</li><li>level-wise 的生成策略，导致一些节点没必要的划分浪费时间</li></ol></li><li>计算特征权重的方式一共五种，常用 Gain：<ol><li>‘weight’：权重形式，表示在所有树中，一个特征在分裂节点时被使用了多少次。</li><li>‘gain’：（平均）增益形式，表示在所有树中，一个特征作为分裂节点存在时，带来的增益的平均值。</li><li>‘cover’：（平均）覆盖度，表示在所有树中，一个特征作为分裂节点存在时，覆盖的样本数量的平均值。</li><li>‘total_gain’：相对于’gain’，这里表示的是带来的总增益大小。</li><li>‘total_cover’：相对于’cover’，这里表示的是覆盖的总样本数量</li></ol></li></ol><h3 id="LGBM-模型"><a href="#LGBM-模型" class="headerlink" title="LGBM 模型"></a>LGBM 模型</h3><ol><li><strong>优化：</strong><ol><li>基于 Histogram 的决策树算法。</li><li>单边梯度采样 Gradient-based One-Side Sampling(GOSS)：使用 GOSS 可以减少大量只具有小梯度的数据实例，这样在计算信息增益的时候只利用剩下的具有高梯度的数据就可以了，相比 XGBoost 遍历所有特征值节省了不少时间和空间上的开销。</li><li>互斥特征捆绑 Exclusive Feature Bundling(EFB)：使用 EFB 可以将许多互斥的特征绑定为一个特征，这样达到了降维的目的。</li><li>带深度限制的 Leaf-wise 的叶子生长策略：大多数 GBDT 工具使用低效的按层生长 (level-wise) 的决策树生长策略，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销。实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。LightGBM 使用了带有深度限制的按叶子生长 (leaf-wise) 算法。（其实 XGBoost 也有）</li><li>直接支持类别特征(Categorical Feature)</li><li>支持高效并行</li><li>Cache 命中率优化</li></ol></li><li><strong>直方图算法：</strong><ol><li>先把连续的浮点特征值离散化成 k 个整数，同时构造一个宽度为 k 的直方图</li><li>在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，<strong>直方图累积了需要的统计量</strong>，然后根据直方图的离散值，遍历寻找最优的分割点。</li><li><strong>内存占用更小</strong>：直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用 8 位整型存储就足够了，内存消耗可以降低为原来的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="1.795ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 793.6 1225.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container> 。XGBoost 需要用 32 位的浮点数去存储特征值，并用 32 位的整形去存储索引，而 LightGBM 只需要用 8 位去存储直方图；</li><li><strong>计算代价更小</strong>：预排序算法 XGBoost 每遍历一个特征值就需要计算一次分裂的增益，而直方图算法 LightGBM 只需要计算 k 次。其实就是把搜索特征分裂值的复杂度降低了。</li></ol></li><li><strong>直方图做差加速：</strong><ol><li>一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到，在速度上可以提升一倍。通常构造直方图时，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的 k 个桶。</li><li>在实际构建树的过程中，LightGBM 还可以先计算直方图小的叶子节点，然后利用直方图做差来获得直方图大的叶子节点，这样就可以用非常微小的代价得到它兄弟叶子的直方图。</li></ol></li><li><strong>带深度限制的 Leaf-wise 算法：</strong><ol><li>和 XGBoost 的 Level-wise 不同，xgb 遍历一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上 Level-wise 是一种低效的算法，因为它不加区分的对待同一层的叶子，实际上很多叶子的分裂增益较低，没必要进行搜索和分裂，因此带来了很多没必要的计算开销。</li><li>Leaf-wise：每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。</li><li>Leaf-wise 的优点是：在分裂次数相同的情况下，Leaf-wise 可以降低更多的误差，得到更好的精度</li><li>Leaf-wise 的缺点是：可能会长出比较深的决策树，产生过拟合。</li><li>LightGBM 会在 Leaf-wise 之上增加了一个<strong>最大深度的限制</strong>，在保证高效率的同时防止过拟合。</li></ol></li><li><strong>单边梯度采样算法：</strong><ol><li>Gradient-based One-Side Sampling，GOSS 算法从<strong>减少样本的角度</strong>出发，排除大部分小梯度的样本，仅用剩下的样本计算信息增益，它是一种在减少数据量和保证精度上平衡的算法。</li><li>梯度小的样本，表示训练误差也比较小了，但是如果直接丢掉这部分数据会改变数据分布，影响准确度，因此提出 GOSS</li><li>GOSS 其实就是对要分裂的特征的所有取值按照梯度绝对值排序，选择绝对值最大的 $a<em>len(I)<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex" xmlns="http://www.w3.org/2000/svg" width="28.507ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 12600 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">个</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">，</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">然</text><text data-variant="normal" transform="translate(2700, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">后</text><text data-variant="normal" transform="translate(3600, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">在</text><text data-variant="normal" transform="translate(4500, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">剩</text><text data-variant="normal" transform="translate(5400, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">下</text><text data-variant="normal" transform="translate(6300, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(7200, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">里</text><text data-variant="normal" transform="translate(8100, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">面</text><text data-variant="normal" transform="translate(9000, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">随</text><text data-variant="normal" transform="translate(9900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">机</text><text data-variant="normal" transform="translate(10800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">选</text><text data-variant="normal" transform="translate(11700, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">择</text></g></g></g></svg></mjx-container>b</em>len(I)<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex" xmlns="http://www.w3.org/2000/svg" width="22.398ex" height="2.149ex" role="img" focusable="false" viewBox="0 -750 9900 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><text data-variant="normal" transform="matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">个</text><text data-variant="normal" transform="translate(900, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">，</text><text data-variant="normal" transform="translate(1800, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">对</text><text data-variant="normal" transform="translate(2700, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">随</text><text data-variant="normal" transform="translate(3600, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">机</text><text data-variant="normal" transform="translate(4500, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">选</text><text data-variant="normal" transform="translate(5400, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">的</text><text data-variant="normal" transform="translate(6300, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">数</text><text data-variant="normal" transform="translate(7200, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">据</text><text data-variant="normal" transform="translate(8100, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">乘</text><text data-variant="normal" transform="translate(9000, 0) matrix(1 0 0 -1 0 0)" font-size="884px" font-family="serif">以</text></g></g></g></svg></mjx-container>\frac{1-a}{b}$，这样的好处是更关注训练不足的样本，而不会过多改变原数据集的分布</li></ol></li><li><strong>互斥特征捆绑算法</strong>(Exclusive Feature Bundling, EFB)：<ol><li>通常被捆绑的特征都是互斥的（即特征不会同时为非零值，像 one-hot），这样两个特征捆绑起来才不会丢失信息。</li><li>如果两个特征并不是完全互斥（部分情况下两个特征都是非零值），可以用一个指标对特征不互斥程度进行衡量，称之为<strong>冲突比率</strong>，当这个值较小时，我们可以选择把不完全互斥的两个特征捆绑，而不影响最后的精度。</li><li>优点：降低特征数量</li><li>把哪些捆绑：<ol><li>构造一个加权无向图，顶点是特征，边有权重，其权重与两个特征间冲突相关；</li><li>根据节点的度进行降序排序，度越大，与其它特征的冲突越大；</li><li>遍历每个特征，将它分配给现有特征包，或者新建一个特征包，使得总体冲突最小</li></ol></li><li>特征如何合并：<ol><li>特征合并算法，其关键在于原始特征能从合并的特征中分离出来。</li><li>考虑到 histogram-based 算法将连续的值保存为离散的 bins，我们可以使得不同特征的值分到 bundle 中的不同 bin（箱子）中，这可以通过在特征值中加一个偏置常量来解决。</li><li>不同特征有对应的偏置，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="23.978ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 10598.2 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(361, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(846, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(1207, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1736, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2034, 0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(2793, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3138, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3738, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(4793.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(5849.6, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(6399.6, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(6844.2, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(7444.2, 0)"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8016.2, 0)"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(8894.2, 0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(9653.2, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(9998.2, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="28.859ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 12755.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mi" transform="translate(429, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(774, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1374, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(2133, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2662, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3262, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(3739, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(4205, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(4674, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(5118.7, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(5647.7, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(6150.7, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(6653.7, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(7119.7, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(7719.7, 0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(8239.7, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8628.7, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(8989.7, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(9474.7, 0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(9835.7, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(10364.7, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(10662.7, 0)"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(11421.7, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(11766.7, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(12366.7, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>，$binRanges 里面的就是每个特征的偏置。这样的结果就是不同特征的范围就不一样了。</li></ol></li></ol></li><li><strong>直接支持类别特征</strong>：<ol><li>针对 one-hot 编码存在的问题：<ol><li>产生样本切分不平衡的问题，导致切分增益很小，和不切分没什么区别。一系列特征上只有少量样本为 1，大量样本为 0，这时候切分样本会产生不平衡，这意味着切分增益也会很小。较小的那个切分样本集，它占总样本的比例太小，无论增益多大，乘以该比例之后几乎可以忽略；较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。</li><li>会影响决策树的学习。因为就算可以对这个类别特征进行切分，独热编码也会把数据切分到很多零散的小空间上，在这些数据量小的空间上，统计信息不准确，学习效果会变差</li></ol></li><li>LGBM 优化了对类别特征的支持，可以直接输入类别特征，采用 many-vs-many 的切分方式，从 one-hot 的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="5.691ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2515.3 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msup" transform="translate(1152, 0)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(500, 363) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(2126.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>降到了 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.052ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4001 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1752, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2050, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(2535, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(3012, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3612, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>算法流程：<ol><li>在枚举分割点之前，先把直方图按照每个类别对应的 label 均值进行排序；然后按照排序的结果依次枚举最优分割点</li><li>这个方法容易过拟合，LGBM 增加了很多对这个方法的约束和正则化</li></ol></li></ol></li><li><strong>支持高效并行：</strong><ol><li>特征并行：<ol><li>XGBoost 使用特征并行，每个机器在不同的特征集合找最优分割点，然后同步最优分割点。这个缺点是每个机器数据不同，划分结果需要在机器间通信，增加了额外复杂度。</li><li>LGBM 也是用特征并行，不进行数据垂直划分，在每台机器上保存所有数据，机器间传输最优分裂特征和分裂点，来得到全局最优分裂信息，（不需要广播分裂结果），然后直接在本地执行划分。</li></ol></li><li>数据并行：<ol><li>传统的数据并行策略为水平划分数据，不同机器在本地构造直方图然后全局合并，这样的划分有很大的缺点是通信开销太大。</li><li>LGBM 数据并行中使用分散规约 (Reduce scatter) 把直方图合并的任务分摊到不同的机器，降低通信和计算，并利用直方图做差，进一步减少了一半的通信量。</li></ol></li><li>投票并行：<ol><li>基于投票的数据并行进一步优化了数据并行中的通信代价</li><li>数据量很大的时候，投票并行只合并部分特征的直方图，从而达到降低通信量的目的</li><li>分两步：1. 在本地找出 Top K 的特征，基于投片筛选出可能是最优分割点的特征，合并时只合并每个机器选出来的特征</li></ol></li><li>Cache 命中率优化：<ol><li>相对 XGBoost：特征对梯度访问是随机访问，不同特征访问的顺序不同，无法对 cache 优化，且每层长树的时候，要随机访问一个行索引到叶子索引的数组，也会造成很大的 cache miss</li><li>LGBM 所有的特征都采用相同的方式获得梯度，只需要对梯度进行排序并可实现连续访问，大大提高了缓存命中率。</li><li>其次，因为不需要存储行索引到叶子索引的数组，降低了存储消耗，而且也不存在 Cache Miss 的问题。</li></ol></li></ol></li><li>相对 XGBoost 优点：<ol><li>速度更快<ol><li>遍历样本-&gt;遍历直方图</li><li>单边梯度采样-&gt;过滤梯度小的样本</li><li>Level-wise -&gt; Leaf-wise，减少不必要计算</li><li>优化后的特征并行、数据并行，以及投票并行</li><li>缓存优化</li></ol></li><li>内存更小<ol><li>相比 XGBoost 不用存储特征值和样本统计值的索引，使用 bin 值，内存变为原本的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.816ex" xmlns="http://www.w3.org/2000/svg" width="1.795ex" height="2.773ex" role="img" focusable="false" viewBox="0 -864.9 793.6 1225.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(220, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mn" transform="translate(220, -345) scale(0.707)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g><rect width="553.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>存储特征值变为存储 bin 值</li><li>互斥特征捆绑减少了特征数量</li></ol></li></ol></li><li>缺点：<ol><li>可能会长出比较深的决策树，过拟合，因此要增加一个最大深度限制</li><li>LGBM 对噪声比较敏感</li><li>寻找最优解的时候，依据的是最优切分变量，没有把最优解是全部特征考虑进去</li></ol></li></ol><h2 id="模型评价指标"><a href="#模型评价指标" class="headerlink" title="模型评价指标"></a>模型评价指标</h2><h3 id="分类指标"><a href="#分类指标" class="headerlink" title="分类指标"></a>分类指标</h3><ol><li><a href="https://www.zhihu.com/question/30643044/answer/510317055" target="_blank" rel="external nofollow noopener noreferrer">参考链接</a></li><li>混淆矩阵<ol><li>TP=11(真实-预测），FP=01，FN=10，TN=00</li></ol></li><li>准确率：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex" xmlns="http://www.w3.org/2000/svg" width="15.033ex" height="3.167ex" role="img" focusable="false" viewBox="0 -1047.1 6644.4 1399.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1183, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mo" transform="translate(1893.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2949.6, 0)"><g data-mml-node="mrow" transform="translate(220, 516.8) scale(0.707)"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(389, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(1093, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1844, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2622, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(3326, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(4214, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mrow" transform="translate(1449.7, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(529, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(827, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g><rect width="3454.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>误差率：<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex" xmlns="http://www.w3.org/2000/svg" width="13.665ex" height="1.805ex" role="img" focusable="false" viewBox="0 -716 6040 798"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(466, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(917, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1645.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(2701.6, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3423.8, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(4424, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(5174, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(5607, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g></g></g></svg></mjx-container></li><li>召回率/查全率：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="15.803ex" height="2.896ex" role="img" focusable="false" viewBox="0 -877 6985.1 1279.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(917, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1350, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1879, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2177, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2752.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3808.6, 0)"><g data-mml-node="mrow" transform="translate(1073.8, 394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1455, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2233, 0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(2982, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><rect width="2936.5" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>针对原始样本而言，召回率越高表示目标被预测出的概率越高</li></ol></li><li>精准率/查准率：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="19.254ex" height="2.896ex" role="img" focusable="false" viewBox="0 -877 8510.2 1279.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(954, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1420, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1853, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(2198, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(2667, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(3012, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(3497, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4374.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(5430.6, 0)"><g data-mml-node="mrow" transform="translate(1025.4, 394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1455, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2233, 0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(2982, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g><rect width="2839.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>针对预测结果而言，在预测结果中，有多少把握可以预测正确。</li></ol></li><li>PR 曲线：<ol><li>纵坐标是 Precision，横坐标是 Recall。</li><li>拿逻辑回归来说，它输出的是 0-1 之间的数，如果要用这个判断正负样本，就需要确定一个阈值，然后这个阈值的前提下可以得到一个查准率和查全率。</li><li>要想知道这个阈值是否符合我们要求，就需要遍历 0-1 之间所有阈值，然后就可以得到这个曲线。</li><li>想要平衡 P 和 R，就需要用 F1 分数。</li><li>同样也是 AUC 越大越好，也可以在能接受的 Precision 下选比较大的 Recall，把更多的正样本分类出来</li></ol></li><li>F1 分数：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="20.641ex" height="2.913ex" role="img" focusable="false" viewBox="0 -884.7 9123.5 1287.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mn" transform="translate(749, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1526.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2582.6, 0)"><g data-mml-node="mrow" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(1000, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(1759, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2225, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(2658, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(3187, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(3485, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(3783, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(4283, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(5034, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5485, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(5951, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(6384, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6729, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(7198, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(7543, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(8028, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mrow" transform="translate(475.3, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(759, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1225, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(1658, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(2187, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2485, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(2783, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(3561, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(4312, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(4763, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(5229, 0)"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mi" transform="translate(5662, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6007, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mi" transform="translate(6476, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6821, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(7306, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><rect width="6300.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>是为了在查准和查全之间找到一个平衡点，让二者都比较高。</li><li>作用相当于 AUC 对 ROC 曲线的作用，一个指标比一条曲线更方便调模型</li></ol></li><li>真阳率：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="15.213ex" height="2.896ex" role="img" focusable="false" viewBox="0 -877 6724.1 1279.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(1455, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(2491.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3547.6, 0)"><g data-mml-node="mrow" transform="translate(1073.8, 394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(704, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1455, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2233, 0)"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(2982, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><rect width="2936.5" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>针对原始真样本来说，预测对的概率，等价于查全率</li></ol></li><li>假阳率：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.912ex" xmlns="http://www.w3.org/2000/svg" width="15.315ex" height="2.896ex" role="img" focusable="false" viewBox="0 -877 6769.1 1279.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(749, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(1500, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(2536.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(3592.6, 0)"><g data-mml-node="mrow" transform="translate(1057.9, 394) scale(0.707)"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(749, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D439" d="M48 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H742Q749 676 749 669Q749 664 736 557T722 447Q720 440 702 440H690Q683 445 683 453Q683 454 686 477T689 530Q689 560 682 579T663 610T626 626T575 633T503 634H480Q398 633 393 631Q388 629 386 623Q385 622 352 492L320 363H375Q378 363 398 363T426 364T448 367T472 374T489 386Q502 398 511 419T524 457T529 475Q532 480 548 480H560Q567 475 567 470Q567 467 536 339T502 207Q500 200 482 200H470Q463 206 463 212Q463 215 468 234T473 274Q473 303 453 310T364 317H309L277 190Q245 66 245 60Q245 46 334 46H359Q365 40 365 39T363 19Q359 6 353 0H336Q295 2 185 2Q120 2 86 2T48 1Z"></path></g><g data-mml-node="mi" transform="translate(749, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1500, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(2278, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(2982, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><rect width="2936.5" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li>针对原始假样本来说，预测错的概率</li></ol></li><li>TPR=FPR：<ol><li>若二者相等，意味着无论一个样本本身是正例还是负例，分类器预测其为正例的概率是一样的，这等同于随机猜测</li></ol></li><li>ROC:<ol><li>用 TPR 和 FPR 分别在实际的正样本和负样本中观察相关概率问题，因此无论样本是否平衡都不会被影响。所以采用 TPR 和 FPR 作为 ROC、AUC 的指标</li><li>ROC 曲线横坐标是 FPR，纵坐标是 TPR</li><li>和 PR 曲线一样，也是遍历所有阈值</li><li>如何判断 ROC 曲线好坏？纵坐标 TPR 越高，横坐标 FPR 越低，即曲线越陡峭，性能越好</li></ol></li><li>AUC：<ol><li>表示 ROC 曲线下面积</li><li>如果直接通过不同阈值多次评估模型的话效率很低，因此用 AUC 来提供这类信息。</li><li>对角线下面的面积是 0.5，表示随机判断相应和不响应，正负样本覆盖率都是 50%，表示随机效果。所以理想值 AUC=1，最差的随机判断为 0.5。</li></ol></li></ol><h3 id="搜索问题指标"><a href="#搜索问题指标" class="headerlink" title="搜索问题指标"></a>搜索问题指标</h3><p>上面的都没有考虑到检索结果的先后顺序，而像搜索问题中我们通常希望第一个结果是与查询最相关的，第二个则是次相关的，以此类推，因而有时候不仅要预测准确，对于相关性的顺序也非常看重。</p><ol><li>MAP（Mean Average Precision）平均准确率均值：<ol><li>AP: 对于单个信息需求，返回结果中在每篇相关文档上 Precision 的平均值被称为 Average Precision (AP)，然后对所有查询取平均得到 MAP。</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="20.026ex" height="3.362ex" role="img" focusable="false" viewBox="0 -1141.1 8851.3 1486.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(750, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(1778.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2834.6, 0)"><g data-mml-node="mrow" transform="translate(220, 582.8) scale(0.707)"><g data-mml-node="munderover"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(521, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1299, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(2544.8, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(3295.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3684.8, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(4205.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(4594.8, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(5372.8, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(5823.8, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(6289.8, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mo" transform="translate(6587.8, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6976.8, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(7497.8, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><g data-mml-node="mi" transform="translate(2636.8, -345) scale(0.707)"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><rect width="5776.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><ol><li>P(k)是取前 k 个结果算出来的 Precision，也可写成 P@k</li><li>rel(k)表示第 𝑘 个结果是否为相关文档，相关为 1 不相关为 0。</li><li>M 表示相关文档的数量</li><li>n 表示所有文档数量</li><li>所以分子上，对一个有序的表，若该位置返回的结果相关，计算截止到该位置的正确率，若不相关，正确率置为 0</li><li>计算方法是对排序位置敏感的，相关文档排序的位置越靠前，检出的相关文档越多，AP 值越大。</li></ol></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.091ex" xmlns="http://www.w3.org/2000/svg" width="18.273ex" height="3.465ex" role="img" focusable="false" viewBox="0 -1049.4 8076.8 1531.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D440" d="M289 629Q289 635 232 637Q208 637 201 638T194 648Q194 649 196 659Q197 662 198 666T199 671T201 676T203 679T207 681T212 683T220 683T232 684Q238 684 262 684T307 683Q386 683 398 683T414 678Q415 674 451 396L487 117L510 154Q534 190 574 254T662 394Q837 673 839 675Q840 676 842 678T846 681L852 683H948Q965 683 988 683T1017 684Q1051 684 1051 673Q1051 668 1048 656T1045 643Q1041 637 1008 637Q968 636 957 634T939 623Q936 618 867 340T797 59Q797 55 798 54T805 50T822 48T855 46H886Q892 37 892 35Q892 19 885 5Q880 0 869 0Q864 0 828 1T736 2Q675 2 644 2T609 1Q592 1 592 11Q592 13 594 25Q598 41 602 43T625 46Q652 46 685 49Q699 52 704 61Q706 65 742 207T813 490T848 631L654 322Q458 10 453 5Q451 4 449 3Q444 0 433 0Q418 0 415 7Q413 11 374 317L335 624L267 354Q200 88 200 79Q206 46 272 46H282Q288 41 289 37T286 19Q282 3 278 1Q274 0 267 0Q265 0 255 0T221 1T157 2Q127 2 95 1T58 0Q43 0 39 2T35 11Q35 13 38 25T43 40Q45 46 65 46Q135 46 154 86Q158 92 223 354T289 629Z"></path></g><g data-mml-node="mi" transform="translate(1051, 0)"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(1801, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(2829.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(3885.6, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 490.8) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mo" transform="translate(460, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1238, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mfrac" transform="translate(6387.2, 0)"><g data-mml-node="mrow" transform="translate(220, 543.1) scale(0.707)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="msub" transform="translate(750, 0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(642, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g><g data-mml-node="mi" transform="translate(565.2, -345) scale(0.707)"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><rect width="1449.6" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><ol><li>Q 是查询次数</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.65ex" xmlns="http://www.w3.org/2000/svg" width="3.998ex" height="2.27ex" role="img" focusable="false" viewBox="0 -716 1767.3 1003.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="msub" transform="translate(750, 0)"><g data-mml-node="mi"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mi" transform="translate(642, -150) scale(0.707)"><path data-c="1D45E" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>是第 q 次查询算出来的 AP 值</li><li>也可以只关注前 k 个结果，把 n 换成 k 就行</li><li>对于单个信息需求来说，Average Precision 是 PR 曲线下面积的近似值，因此 MAP 可粗略地认为是某个查询集合对应的多条 PR 曲线下面积的平均值。</li></ol></li></ol></li><li>NDCG(Normalized Discounted Cumulative Gain)<ol><li>归一化折扣累计增益：<ol><li>N 指的是归一化，</li><li>D 指的是衰减率，</li><li>C 指的累加，</li><li>G 指的是熵，</li><li>其实这个公式的关键就是就是熵，再多了三个形容词：归一化的，带有衰减函数的，再带有累加的熵。</li><li>如果说 MAP 是基于 0/1 二值描述相关性，那么 NDCG 则是可将相关性分为多个等级的指标。</li></ol></li><li>两个思想：<ol><li><strong>高关联度的结果</strong>比一般关联度的结果更影响最终的指标得分；</li><li>有高关联度的结果出现在<strong>更靠前的位置</strong>的时候，指标会越高；</li></ol></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.777ex" xmlns="http://www.w3.org/2000/svg" width="16.351ex" height="2.966ex" role="img" focusable="false" viewBox="0 -967.8 7227.2 1311.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(760, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(2242.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(3298, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mi" transform="translate(5718.3, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(6169.3, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msub" transform="translate(6635.3, 0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container><ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="3.414ex" height="1.927ex" role="img" focusable="false" viewBox="0 -694 1509 851.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msub" transform="translate(917, 0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>表示第 i 这个位置上的相关度</li><li>k 表示前 k 个结果</li><li>CG 没有考虑推荐的次序，所以在此基础上引入对结果顺序的考虑，即相关性高的结果若排在后面则会受更多的惩罚，即 DCG</li></ol></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="22.308ex" height="3.504ex" role="img" focusable="false" viewBox="0 -1001.7 9860.3 1548.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828, 0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(1588, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(3070.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(4126, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="mi" transform="translate(1056, 477.1) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, -285.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mfrac" transform="translate(6546.3, 0)"><g data-mml-node="mrow" transform="translate(608.2, 398) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msub" transform="translate(917, 0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, -307.4)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(1688.4, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(2466.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(783, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mn" transform="translate(477, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1663.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2052.6, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2397.6, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3175.6, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3675.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3074.1" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><ol><li>如果该结果 rel 很高，但排在后面，意味着分母 log2(i+1) 会变大，则相应的总体 DCG 会变小 (注意这里的 log 是以 2 为底的)</li><li>不同的查询，往往会返回不同的结果集，而不同结果集之间因为大小不同难以直接用 DCG 进行比较，所以需要进行归一化</li></ol></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.033ex" xmlns="http://www.w3.org/2000/svg" width="17.613ex" height="3.182ex" role="img" focusable="false" viewBox="0 -950.1 7784.9 1406.6"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mi" transform="translate(888, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(1716, 0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(2476, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(3958.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(5014, 0)"><g data-mml-node="mrow" transform="translate(398.2, 451.6) scale(0.707)"><g data-mml-node="mi"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(828, 0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(1588, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mrow" transform="translate(220, -345) scale(0.707)"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(1332, 0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(2092, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><rect width="2530.9" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container><ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.238ex" xmlns="http://www.w3.org/2000/svg" width="25.268ex" height="3.636ex" role="img" focusable="false" viewBox="0 -1060 11168.3 1607.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43C" d="M43 1Q26 1 26 10Q26 12 29 24Q34 43 39 45Q42 46 54 46H60Q120 46 136 53Q137 53 138 54Q143 56 149 77T198 273Q210 318 216 344Q286 624 286 626Q284 630 284 631Q274 637 213 637H193Q184 643 189 662Q193 677 195 680T209 683H213Q285 681 359 681Q481 681 487 683H497Q504 676 504 672T501 655T494 639Q491 637 471 637Q440 637 407 634Q393 631 388 623Q381 609 337 432Q326 385 315 341Q245 65 245 59Q245 52 255 50T307 46H339Q345 38 345 37T342 19Q338 6 332 0H316Q279 2 179 2Q143 2 113 2T65 2T43 1Z"></path></g><g data-mml-node="mi" transform="translate(504, 0)"><path data-c="1D437" d="M287 628Q287 635 230 637Q207 637 200 638T193 647Q193 655 197 667T204 682Q206 683 403 683Q570 682 590 682T630 676Q702 659 752 597T803 431Q803 275 696 151T444 3L430 1L236 0H125H72Q48 0 41 2T33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM703 469Q703 507 692 537T666 584T629 613T590 629T555 636Q553 636 541 636T512 636T479 637H436Q392 637 386 627Q384 623 313 339T242 52Q242 48 253 48T330 47Q335 47 349 47T373 46Q499 46 581 128Q617 164 640 212T683 339T703 469Z"></path></g><g data-mml-node="mi" transform="translate(1332, 0)"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="msub" transform="translate(2092, 0)"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, -150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(3574.2, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="munderover" transform="translate(4630, 0)"><g data-mml-node="mo"><path data-c="2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></g><g data-mml-node="TeXAtom" transform="translate(1056, 530) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(1037, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(1801, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(2482, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g><g data-mml-node="TeXAtom" transform="translate(1056, -297.3) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mfrac" transform="translate(7854.2, 0)"><g data-mml-node="mrow" transform="translate(608.2, 398) scale(0.707)"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(451, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="msub" transform="translate(917, 0)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, -307.4)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g><g data-mml-node="mo" transform="translate(1688.4, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(2466.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mrow" transform="translate(220, -370.3) scale(0.707)"><g data-mml-node="mi"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(298, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(783, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mn" transform="translate(477, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(1663.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2052.6, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(2397.6, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(3175.6, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3675.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g><rect width="3074.1" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>，为理想情况下最大的 DCG 值，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.564ex" xmlns="http://www.w3.org/2000/svg" width="6.244ex" height="2.26ex" role="img" focusable="false" viewBox="0 -749.5 2760 999"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(278, 0)"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mi" transform="translate(1037, 0)"><path data-c="1D438" d="M492 213Q472 213 472 226Q472 230 477 250T482 285Q482 316 461 323T364 330H312Q311 328 277 192T243 52Q243 48 254 48T334 46Q428 46 458 48T518 61Q567 77 599 117T670 248Q680 270 683 272Q690 274 698 274Q718 274 718 261Q613 7 608 2Q605 0 322 0H133Q31 0 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q146 66 215 342T285 622Q285 629 281 629Q273 632 228 634H197Q191 640 191 642T193 659Q197 676 203 680H757Q764 676 764 669Q764 664 751 557T737 447Q735 440 717 440H705Q698 445 698 453L701 476Q704 500 704 528Q704 558 697 578T678 609T643 625T596 632T532 634H485Q397 633 392 631Q388 629 386 622Q385 619 355 499T324 377Q347 376 372 376H398Q464 376 489 391T534 472Q538 488 540 490T557 493Q562 493 565 493T570 492T572 491T574 487T577 483L544 351Q511 218 508 216Q505 213 492 213Z"></path></g><g data-mml-node="mi" transform="translate(1801, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(2482, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g></svg></mjx-container> 表示按相关性顺序排列的结果集，取前 p 个结果组成的集合</li><li>IDCG 是 DCG 能取到的最大值，因此 NDCG 范围为(0,1]</li><li>缺点是需要预先指定每一个返回结果的相关性，这个超参数需要人为指定</li><li><a href="https://www.cnblogs.com/by-dream/p/9403984.html" target="_blank" rel="external nofollow noopener noreferrer">参考链接</a></li></ol></li></ol></li></ol><h2 id="采样方法"><a href="#采样方法" class="headerlink" title="采样方法"></a>采样方法</h2><h3 id="过采样"><a href="#过采样" class="headerlink" title="过采样"></a>过采样</h3><h4 id="随机采样"><a href="#随机采样" class="headerlink" title="随机采样"></a>随机采样</h4><ol><li>从样本少的类别中随机抽样，再将抽样得来的样本添加到数据集中</li><li>已经不大使用了，因为重复采样往往会导致严重的过拟合，因而现在的主流过采样方法是通过某种方式人工合成一些少数类样本，从而达到类别平衡的目的，而这其中的鼻祖就是 SMOTE。</li></ol><h4 id="SMOTE-synthetic-minority-oversampling-technique"><a href="#SMOTE-synthetic-minority-oversampling-technique" class="headerlink" title="SMOTE(synthetic minority oversampling technique)"></a>SMOTE(synthetic minority oversampling technique)</h4><ol><li>概括起来就是在少数类样本之间进行插值来产生额外的样本。</li><li>步骤 1：对一个少数类样本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，用 K 近邻法求出离它最近的 k 个少数类样本</li><li>步骤 2：从 k 个近邻点中随机选取一个，用它生成一个新样本： <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="23.549ex" height="2.296ex" role="img" focusable="false" viewBox="0 -765 10408.9 1015"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(600, 0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(1066, 0)"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"></path></g></g></g><g data-mml-node="mo" transform="translate(2159.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3215.6, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4303.8, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mo" transform="translate(5304, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(5693, 0)"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g><g data-mml-node="mo" transform="translate(6487.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(7487.5, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(8353.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8964.6, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(9964.9, 0)"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g></g></g></svg></mjx-container>，其中 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.756ex" role="img" focusable="false" viewBox="0 -765 572 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>是随机选出的样本，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex" xmlns="http://www.w3.org/2000/svg" width="1.005ex" height="1.645ex" role="img" focusable="false" viewBox="0 -717 444 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FF" d="M195 609Q195 656 227 686T302 717Q319 716 351 709T407 697T433 690Q451 682 451 662Q451 644 438 628T403 612Q382 612 348 641T288 671T249 657T235 628Q235 584 334 463Q401 379 401 292Q401 169 340 80T205 -10H198Q127 -10 83 36T36 153Q36 286 151 382Q191 413 252 434Q252 435 245 449T230 481T214 521T201 566T195 609ZM112 130Q112 83 136 55T204 27Q233 27 256 51T291 111T309 178T316 232Q316 267 309 298T295 344T269 400L259 396Q215 381 183 342T137 256T118 179T112 130Z"></path></g></g></g></svg></mjx-container>是 0-1 之间的数，因此新样本在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>和 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.756ex" role="img" focusable="false" viewBox="0 -765 572 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>的连线上</li><li>缺陷（没有考虑周边样本）：<ol><li>如果选取的少数类样本周围也都是少数类样本，则新合成的样本不会提供太多有用信息。这就像支持向量机中远离 margin 的点对决策边界影响不大。</li><li>如果选取的少数类样本周围都是多数类样本，这类的样本可能是噪音，则新合成的样本会与周围的多数类样本产生大部分重叠，致使分类困难。</li></ol></li></ol><h4 id="Border-line-SMOTE"><a href="#Border-line-SMOTE" class="headerlink" title="Border-line SMOTE"></a>Border-line SMOTE</h4><ol><li>将所有的少数类样本分成三类：<ol><li>“noise” ： 所有的 k 近邻个样本都属于多数类</li><li>“danger” ： 超过一半的 k 近邻样本属于多数类</li><li>“safe”： 超过一半的 k 近邻样本属于少数类</li></ol></li><li>算法只会从处于”danger“状态的样本中随机选择样本使用 SMOTE，处于”danger“状态的样本更靠近”边界“，往往更容易被误分类。</li><li>和传统 SMOTE 区别是，它只用边界附近的少数类样本进行人工合成，而 SMOTE 不区分是不是边界。</li><li>有两种模式：1. 公式中的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.756ex" role="img" focusable="false" viewBox="0 -765 572 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(63.8, -29)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path></g></g></g></g></g></svg></mjx-container>是一个少数类样本，或者，2. 是 k 近邻中的任意一个样本</li></ol><h4 id="自适应合成抽样-ADASYN-adaptive-synthetic-sampling"><a href="#自适应合成抽样-ADASYN-adaptive-synthetic-sampling" class="headerlink" title="自适应合成抽样 ADASYN(adaptive synthetic sampling)"></a>自适应合成抽样 ADASYN(adaptive synthetic sampling)</h4><ol><li>最大的特点是采用某种机制自动决定每个少数类样本需要产生多少合成样本，而不是像 SMOTE 那样对每个少数类样本合成同数量的样本</li><li>计算需要合成的样本总量：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="21.565ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 9531.7 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mo" transform="translate(1063.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(2119.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(2508.6, 0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(613, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1407, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4680, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(5680.2, 0)"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(613, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1223, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(7632.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(8243.5, 0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="mi" transform="translate(8965.7, 0)"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="4.41ex" height="2.261ex" role="img" focusable="false" viewBox="0 -705 1949.2 999.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(613, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(1407, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container>为多数类样本数量</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="4.416ex" height="1.952ex" role="img" focusable="false" viewBox="0 -705 1952.1 862.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="TeXAtom" transform="translate(613, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(1223, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></g></svg></mjx-container>为少数类样本数</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.573ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3789.2 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D6FD" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path></g><g data-mml-node="mo" transform="translate(843.8, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="mo" transform="translate(1788.6, 0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="mn" transform="translate(2066.6, 0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(2566.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(3011.2, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3511.2, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container></li><li>G 即为总共想要合成的少数类样本数量</li></ol></li><li>每个少类别样本，找出其 K 近邻个点，计算：<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="10.024ex" height="3.15ex" role="img" focusable="false" viewBox="0 -1047.1 4430.6 1392.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(584, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1155.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(2211.5, 0)"><g data-mml-node="mrow" transform="translate(220, 516.8) scale(0.707)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path></g><g data-mml-node="mi" transform="translate(833, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1127, 0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(1627, 0)"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g></g><g data-mml-node="mi" transform="translate(853.9, -345) scale(0.707)"><path data-c="1D44D" d="M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"></path></g><rect width="1979" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container></li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="2.55ex" height="1.977ex" role="img" focusable="false" viewBox="0 -716 1127 873.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="394" d="M51 0Q46 4 46 7Q46 9 215 357T388 709Q391 716 416 716Q439 716 444 709Q447 705 616 357T786 7Q786 4 781 0H51ZM507 344L384 596L137 92L383 91H630Q630 93 507 344Z"></path></g><g data-mml-node="mi" transform="translate(833, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 为 K 近邻个点中多数类样本的数量</li><li>Z 为规范化因子以确保 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.889ex" role="img" focusable="false" viewBox="0 -677 878 834.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(584, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 构成一个分布</li><li>这样若一个少数类样本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>的周围多数类样本越多，则其 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.986ex" height="1.889ex" role="img" focusable="false" viewBox="0 -677 878 834.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(584, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 也就越高。</li></ol></li><li>对每个少类别样本 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.959ex" height="1.357ex" role="img" focusable="false" viewBox="0 -442 866 599.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>计算需要合成的样本数量，再用 SMOTE 算法合成新样本<ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="11.292ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 4990.9 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mi" transform="translate(477, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1048.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(2104.5, 0)"><g data-mml-node="mi"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mi" transform="translate(584, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3204.7, 0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mi" transform="translate(4204.9, 0)"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g></g></g></svg></mjx-container></li></ol></li><li>缺点是：易受离群点的影响，如果一个少数类样本的 K 近邻都是多数类样本，则其权重会变得相当大，进而会在其周围生成较多的样本</li></ol><h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><ol><li><code>SMOTE</code> 合成的样本分布比较平均，</li><li><code>Border-line SMOTE</code>合成的样本则集中在类别边界处。</li><li><code>ADASYN</code>的特性是一个少数类样本周围多数类样本越多，则算法会为其生成越多的样本，生成的样本大都来自于原来与多数类比较靠近的那些少数类样本</li></ol><h3 id="欠采样"><a href="#欠采样" class="headerlink" title="欠采样"></a>欠采样</h3><h4 id="随机欠采样"><a href="#随机欠采样" class="headerlink" title="随机欠采样"></a>随机欠采样</h4><ul><li>随机欠采样的思想同样比较简单，就是从多数类样本中随机选取一些剔除掉。</li><li>这种方法的缺点是被剔除的样本可能包含着一些重要信息，致使学习出来的模型效果不好。</li></ul><h4 id="EasyEnsemble-和-BalanceCascade"><a href="#EasyEnsemble-和-BalanceCascade" class="headerlink" title="EasyEnsemble 和 BalanceCascade"></a>EasyEnsemble 和 BalanceCascade</h4><p>它们集成学习机制来处理传统随机欠采样中的信息丢失问题。</p><ul><li>EasyEnsemble：<ol><li>将多数类样本随机划分成 n 个子集，每个子集的数量等于少数类样本的数量，这相当于欠采样。</li><li>接着将每个子集与少数类样本结合起来分别训练一个模型，最后将 n 个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。</li></ol></li></ul><blockquote><p>如果说 EasyEnsemble 是基于无监督的方式从多数类样本中生成子集进行欠采样，那么 BalanceCascade 则是采用了有监督结合 Boosting 的方式</p></blockquote><ul><li>BalanceCascade:<ol><li>在第 n 轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器 H，训练完后多数类中能被<strong>H 正确分类</strong>的样本会被剔除。</li><li>在接下来的第 n+1 轮中，从<strong>被剔除后的</strong>多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。</li><li>BalanceCascade 的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其 Boosting 特点则体现在<strong>每一轮丢弃被正确分类的样本</strong>，进而后续基学习器会更注重那些之前分类错误的样本。​</li></ol></li></ul><h4 id="NearMiss"><a href="#NearMiss" class="headerlink" title="NearMiss"></a>NearMiss</h4><ol><li>NearMiss 本质上是一种原型选择(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。</li><li>NearMiss 采用一些启发式的规则来选择样本，根据规则的不同可分为 3 类，1 和 2 要计算 KNN：<ol><li>NearMiss-1：选择多数类样本中，到最近的 K 个少数类样本平均距离最近的</li><li>NearMiss-2：选择多数类样本中，到最远的 K 个少数类样本，平均距离最近的</li><li>NearMiss-3：对于每个少数类样本，选择 K 个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围</li></ol></li><li>NearMiss-1 和 NearMiss-2 的计算开销很大，因为需要计算每个多类别样本的 K 近邻点。另外，NearMiss-1 易受离群点的影响，处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下 NearMiss-2 和 NearMiss-3 不易产生这方面的问题。</li></ol><h4 id="数据清洗方法"><a href="#数据清洗方法" class="headerlink" title="数据清洗方法"></a>数据清洗方法</h4><p>主要通过某种规则来清洗重叠的数据，从而达到欠采样的目的，而这些规则往往也是启发性的</p><ol><li>Tomek Link：<ol><li>Tomek Link 表示<strong>不同类别之间距离最近</strong>的一对样本，即这两个样本互为最近邻且分属不同类别。</li><li>这样如果两个样本形成了一个 Tomek Link，则要么其中一个是噪音，要么两个样本都在边界附近，都可以去掉。</li><li>这样通过移除 Tomek Link 就能“清洗掉”类间重叠样本，使得互为最近邻的样本皆属于同一类别，从而能更好地进行分类。</li></ol></li><li>Edited Nearest Neighbours(ENN)：<ol><li>对于属于<strong>多数类</strong>的一个样本，如果其 K 个近邻点有超过一半都<strong>不属于</strong>多数类，则这个样本会被剔除。</li><li>这个方法的另一个变种是<strong>所有的 K 个近邻点</strong>都不属于多数类，则这个样本会被剔除。</li></ol></li></ol><blockquote><p>数据清洗技术最大的缺点是无法控制欠采样的数量。由于都在某种程度上采用了 K 近邻法，而事实上大部分多数类样本周围也都是多数类，因而能剔除的多数类样本比较有限。</p></blockquote><h2 id="面试总结"><a href="#面试总结" class="headerlink" title="面试总结"></a>面试总结</h2><h3 id="京东"><a href="#京东" class="headerlink" title="京东"></a>京东</h3><ul><li>京东是第一个面试，一面挂</li><li>存在问题：没有和 HR 沟通好时间，预期是下周面试，HR 确定时间在第二天也没有沟通调整。导致一面的时候还没有过完简历。</li></ul><h3 id="字节跳动"><a href="#字节跳动" class="headerlink" title="字节跳动"></a>字节跳动</h3><ul><li>一面和二面是一起的，所以整个的压力会比较大。如果全程都很紧张的话，会导致做算法题的时候头脑不清楚。可以在一面二面的间隔出去走走，放松下脑子。</li><li>字节跳动比较看重算法基础。可能和他们是偏向业务有关系，所以不怎么在意 NLP 理论 or 数学方面的知识。这意味着杂活可能会比较多。</li><li>字节跳动三面挂了，问题是在算法题上。是一道有把握的题，但是输在细节上。平时刷题要注意细节和易错点的整理，光刷题不积累下次遇到了还是会错。</li></ul><h4 id="字节一面题"><a href="#字节一面题" class="headerlink" title="字节一面题"></a>字节一面题</h4><ul><li>在有序旋转数组中找到一个数<blockquote><p>有序数组 arr 可能经过一次旋转处理，也可能没有，且 arr 可能存在重复的数。例如，有序数组[1, 2, 3, 4, 5, 6, 7]，可以旋转处理成[4, 5, 6, 7, 1, 2, 3]等。给定一个可能旋转过的有序数组 arr，再给定一个数 num，返回 arr 中是否含有 num<br>关于旋转操作：可以简单的理解为把序列从某个位置切成两段然后交换位置<br>期望复杂度为 O(\log n)O(logn)<br>示例 1<br>输入<br>[4,5,6,7,1,2,3],7<br>输出<br>true</p></blockquote></li></ul><h4 id="字节二面算法题"><a href="#字节二面算法题" class="headerlink" title="字节二面算法题"></a>字节二面算法题</h4><ul><li>跳方格问题（类似 LeetCode 青蛙跳）<blockquote><p>N 个方格，每个方格上有加体力值的，也可能是减体力值的。给定初始体力值，每次可以跳任意步，问能否到达终点，到达终点时能剩余的最大体力值是多少。</p></blockquote></li></ul><h4 id="字节三面题"><a href="#字节三面题" class="headerlink" title="字节三面题"></a>字节三面题</h4><ul><li>建树和之字形遍历<blockquote><p>给一串数字，自己建树，然后打印出这个树之字形遍历的结果。</p></blockquote></li></ul><h4 id="第二次字节一面题"><a href="#第二次字节一面题" class="headerlink" title="第二次字节一面题"></a>第二次字节一面题</h4><ul><li>阶乘末尾为 0 的个数<blockquote><p>求大整数 N 的阶乘，末尾有多少个 0。<br>3! = 6, res = 0<br>5! = 120, res = 1</p></blockquote></li></ul><h3 id="快手"><a href="#快手" class="headerlink" title="快手"></a>快手</h3><ul><li>一面二面在一起，但是没有三面</li><li>整体难度比字节低一个 level，问的问题也不是很深，面试官也不会像字节那么严肃，会和你聊天，全程下来都不紧张。</li><li>还是那句话，所有命运的馈赠，都在暗中标好了价格。组越好那肯定面试越难进。</li></ul><h4 id="快手一面题"><a href="#快手一面题" class="headerlink" title="快手一面题"></a>快手一面题</h4><ol><li><p>数值的整数次方</p><blockquote><p>给定一个 double 类型的浮点数 base 和 int 类型的整数 exponent。求 base 的 exponent 次方。<br>没有限制底数和 exponent 的范围，所以都可能是负数。</p></blockquote></li><li><p>连续子数组的最大和</p><blockquote><p>HZ 偶尔会拿些专业问题来忽悠那些非计算机专业的同学。今天测试组开完会后,他又发话了:在古老的一维模式识别中,常常需要计算连续子向量的最大和,当向量全为正数的时候,问题很好解决。但是,如果向量中包含负数,是否应该包含某个负数,并期望旁边的正数会弥补它呢？例如:{6,-3,-2,7,-15,1,2,2},连续子向量的最大和为 8(从第 0 个开始,到第 3 个为止)。给一个数组，返回它的最大连续子序列的和，你会不会被他忽悠住？(子向量的长度至少是 1)</p></blockquote></li></ol><h4 id="快手二面题"><a href="#快手二面题" class="headerlink" title="快手二面题"></a>快手二面题</h4><ol><li><p>最长回文串</p><blockquote><p>对于一个字符串，请设计一个高效算法，计算其中最长回文子串的长度。给定字符串 A 以及它的长度 n，请返回最长回文子串的长度。<br>牛客/LeetCode 原题。直接中心扩散法，初始化中心两个位置，然后 dp。</p></blockquote></li><li><p>链式 A+B</p><blockquote><p>有两个用链表表示的整数，每个结点包含一个数位。这些数位是反向存放的，也就是个位排在链表的首部。编写函数对这两个整数求和，并用链表形式返回结果。<br>给定两个链表 ListNode* A，ListNode* B，请返回 A+B 的结果(ListNode*)。<br>测试样例：{1,2,3},{3,2,1}<br>返回：{4,4,4}</p></blockquote></li></ol><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="积累阶段"><a href="#积累阶段" class="headerlink" title="积累阶段"></a>积累阶段</h4><ol><li>自己打比赛的时候要注意选择和自己想做方向相关的比赛，不然帮助不大</li><li>多打比赛增加实战经验</li><li>对于一些算法不能停留表面，要理解底层的原理和数学知识，不然面试官会问你为什么，就答不出来</li><li>还是得看 paper，要有基本的科研素质，投 research 岗位的时候才会有人要。</li><li>招聘写的论文优先，主要目的是想让招进来的人知道怎么科研，少走弯路。所以自己如果想投 research，就算没论文，也得有基本的科研素质。多看论文，了解前沿动态。</li><li>综合多种信息之后，模型的能力会更好，不要光局限于文本序列信息，还有很多 Structured Data。</li><li>算法题还是要多练，做笔记整理，不然总是忘。以及一些 STL 的东西，很不熟练。刷完的题得积累，方便查阅。</li><li>算法不要光停留在理解思路上，还要有很强的把思路转换为code的能力</li></ol><h4 id="准备阶段"><a href="#准备阶段" class="headerlink" title="准备阶段"></a>准备阶段</h4><ol><li>自己的项目，一定要很熟悉，熟悉到特征维数、输入输出、shape 转换情况之类的。</li><li>面试的之前还是得自己问自己点问题，然后自己说出来，多练习，不然知道会问啥也懂但是说不清楚，这就很糟糕。</li><li>有时间的多准备准备，可以做个 PPT 讲自己的项目细节，不然你感觉自己讲的很清楚了，面试官还不清楚你说的是啥。</li><li>每个类型的题目最好都过一遍，每个结构的都要熟悉一遍。不然基础的东西反而可能会坑到自己。</li></ol><h4 id="面试阶段"><a href="#面试阶段" class="headerlink" title="面试阶段"></a>面试阶段</h4><ol><li>面试过程中语速不要太快，不要慌，分点说，一定要说的逻辑比较清楚，不然会让面试官觉得你这方面不熟。</li><li>面试官提问之后的思考过程不能太久，会显得反应迟钝，最好先从一个很清楚的点说起，然后边理思路边说。</li></ol><h4 id="面试的算法题"><a href="#面试的算法题" class="headerlink" title="面试的算法题"></a>面试的算法题</h4><ol><li>算法题要先审题，不要拿到题目就冲上去写。如果没思路可以试试最简单的 case。</li><li>算法题的过程中，如果卡住了，一定要和面试官交流，不要一直埋头 debug。</li><li>算法题不仅考察 coding 能力，还看你解决问题的能力，以及沟通交流能力。所以可以和面试官聊聊解题思路，讲讲自己的代码怎么写的，面试官一般也会给提示，千万不要啥都不说，这样显得沟通能力很差，毕竟工作中的交流和沟通很重要。</li></ol><h4 id="最后的问答"><a href="#最后的问答" class="headerlink" title="最后的问答"></a>最后的问答</h4><p>面试结束的时候，面试官会问有什么要问的，这时候可以问下面这些：</p><ol><li>感觉我的面试过程怎么样，对我的面试有什么建议？这可以对下一轮面试有帮助。因为面试官写面评的时候，会给下一轮面试的面试官建议考察的偏向。比如算法基础不行，就让下一轮重点考察算法。所以问面试官这个可以针对性地准备和心里准备。</li><li>可以聊实习的内容，具体的问细节，防止最后不清楚工作内容，导致工作不满意。</li></ol>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;近两周的实习总结。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="Private" scheme="https://hanielxx.com/tags/Private/"/>
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
    <category term="JobHunting" scheme="https://hanielxx.com/tags/JobHunting/"/>
    
    <category term="Interview" scheme="https://hanielxx.com/tags/Interview/"/>
    
    <category term="ByteDance" scheme="https://hanielxx.com/tags/ByteDance/"/>
    
    <category term="KuaiShou" scheme="https://hanielxx.com/tags/KuaiShou/"/>
    
    <category term="BaiDu" scheme="https://hanielxx.com/tags/BaiDu/"/>
    
  </entry>
  
  <entry>
    <title>BERT-预训练源码理解</title>
    <link href="https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis"/>
    <id>https://hanielxx.com/MachineLearning/2021-02-23-bert-create-pretrain-data-analysis</id>
    <published>2021-02-23T08:11:01.000Z</published>
    <updated>2021-05-17T15:34:23.000Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>BERT 作为一个里程碑式的预训练模型，很多时候我们都是直接用训练好的 model 直接 fine-tune，对它的理解只停留在 MLM 和 NSP 上。后续的很多 SOTA 模型都是在 BERT 的基础上发展来，比如 ALBERT、RoBERTa、XLNet 之类。</p><p>这里对 BERT 创建预训练数据的源码：<code>create_pretraining_data.py</code>和<code>run_pretrain.py</code>进行分析和理解。</p><p><a href="https://github.com/google-research/bert/tree/eedf5716ce1268e56f0a50264a88cafad334ac61" target="_blank" rel="external nofollow noopener noreferrer">当前 BERT 对应的 commit</a></p><p>部分参考<a href="https://carlos9310.github.io/2019/09/30/pre-trained-bert/" target="_blank" rel="external nofollow noopener noreferrer">预训练模型-BERT预训练源码解读笔记</a></p></div><a id="more"></a><h2 id="原始数据格式"><a href="#原始数据格式" class="headerlink" title="原始数据格式"></a>原始数据格式</h2><ol><li>每行一句话，每个文档中间用空格分开。</li><li>可以输入多个文件，也可以输出多个 tfrecord 文件</li><li>参考样例可以看 bert 中附带的<code>sample_text.txt</code></li></ol><h2 id="数据生成-tfrecord"><a href="#数据生成-tfrecord" class="headerlink" title="数据生成 tfrecord"></a>数据生成 tfrecord</h2><p>主要为<code>create_pretraining_data.py</code>分析。</p><h3 id="生成-tfrecord-命令"><a href="#生成-tfrecord-命令" class="headerlink" title="生成 tfrecord 命令"></a>生成 tfrecord 命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">python create_pretraining_data.py \</span><br><span class="line">  --input_file=./sample_text.txt \</span><br><span class="line">  --output_file=/tmp/tf_examples.tfrecord \</span><br><span class="line">  --vocab_file=<span class="variable">$BERT_BASE_DIR</span>/vocab.txt \</span><br><span class="line">  --do_lower_case=True \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --masked_lm_prob=0.15 \</span><br><span class="line">  --random_seed=12345 \</span><br><span class="line">  --dupe_factor=5</span><br></pre></td></tr></table></figure><p>这里并不是所有的参数，所有的参数和说明可以看<code>create_pretraining_data.py</code>中的<a href="#%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><code>flags.DEFINE_string</code></a>。</p><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">flags.DEFINE_string(<span class="string">"input_file"</span>, <span class="literal">None</span>,</span><br><span class="line">                    <span class="string">"Input raw text file (or comma-separated list of files)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"output_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Output TF example file (or comma-separated list of files)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(<span class="string">"vocab_file"</span>, <span class="literal">None</span>,</span><br><span class="line">                    <span class="string">"The vocabulary file that the BERT model was trained on."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">"do_lower_case"</span>, <span class="literal">True</span>,</span><br><span class="line">    <span class="string">"Whether to lower case the input text. Should be True for uncased "</span></span><br><span class="line">    <span class="string">"models and False for cased models."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(</span><br><span class="line">    <span class="string">"do_whole_word_mask"</span>, <span class="literal">False</span>,</span><br><span class="line">    <span class="string">"Whether to use whole word masking rather than per-WordPiece masking."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_seq_length"</span>, <span class="number">128</span>, <span class="string">"Maximum sequence length."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_predictions_per_seq"</span>, <span class="number">20</span>,</span><br><span class="line">                     <span class="string">"Maximum number of masked LM predictions per sequence."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"random_seed"</span>, <span class="number">12345</span>, <span class="string">"Random seed for data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"dupe_factor"</span>, <span class="number">10</span>,</span><br><span class="line">    <span class="string">"Number of times to duplicate the input data (with different masks)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(<span class="string">"masked_lm_prob"</span>, <span class="number">0.15</span>, <span class="string">"Masked LM probability."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(</span><br><span class="line">    <span class="string">"short_seq_prob"</span>, <span class="number">0.1</span>,</span><br><span class="line">    <span class="string">"Probability of creating sequences which are shorter than the "</span></span><br><span class="line">    <span class="string">"maximum length."</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>input_file：是输入文件，就按照上面说的格式。如果有多个文件可以用逗号分开</li><li>output_file：是输出的 tfrecord 文件，多个可以用逗号分开</li><li>vocab_file：是词表，可以直接用 bert 模型里面的 vocab。如果重新训练也可以用自己的词表</li><li>do_lower_case：是表示是否把输入小写</li><li>do_whole_word_mask：表示是否要进行整个单词的 mask，而不是 word piece 的 mask。word piece 会把单词拆分，非单词首部的用##开头</li><li>max_seq_length：表示拼接后的句子对组成的序列中包含 Wordpiece 级别的 token 数的上限，超过部分，需将较长的句子进行首尾截断</li><li>max_predictions_per_seq：表示每个序列中需要预测的 token 的上限</li><li>masked_lm_prob：表示生成的序列中被 masked 的 token 占总 token 数的比例。(这里的 masked 是广义的 mask，即将选中的 token 替换成[mask]或保持原词汇或随机替换成词表中的另一个词)，且有如下关系<code>max_predictions_per_seq = max_seq_length * masked_lm_prob</code></li><li>random_seed：用于复现结果，每次保持一样</li><li>dupe_factor：对输入使用不同的 mask 的次数，会重复创建 TrainingInstance</li><li>masked_lm_prob：mask LM 的概率，一般按照上面的公式确定</li><li>short_seq_prob：会按照这个概率创建比最大长度短的句子</li></ul><h3 id="main-of-pretraining-data"><a href="#main-of-pretraining-data" class="headerlink" title="main of pretraining data"></a>main of pretraining data</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  tokenizer = tokenization.FullTokenizer(</span><br><span class="line">      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">","</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"*** Reading from input files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span>, input_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 这个rng会一直用下去，作为参数传递</span></span><br><span class="line">  rng = random.Random(FLAGS.random_seed)</span><br><span class="line">  <span class="comment"># 得到的是一个一维instance列表，每个instance是一行预处理得到</span></span><br><span class="line">  instances = create_training_instances(</span><br><span class="line">      input_files, tokenizer, FLAGS.max_seq_length, FLAGS.dupe_factor,</span><br><span class="line">      FLAGS.short_seq_prob, FLAGS.masked_lm_prob, FLAGS.max_predictions_per_seq,</span><br><span class="line">      rng)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 所以输出文件也是可以为多个，用逗号分割</span></span><br><span class="line">  output_files = FLAGS.output_file.split(<span class="string">","</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"*** Writing to output files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span>, output_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 写入文件</span></span><br><span class="line">  write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length,</span><br><span class="line">                                  FLAGS.max_predictions_per_seq, output_files)</span><br></pre></td></tr></table></figure><p>可以看到，这个模块的流程大概是：</p><ol><li>创建 tokenizer，使用到了 vocab 和 do_lower_case 这两个参数</li><li>将 input files 整理到数组中</li><li>创建 training instances，见<a href="#create_training_instances">create_training_instances</a></li><li>将生成的每一个 TrainingInstance 对象依此转成 tf.train.Example 对象后</li><li>将上述生成的对象序列化到.tfrecord 格式的文件中。最终生成的.tfrecord 格式的文件是 BERT 预训练时的数据源，见<a href="#write_instance_to_example_files">write_instance_to_example_files</a></li></ol><p>下面先看如何<a href="#create_training_instances">创建 training instances</a></p><h3 id="create-training-instances"><a href="#create-training-instances" class="headerlink" title="create_training_instances"></a>create_training_instances</h3><p>直接在代码上写注释了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_training_instances</span><span class="params">(input_files, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                              dupe_factor, short_seq_prob, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                              max_predictions_per_seq, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Create `TrainingInstance`s from raw text."""</span></span><br><span class="line">  all_documents = [[]]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Input file format:</span></span><br><span class="line">  <span class="comment"># (1) One sentence per line. These should ideally be actual sentences, not</span></span><br><span class="line">  <span class="comment"># entire paragraphs or arbitrary spans of text. (Because we use the</span></span><br><span class="line">  <span class="comment"># sentence boundaries for the "next sentence prediction" task).</span></span><br><span class="line">  <span class="comment"># (2) Blank lines between documents. Document boundaries are needed so</span></span><br><span class="line">  <span class="comment"># that the "next sentence prediction" task doesn't span between documents.</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 读取文件并保存在all_documents二维列表中。</span></span><br><span class="line">  <span class="comment"># 每个文件按行读取，每读一行，转换成Unicode，如果没有了就break</span></span><br><span class="line">  <span class="comment"># 如果读到空行，就再all_documents中加入一个list表示下一个文件的开始。</span></span><br><span class="line">  <span class="comment"># 每行使用tokenizer进行tokenize，加到最后一个文档中。</span></span><br><span class="line">  <span class="comment"># 最终形成如下形式的all_documents:</span></span><br><span class="line">  <span class="comment"># [[[d1_s1],[d1_s2],…,[d2_sn]],…,[d2_sm]],…,[[dk_s1],…,[dk_sz]]]。</span></span><br><span class="line">  <span class="comment"># 上述表示一个语料中有k个文档，第一个文档有n句话，第二个文档有m句话，</span></span><br><span class="line">  <span class="comment"># 第k个文档有z句话，d1_s1表示第一个文档中的第一句话被分割成wordpiece级别的list。</span></span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(input_file, <span class="string">"r"</span>) <span class="keyword">as</span> reader:</span><br><span class="line">      <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        line = tokenization.convert_to_unicode(reader.readline())</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          <span class="keyword">break</span></span><br><span class="line">        line = line.strip()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Empty lines are used as document delimiters</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> line:</span><br><span class="line">          all_documents.append([])</span><br><span class="line">        tokens = tokenizer.tokenize(line)</span><br><span class="line">        <span class="keyword">if</span> tokens:</span><br><span class="line">          all_documents[<span class="number">-1</span>].append(tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Remove empty documents</span></span><br><span class="line">  <span class="comment"># 过滤空文档，并且随机打乱文档顺序</span></span><br><span class="line">  all_documents = [x <span class="keyword">for</span> x <span class="keyword">in</span> all_documents <span class="keyword">if</span> x]</span><br><span class="line">  rng.shuffle(all_documents)</span><br><span class="line"></span><br><span class="line">  vocab_words = list(tokenizer.vocab.keys())</span><br><span class="line">  instances = []</span><br><span class="line">  <span class="comment"># 重复dupe_factor次，为每个文档创建创建instances列表</span></span><br><span class="line">  <span class="keyword">for</span> _ <span class="keyword">in</span> range(dupe_factor):</span><br><span class="line">    <span class="comment"># 对all_documents中的每一个文档生成由TrainingInstance对象组成的instances列表</span></span><br><span class="line">    <span class="comment"># 即create_instances_from_document，并拼接(extend)所有的instances到一个instances中</span></span><br><span class="line">    <span class="keyword">for</span> document_index <span class="keyword">in</span> range(len(all_documents)):</span><br><span class="line">      instances.extend(</span><br><span class="line">          <span class="comment"># 一个文档是一个instances列表，里面每个instance对象是一行预处理的结果</span></span><br><span class="line">          create_instances_from_document(</span><br><span class="line">              all_documents, document_index, max_seq_length, short_seq_prob,</span><br><span class="line">              masked_lm_prob, max_predictions_per_seq, vocab_words, rng))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 随机打乱instances并返回，这时候所有的instance对象都在一个列表里</span></span><br><span class="line">  rng.shuffle(instances)</span><br><span class="line">  <span class="keyword">return</span> instances</span><br></pre></td></tr></table></figure><p>这里引申出一个问题，怎么从文档生成 TrainingInstance 对象组成的 instances 列表</p><p>见<a href="#create_instances_from_document">create_instances_from_document</a></p><p>debug 截图如下所示</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BOBnSQ.png" alt="BOBnSQ"></p><h3 id="create-instances-from-document"><a href="#create-instances-from-document" class="headerlink" title="create_instances_from_document"></a>create_instances_from_document</h3><p>同样是注释的形式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_instances_from_document</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">    all_documents, document_index, max_seq_length, short_seq_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">    masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates `TrainingInstance`s for a single document."""</span></span><br><span class="line">  <span class="comment"># 当前文档</span></span><br><span class="line">  document = all_documents[document_index]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Account for [CLS], [SEP], [SEP]</span></span><br><span class="line">  max_num_tokens = max_seq_length - <span class="number">3</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># We *usually* want to fill up the entire sequence since we are padding</span></span><br><span class="line">  <span class="comment"># to `max_seq_length` anyways, so short sequences are generally wasted</span></span><br><span class="line">  <span class="comment"># computation. However, we *sometimes*</span></span><br><span class="line">  <span class="comment"># (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter</span></span><br><span class="line">  <span class="comment"># sequences to minimize the mismatch between pre-training and fine-tuning.</span></span><br><span class="line">  <span class="comment"># The `target_seq_length` is just a rough target however, whereas</span></span><br><span class="line">  <span class="comment"># `max_seq_length` is a hard limit.</span></span><br><span class="line">  <span class="comment"># 通过short_seq_prob随机生成小于max_num_tokens的短句</span></span><br><span class="line">  target_seq_length = max_num_tokens</span><br><span class="line">  <span class="keyword">if</span> rng.random() &lt; short_seq_prob:</span><br><span class="line">    target_seq_length = rng.randint(<span class="number">2</span>, max_num_tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We DON'T just concatenate all of the tokens from a document into a long</span></span><br><span class="line">  <span class="comment"># sequence and choose an arbitrary split point because this would make the</span></span><br><span class="line">  <span class="comment"># next sentence prediction task too easy. Instead, we split the input into</span></span><br><span class="line">  <span class="comment"># segments "A" and "B" based on the actual "sentences" provided by the user</span></span><br><span class="line">  <span class="comment"># input.</span></span><br><span class="line">  instances = []</span><br><span class="line">  current_chunk = []</span><br><span class="line">  current_length = <span class="number">0</span></span><br><span class="line">  i = <span class="number">0</span></span><br><span class="line">  <span class="keyword">while</span> i &lt; len(document):</span><br><span class="line">    <span class="comment"># document是二维的，document[i]就是其中的一句话，被tokenize之后的wordpiece</span></span><br><span class="line">    segment = document[i]</span><br><span class="line">    <span class="comment"># chunk一直保存当前的segment</span></span><br><span class="line">    current_chunk.append(segment)</span><br><span class="line">    current_length += len(segment)</span><br><span class="line">    <span class="comment"># 只有当前chunk保存的token数达到了目标序列长度（不一定是max_num_tokens)，</span></span><br><span class="line">    <span class="comment"># 或者是文档的最后一行了，才进入这个if</span></span><br><span class="line">    <span class="comment"># 否则就直接i+1，继续把下一个segment加入到current_chunk中</span></span><br><span class="line">    <span class="comment"># 所以current_chunk有可能是一句话，也有可能使得多句话</span></span><br><span class="line">    <span class="comment"># 同理，segment A和segment B也是有可能一句，也可能多句</span></span><br><span class="line">    <span class="keyword">if</span> i == len(document) - <span class="number">1</span> <span class="keyword">or</span> current_length &gt;= target_seq_length:</span><br><span class="line">      <span class="comment"># 当前chunk非空就得进，防止chunk被置空之后没有新的segment加进去</span></span><br><span class="line">      <span class="keyword">if</span> current_chunk:</span><br><span class="line">        <span class="comment"># `a_end` is how many segments from `current_chunk` go into the `A`</span></span><br><span class="line">        <span class="comment"># (first) sentence.</span></span><br><span class="line">        <span class="comment"># a_end是用于确定把current_chunk中的多少token分给segment A的</span></span><br><span class="line">        a_end = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> len(current_chunk) &gt;= <span class="number">2</span>:</span><br><span class="line">          a_end = rng.randint(<span class="number">1</span>, len(current_chunk) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        tokens_a = []</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(a_end):</span><br><span class="line">          tokens_a.extend(current_chunk[j])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 现在开始确定segment B，有两种方式，一种是random_next，</span></span><br><span class="line">        <span class="comment"># 另一种是依次拼接current_chunk中剩下的token</span></span><br><span class="line">        tokens_b = []</span><br><span class="line">        <span class="comment"># Random next</span></span><br><span class="line">        is_random_next = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 如果current_chunk长度为1，那么就肯定被分给A，segment B只能random next</span></span><br><span class="line">        <span class="comment"># random next的概率是50%</span></span><br><span class="line">        <span class="comment"># 此时segment B的长度就被确定为：target_seq_length - len(tokens_a)，</span></span><br><span class="line">        <span class="comment"># 因为A和B最后要拼在一起训练</span></span><br><span class="line">        <span class="keyword">if</span> len(current_chunk) == <span class="number">1</span> <span class="keyword">or</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          is_random_next = <span class="literal">True</span></span><br><span class="line">          target_b_length = target_seq_length - len(tokens_a)</span><br><span class="line"></span><br><span class="line">          <span class="comment"># This should rarely go for more than one iteration for large</span></span><br><span class="line">          <span class="comment"># corpora. However, just to be careful, we try to make sure that</span></span><br><span class="line">          <span class="comment"># the random document is not the same as the document</span></span><br><span class="line">          <span class="comment"># we're processing.</span></span><br><span class="line">          <span class="comment"># 确保random next的不是当前文档</span></span><br><span class="line">          <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">            random_document_index = rng.randint(<span class="number">0</span>, len(all_documents) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> random_document_index != document_index:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">          <span class="comment"># 随机确定segment B在random doc中的开始位置</span></span><br><span class="line">          random_document = all_documents[random_document_index]</span><br><span class="line">          random_start = rng.randint(<span class="number">0</span>, len(random_document) - <span class="number">1</span>)</span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> range(random_start, len(random_document)):</span><br><span class="line">            tokens_b.extend(random_document[j])</span><br><span class="line">            <span class="keyword">if</span> len(tokens_b) &gt;= target_b_length:</span><br><span class="line">              <span class="keyword">break</span></span><br><span class="line">          <span class="comment"># We didn't actually use these segments so we "put them back" so</span></span><br><span class="line">          <span class="comment"># they don't go to waste.</span></span><br><span class="line">          <span class="comment"># 剩下的没有用上的segment，直接回退回去，不能浪费，i控制的是document[i]</span></span><br><span class="line">          num_unused_segments = len(current_chunk) - a_end</span><br><span class="line">          i -= num_unused_segments</span><br><span class="line">        <span class="comment"># Actual next，segment B直接就是current chunk中剩下的内容</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          is_random_next = <span class="literal">False</span></span><br><span class="line">          <span class="keyword">for</span> j <span class="keyword">in</span> range(a_end, len(current_chunk)):</span><br><span class="line">            tokens_b.extend(current_chunk[j])</span><br><span class="line">        <span class="comment"># 确保segment A和B拼起来不会超过num_tokens</span></span><br><span class="line">        <span class="comment"># 每次哪个长去掉哪个，并且按照50%的几率从头部去掉or从尾部去掉，直到符合要求</span></span><br><span class="line">        truncate_seq_pair(tokens_a, tokens_b, max_num_tokens, rng)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> len(tokens_a) &gt;= <span class="number">1</span></span><br><span class="line">        <span class="keyword">assert</span> len(tokens_b) &gt;= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最终拼接后的两个segments形成的tokens的基础上做mask操作，</span></span><br><span class="line">        <span class="comment"># 生成masked LM任务需要的tokens形式</span></span><br><span class="line">        tokens = []</span><br><span class="line">        segment_ids = []</span><br><span class="line">        <span class="comment"># token和segment ids数量相同，segment A对应的ids是0，B对应的是1</span></span><br><span class="line">        tokens.append(<span class="string">"[CLS]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_a:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># segment A和B之间用[SEP]分开</span></span><br><span class="line">        tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> tokens_b:</span><br><span class="line">          tokens.append(token)</span><br><span class="line">          segment_ids.append(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 最后也要加一个[SEP]</span></span><br><span class="line">        tokens.append(<span class="string">"[SEP]"</span>)</span><br><span class="line">        segment_ids.append(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 创建Masked LM</span></span><br><span class="line">        (tokens, masked_lm_positions,</span><br><span class="line">         masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">             tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 一个pair(A, B)创建一个instance</span></span><br><span class="line">        instance = TrainingInstance(</span><br><span class="line">            tokens=tokens,</span><br><span class="line">            segment_ids=segment_ids,</span><br><span class="line">            is_random_next=is_random_next,</span><br><span class="line">            masked_lm_positions=masked_lm_positions,</span><br><span class="line">            masked_lm_labels=masked_lm_labels)</span><br><span class="line">        <span class="comment"># 每一行是一个instance，每个文档是instances列表，最后返回这个列表</span></span><br><span class="line">        instances.append(instance)</span><br><span class="line">      <span class="comment"># 一个current_chunk创建一个instance，用完清空</span></span><br><span class="line">      current_chunk = []</span><br><span class="line">      current_length = <span class="number">0</span></span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> instances</span><br></pre></td></tr></table></figure><p>debug 截图如下所示：</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LaGZ7B.png" alt="LaGZ7B"></p><p>上面主要是解决了 BERT 中的 NSP 问题，然后又引出了两个问题：</p><ol><li>MLM 问题，就是那个<a href="#create_masked_lm_predictions">create_masked_lm_predictions()</a></li><li>生成 TrainingInstance 问题，即<a href="#traininginstance">TrainingInstance()</a></li></ol><h3 id="create-masked-lm-predictions"><a href="#create-masked-lm-predictions" class="headerlink" title="create_masked_lm_predictions"></a>create_masked_lm_predictions</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_masked_lm_predictions</span><span class="params">(tokens, masked_lm_prob,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 max_predictions_per_seq, vocab_words, rng)</span>:</span></span><br><span class="line">  <span class="string">"""Creates the predictions for the masked LM objective.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  输入的tokens：['[CLS]', 'ancient', 'sage', '-', 'un', '##im', '##port', '##ant', ...]</span></span><br><span class="line"><span class="string">  输入的rng: &lt;random.Random object at 0x7fbcdc9e8020&gt;</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  cand_indexes = []</span><br><span class="line">  <span class="comment"># 这个for循环的作用是创建candidate indexes列表，如果要whole word mask，需要进行拼接处理</span></span><br><span class="line">  <span class="comment"># wordpiece 会把一些词拆成多个，单词的第一个word piece没有任何标记，</span></span><br><span class="line">  <span class="comment"># 后续的word piece会在开头加上##标记</span></span><br><span class="line">  <span class="keyword">for</span> (i, token) <span class="keyword">in</span> enumerate(tokens):</span><br><span class="line">    <span class="keyword">if</span> token == <span class="string">"[CLS]"</span> <span class="keyword">or</span> token == <span class="string">"[SEP]"</span>:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># Whole Word Masking means that if we mask all of the wordpieces</span></span><br><span class="line">    <span class="comment"># corresponding to an original word. When a word has been split into</span></span><br><span class="line">    <span class="comment"># WordPieces, the first token does not have any marker and any subsequence</span></span><br><span class="line">    <span class="comment"># tokens are prefixed with ##. So whenever we see the ## token, we</span></span><br><span class="line">    <span class="comment"># append it to the previous set of word indexes.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Note that Whole Word Masking does *not* change the training code</span></span><br><span class="line">    <span class="comment"># at all -- we still predict each WordPiece independently, softmaxed</span></span><br><span class="line">    <span class="comment"># over the entire vocabulary.</span></span><br><span class="line">    <span class="keyword">if</span> (FLAGS.do_whole_word_mask <span class="keyword">and</span> len(cand_indexes) &gt;= <span class="number">1</span> <span class="keyword">and</span></span><br><span class="line">        token.startswith(<span class="string">"##"</span>)):</span><br><span class="line">      cand_indexes[<span class="number">-1</span>].append(i)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      cand_indexes.append([i])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 打乱indexs</span></span><br><span class="line">  rng.shuffle(cand_indexes)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># output_tokens 是 tokens输入的copy</span></span><br><span class="line">  output_tokens = list(tokens)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># mask掉的token数量不可以超过max_predictions_per_seq</span></span><br><span class="line">  num_to_predict = min(max_predictions_per_seq,</span><br><span class="line">                       max(<span class="number">1</span>, int(round(len(tokens) * masked_lm_prob))))</span><br><span class="line"></span><br><span class="line">  masked_lms = []</span><br><span class="line">  covered_indexes = set()</span><br><span class="line">  <span class="keyword">for</span> index_set <span class="keyword">in</span> cand_indexes:</span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) &gt;= num_to_predict:</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># If adding a whole-word mask would exceed the maximum number of</span></span><br><span class="line">    <span class="comment"># predictions, then just skip this candidate.</span></span><br><span class="line">    <span class="keyword">if</span> len(masked_lms) + len(index_set) &gt; num_to_predict:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line">    <span class="comment"># 没看懂下面这段什么意思index不应该都是唯一的吗，为什么可能index in covered_indexes == True的情况</span></span><br><span class="line">    is_any_index_covered = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      <span class="keyword">if</span> index <span class="keyword">in</span> covered_indexes:</span><br><span class="line">        is_any_index_covered = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> is_any_index_covered:</span><br><span class="line">      <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 针对whole word mask，</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> index_set:</span><br><span class="line">      covered_indexes.add(index)</span><br><span class="line"></span><br><span class="line">      masked_token = <span class="literal">None</span></span><br><span class="line">      <span class="comment"># 80% of the time, replace with [MASK]</span></span><br><span class="line">      <span class="keyword">if</span> rng.random() &lt; <span class="number">0.8</span>:</span><br><span class="line">        masked_token = <span class="string">"[MASK]"</span></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># 10% of the time, keep original</span></span><br><span class="line">        <span class="keyword">if</span> rng.random() &lt; <span class="number">0.5</span>:</span><br><span class="line">          masked_token = tokens[index]</span><br><span class="line">        <span class="comment"># 10% of the time, replace with random word</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          masked_token = vocab_words[rng.randint(<span class="number">0</span>, len(vocab_words) - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">      output_tokens[index] = masked_token</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 为这个token创建MaskedLmInstance</span></span><br><span class="line">      <span class="comment"># 声明：MaskedLmInstance = collections.namedtuple("MaskedLmInstance", ["index", "label"])</span></span><br><span class="line">      <span class="comment"># 类型namedtuple: Returns a new subclass of tuple with named fields.</span></span><br><span class="line">      <span class="comment"># 用于创建position embedding和real label</span></span><br><span class="line">      masked_lms.append(MaskedLmInstance(index=index, label=tokens[index]))</span><br><span class="line">  <span class="keyword">assert</span> len(masked_lms) &lt;= num_to_predict</span><br><span class="line">  <span class="comment"># 因为之前对cand_indexes进行了shuffle，所以返回前要进行sort</span></span><br><span class="line">  masked_lms = sorted(masked_lms, key=<span class="keyword">lambda</span> x: x.index)</span><br><span class="line"></span><br><span class="line">  masked_lm_positions = []</span><br><span class="line">  masked_lm_labels = []</span><br><span class="line">  <span class="keyword">for</span> p <span class="keyword">in</span> masked_lms:</span><br><span class="line">    masked_lm_positions.append(p.index)</span><br><span class="line">    masked_lm_labels.append(p.label)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (output_tokens, masked_lm_positions, masked_lm_labels)</span><br></pre></td></tr></table></figure><p>最后返回给<code>create_instances_from_document</code>中的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(tokens, masked_lm_positions,</span><br><span class="line">  masked_lm_labels) = create_masked_lm_predictions(</span><br><span class="line">      tokens, masked_lm_prob, max_predictions_per_seq, vocab_words, rng)</span><br></pre></td></tr></table></figure><h3 id="TrainingInstance"><a href="#TrainingInstance" class="headerlink" title="TrainingInstance"></a>TrainingInstance</h3><p>在结束 Masked LM 之后就可以来创建 TrainingInstance。</p><p>在<code>create_instances_from_document</code>中调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">instance = TrainingInstance(</span><br><span class="line">    tokens=tokens,</span><br><span class="line">    segment_ids=segment_ids,</span><br><span class="line">    is_random_next=is_random_next,</span><br><span class="line">    masked_lm_positions=masked_lm_positions,</span><br><span class="line">    masked_lm_labels=masked_lm_labels)</span><br><span class="line">instances.append(instance)</span><br></pre></td></tr></table></figure><p>TrainingInstance 类别源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TrainingInstance</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""A single training instance (sentence pair)."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, tokens, segment_ids, masked_lm_positions, masked_lm_labels,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_random_next)</span>:</span></span><br><span class="line">    self.tokens = tokens</span><br><span class="line">    self.segment_ids = segment_ids</span><br><span class="line">    self.is_random_next = is_random_next</span><br><span class="line">    self.masked_lm_positions = masked_lm_positions</span><br><span class="line">    self.masked_lm_labels = masked_lm_labels</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 用于print函数调用的，一般都是return一个什么东西</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">    s = <span class="string">""</span></span><br><span class="line">    s += <span class="string">"tokens: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.tokens]))</span><br><span class="line">    s += <span class="string">"segment_ids: %s\n"</span> % (<span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.segment_ids]))</span><br><span class="line">    s += <span class="string">"is_random_next: %s\n"</span> % self.is_random_next</span><br><span class="line">    s += <span class="string">"masked_lm_positions: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [str(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.masked_lm_positions]))</span><br><span class="line">    s += <span class="string">"masked_lm_labels: %s\n"</span> % (<span class="string">" "</span>.join(</span><br><span class="line">        [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> self.masked_lm_labels]))</span><br><span class="line">    s += <span class="string">"\n"</span></span><br><span class="line">    <span class="keyword">return</span> s</span><br><span class="line"></span><br><span class="line">  <span class="comment"># __str__()用于显示给用户，而__repr__()用于显示给开发人员</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.__str__()</span><br></pre></td></tr></table></figure><h3 id="write-instance-to-example-files"><a href="#write-instance-to-example-files" class="headerlink" title="write_instance_to_example_files"></a>write_instance_to_example_files</h3><p>返回到<a href="#main-of-pretraining-data">main 函数</a>中可以看到，还需要把 TrainingInstance 对象写入到输出文件中，即<br><code>write_instance_to_example_files(instances, tokenizer, FLAGS.max_seq_length, FLAGS.max_predictions_per_seq, output_files)</code>那段。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">write_instance_to_example_files</span><span class="params">(instances, tokenizer, max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                                    max_predictions_per_seq, output_files)</span>:</span></span><br><span class="line">  <span class="string">"""Create TF example files from `TrainingInstance`s."""</span></span><br><span class="line">  writers = []</span><br><span class="line">  <span class="keyword">for</span> output_file <span class="keyword">in</span> output_files:</span><br><span class="line">    writers.append(tf.python_io.TFRecordWriter(output_file))</span><br><span class="line"></span><br><span class="line">  writer_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  total_written = <span class="number">0</span></span><br><span class="line">  <span class="keyword">for</span> (inst_index, instance) <span class="keyword">in</span> enumerate(instances):</span><br><span class="line">    input_ids = tokenizer.convert_tokens_to_ids(instance.tokens)</span><br><span class="line">    input_mask = [<span class="number">1</span>] * len(input_ids)</span><br><span class="line">    segment_ids = list(instance.segment_ids)</span><br><span class="line">    <span class="keyword">assert</span> len(input_ids) &lt;= max_seq_length</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保证各个数据的长度都是max_seq_length，不足补0</span></span><br><span class="line">    <span class="keyword">while</span> len(input_ids) &lt; max_seq_length:</span><br><span class="line">      input_ids.append(<span class="number">0</span>)</span><br><span class="line">      input_mask.append(<span class="number">0</span>)</span><br><span class="line">      segment_ids.append(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> len(input_ids) == max_seq_length</span><br><span class="line">    <span class="keyword">assert</span> len(input_mask) == max_seq_length</span><br><span class="line">    <span class="keyword">assert</span> len(segment_ids) == max_seq_length</span><br><span class="line"></span><br><span class="line">    masked_lm_positions = list(instance.masked_lm_positions)</span><br><span class="line">    masked_lm_ids = tokenizer.convert_tokens_to_ids(instance.masked_lm_labels)</span><br><span class="line">    <span class="comment"># masked_lm_weights和input_mask 的作用一样，标注masked_lm_ids</span></span><br><span class="line">    <span class="comment"># 和 masked_lm_positions 哪些是真实值，哪些是补全值</span></span><br><span class="line">    masked_lm_weights = [<span class="number">1.0</span>] * len(masked_lm_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> len(masked_lm_positions) &lt; max_predictions_per_seq:</span><br><span class="line">      masked_lm_positions.append(<span class="number">0</span>)</span><br><span class="line">      masked_lm_ids.append(<span class="number">0</span>)</span><br><span class="line">      masked_lm_weights.append(<span class="number">0.0</span>)</span><br><span class="line"></span><br><span class="line">    next_sentence_label = <span class="number">1</span> <span class="keyword">if</span> instance.is_random_next <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 关于创建feature的函数</span></span><br><span class="line">    <span class="comment"># def create_int_feature(values):</span></span><br><span class="line">    <span class="comment">#   feature = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))</span></span><br><span class="line">    <span class="comment">#   return feature</span></span><br><span class="line">    features = collections.OrderedDict()</span><br><span class="line">    features[<span class="string">"input_ids"</span>] = create_int_feature(input_ids)</span><br><span class="line">    features[<span class="string">"input_mask"</span>] = create_int_feature(input_mask)</span><br><span class="line">    features[<span class="string">"segment_ids"</span>] = create_int_feature(segment_ids)</span><br><span class="line">    features[<span class="string">"masked_lm_positions"</span>] = create_int_feature(masked_lm_positions)</span><br><span class="line">    features[<span class="string">"masked_lm_ids"</span>] = create_int_feature(masked_lm_ids)</span><br><span class="line">    features[<span class="string">"masked_lm_weights"</span>] = create_float_feature(masked_lm_weights)</span><br><span class="line">    features[<span class="string">"next_sentence_labels"</span>] = create_int_feature([next_sentence_label])</span><br><span class="line"></span><br><span class="line">    tf_example = tf.train.Example(features=tf.train.Features(feature=features))</span><br><span class="line">    <span class="comment"># 保存为tf.train.Example(features字典[key: 特征, value: tf.train.Feature对象])保存</span></span><br><span class="line">    writers[writer_index].write(tf_example.SerializeToString())</span><br><span class="line">    writer_index = (writer_index + <span class="number">1</span>) % len(writers)</span><br><span class="line"></span><br><span class="line">    total_written += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印前20个样本</span></span><br><span class="line">    <span class="keyword">if</span> inst_index &lt; <span class="number">20</span>:</span><br><span class="line">      tf.logging.info(<span class="string">"*** Example ***"</span>)</span><br><span class="line">      tf.logging.info(<span class="string">"tokens: %s"</span> % <span class="string">" "</span>.join(</span><br><span class="line">          [tokenization.printable_text(x) <span class="keyword">for</span> x <span class="keyword">in</span> instance.tokens]))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">for</span> feature_name <span class="keyword">in</span> features.keys():</span><br><span class="line">        feature = features[feature_name]</span><br><span class="line">        values = []</span><br><span class="line">        <span class="keyword">if</span> feature.int64_list.value:</span><br><span class="line">          values = feature.int64_list.value</span><br><span class="line">        <span class="keyword">elif</span> feature.float_list.value:</span><br><span class="line">          values = feature.float_list.value</span><br><span class="line">        tf.logging.info(</span><br><span class="line">            <span class="string">"%s: %s"</span> % (feature_name, <span class="string">" "</span>.join([str(x) <span class="keyword">for</span> x <span class="keyword">in</span> values])))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> writer <span class="keyword">in</span> writers:</span><br><span class="line">    writer.close()</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"Wrote %d total instances"</span>, total_written)</span><br></pre></td></tr></table></figure><h2 id="预训练-run-pretrain"><a href="#预训练-run-pretrain" class="headerlink" title="预训练 run_pretrain"></a>预训练 run_pretrain</h2><h3 id="预训练命令"><a href="#预训练命令" class="headerlink" title="预训练命令"></a>预训练命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">python run_pretraining.py \</span><br><span class="line">  --input_file=/tmp/tf_examples.tfrecord \</span><br><span class="line">  --output_dir=/tmp/pretraining_output \</span><br><span class="line">  --do_train=True \</span><br><span class="line">  --do_eval=True \</span><br><span class="line">  --bert_config_file=<span class="variable">$BERT_BASE_DIR</span>/bert_config.json \</span><br><span class="line">  --init_checkpoint=<span class="variable">$BERT_BASE_DIR</span>/bert_model.ckpt \</span><br><span class="line">  --train_batch_size=32 \</span><br><span class="line">  --max_seq_length=128 \</span><br><span class="line">  --max_predictions_per_seq=20 \</span><br><span class="line">  --num_train_steps=20 \</span><br><span class="line">  --num_warmup_steps=10 \</span><br><span class="line">  --learning_rate=2e-5</span><br></pre></td></tr></table></figure><h3 id="参数说明-1"><a href="#参数说明-1" class="headerlink" title="参数说明"></a>参数说明</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Required parameters</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"bert_config_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The config json file corresponding to the pre-trained BERT model. "</span></span><br><span class="line">    <span class="string">"This specifies the model architecture."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"input_file"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Input TF example files (can be a glob or comma separated)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"output_dir"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The output directory where the model checkpoints will be written."</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## Other parameters</span></span><br><span class="line">flags.DEFINE_string(</span><br><span class="line">    <span class="string">"init_checkpoint"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"Initial checkpoint (usually from a pre-trained BERT model)."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"max_seq_length"</span>, <span class="number">128</span>,</span><br><span class="line">    <span class="string">"The maximum total input sequence length after WordPiece tokenization. "</span></span><br><span class="line">    <span class="string">"Sequences longer than this will be truncated, and sequences shorter "</span></span><br><span class="line">    <span class="string">"than this will be padded. Must match data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"max_predictions_per_seq"</span>, <span class="number">20</span>,</span><br><span class="line">    <span class="string">"Maximum number of masked LM predictions per sequence. "</span></span><br><span class="line">    <span class="string">"Must match data generation."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"do_train"</span>, <span class="literal">False</span>, <span class="string">"Whether to run training."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"do_eval"</span>, <span class="literal">False</span>, <span class="string">"Whether to run eval on the dev set."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"train_batch_size"</span>, <span class="number">32</span>, <span class="string">"Total batch size for training."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"eval_batch_size"</span>, <span class="number">8</span>, <span class="string">"Total batch size for eval."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_float(<span class="string">"learning_rate"</span>, <span class="number">5e-5</span>, <span class="string">"The initial learning rate for Adam."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"num_train_steps"</span>, <span class="number">100000</span>, <span class="string">"Number of training steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"num_warmup_steps"</span>, <span class="number">10000</span>, <span class="string">"Number of warmup steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"save_checkpoints_steps"</span>, <span class="number">1000</span>,</span><br><span class="line">                     <span class="string">"How often to save the model checkpoint."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"iterations_per_loop"</span>, <span class="number">1000</span>,</span><br><span class="line">                     <span class="string">"How many steps to make in each estimator call."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(<span class="string">"max_eval_steps"</span>, <span class="number">100</span>, <span class="string">"Maximum number of eval steps."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_bool(<span class="string">"use_tpu"</span>, <span class="literal">False</span>, <span class="string">"Whether to use TPU or GPU/CPU."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"tpu_name"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"The Cloud TPU to use for training. This should be either the name "</span></span><br><span class="line">    <span class="string">"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 "</span></span><br><span class="line">    <span class="string">"url."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"tpu_zone"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"[Optional] GCE zone where the Cloud TPU is located in. If not "</span></span><br><span class="line">    <span class="string">"specified, we will attempt to automatically detect the GCE project from "</span></span><br><span class="line">    <span class="string">"metadata."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(</span><br><span class="line">    <span class="string">"gcp_project"</span>, <span class="literal">None</span>,</span><br><span class="line">    <span class="string">"[Optional] Project name for the Cloud TPU-enabled project. If not "</span></span><br><span class="line">    <span class="string">"specified, we will attempt to automatically detect the GCE project from "</span></span><br><span class="line">    <span class="string">"metadata."</span>)</span><br><span class="line"></span><br><span class="line">tf.flags.DEFINE_string(<span class="string">"master"</span>, <span class="literal">None</span>, <span class="string">"[Optional] TensorFlow master URL."</span>)</span><br><span class="line"></span><br><span class="line">flags.DEFINE_integer(</span><br><span class="line">    <span class="string">"num_tpu_cores"</span>, <span class="number">8</span>,</span><br><span class="line">    <span class="string">"Only used if `use_tpu` is True. Total number of TPU cores to use."</span>)</span><br></pre></td></tr></table></figure><p>其中：</p><ul><li>bert_config_file: 预训练模型的配置文件，直接使用对应大小 bert model 里的 config，或者自己调</li><li>input_file: tfrecord 格式的文件</li><li>output_dir: 预训练生成的模型文件路径，会自动创建</li><li>init_checkpoint: 预训练模型的初始检查点，从头开始训练就不需要这个参数，fine-tune 的话就加载 bert 预训练的 ckpt</li><li>max_seq_length: 最大序列长度，超过这个的会被阶段，不足的会补齐。要和数据生成 tfrecord 过程的一致。类似于 RNN 中的最大时间步，每次可动态调整。针对某一特定领域的语料，可在通用的语言模型的基础上，每次通过设置不同长度的专业领域的句子对微调语言模型，使最终生成的预训练的语言模型更适合某一特定领域</li><li>train_batch_size: 训练的 mini batch 大小。如果出现内存不够的问题，那么调小 max_seq_length 或者 batch 大小就可以。</li><li>do_train: 如果不训练只是预测 or 验证，可以设置为 false</li><li>do_eval: 是否进行 eval 验证</li><li>eval_batch_size: 验证的时候的 batch 大小</li><li>learning_rate: Adam 的学习率，有论文表明越小越好，一般是 2e-5 级别</li><li>num_train_steps: 训练的步数，如果是自己从头训练，这个步数要根据语料大小看，一般设置 w 级别。</li><li>num_warmup_steps: warmup 步数，学习率从 0 逐渐增加到初始学习率所需的步数，以后的步数保持固定学习率。参考<a href="https://github.com/google-research/bert/issues/529" target="_blank" rel="external nofollow noopener noreferrer">github-issue</a></li><li>save_checkpoints_steps: 每隔多少步保存一次模型</li><li>iterations_per_loop: 每次调用 estimator 的步数</li><li>max_eval_steps: 最大的 eval 步数</li><li>tup 相关参数看说明</li></ul><h3 id="main-of-pretrain"><a href="#main-of-pretrain" class="headerlink" title="main of pretrain"></a>main of pretrain</h3><p>在源码中写了注释说明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(_)</span>:</span></span><br><span class="line">  tf.logging.set_verbosity(tf.logging.INFO)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 可以选择不训练的，不改变模型参数</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.do_train <span class="keyword">and</span> <span class="keyword">not</span> FLAGS.do_eval:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"At least one of `do_train` or `do_eval` must be True."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 读取模型参数</span></span><br><span class="line">  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)</span><br><span class="line"></span><br><span class="line">  tf.gfile.MakeDirs(FLAGS.output_dir)</span><br><span class="line"></span><br><span class="line">  input_files = []</span><br><span class="line">  <span class="keyword">for</span> input_pattern <span class="keyword">in</span> FLAGS.input_file.split(<span class="string">","</span>):</span><br><span class="line">    input_files.extend(tf.gfile.Glob(input_pattern))</span><br><span class="line"></span><br><span class="line">  tf.logging.info(<span class="string">"*** Input Files ***"</span>)</span><br><span class="line">  <span class="keyword">for</span> input_file <span class="keyword">in</span> input_files:</span><br><span class="line">    tf.logging.info(<span class="string">"  %s"</span> % input_file)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># TPU相关</span></span><br><span class="line">  tpu_cluster_resolver = <span class="literal">None</span></span><br><span class="line">  <span class="keyword">if</span> FLAGS.use_tpu <span class="keyword">and</span> FLAGS.tpu_name:</span><br><span class="line">    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(</span><br><span class="line">        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)</span><br><span class="line"></span><br><span class="line">  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2</span><br><span class="line">  run_config = tf.contrib.tpu.RunConfig(</span><br><span class="line">      cluster=tpu_cluster_resolver,</span><br><span class="line">      master=FLAGS.master,</span><br><span class="line">      model_dir=FLAGS.output_dir,</span><br><span class="line">      save_checkpoints_steps=FLAGS.save_checkpoints_steps,</span><br><span class="line">      tpu_config=tf.contrib.tpu.TPUConfig(</span><br><span class="line">          iterations_per_loop=FLAGS.iterations_per_loop,</span><br><span class="line">          num_shards=FLAGS.num_tpu_cores,</span><br><span class="line">          per_host_input_for_training=is_per_host))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 正式创建模型，其实返回的是output_spec = tf.contrib.tpu.TPUEstimatorSpec()</span></span><br><span class="line">  model_fn = model_fn_builder(</span><br><span class="line">      bert_config=bert_config,</span><br><span class="line">      init_checkpoint=FLAGS.init_checkpoint,</span><br><span class="line">      learning_rate=FLAGS.learning_rate,</span><br><span class="line">      num_train_steps=FLAGS.num_train_steps,</span><br><span class="line">      num_warmup_steps=FLAGS.num_warmup_steps,</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      use_one_hot_embeddings=FLAGS.use_tpu)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># If TPU is not available, this will fall back to normal Estimator on CPU</span></span><br><span class="line">  <span class="comment"># or GPU.</span></span><br><span class="line">  estimator = tf.contrib.tpu.TPUEstimator(</span><br><span class="line">      use_tpu=FLAGS.use_tpu,</span><br><span class="line">      model_fn=model_fn,</span><br><span class="line">      config=run_config,</span><br><span class="line">      train_batch_size=FLAGS.train_batch_size,</span><br><span class="line">      eval_batch_size=FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">    tf.logging.info(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">    tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.train_batch_size)</span><br><span class="line">    <span class="comment"># 从tfrecord解析出BERT的输入数据</span></span><br><span class="line">    train_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">True</span>)</span><br><span class="line">    estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> FLAGS.do_eval:</span><br><span class="line">    tf.logging.info(<span class="string">"***** Running evaluation *****"</span>)</span><br><span class="line">    tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.eval_batch_size)</span><br><span class="line">    <span class="comment"># 如果不训练只是eval，输入也是一样的tfrecord</span></span><br><span class="line">    eval_input_fn = input_fn_builder(</span><br><span class="line">        input_files=input_files,</span><br><span class="line">        max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">        max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">        is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># evaluate操作得到结果</span></span><br><span class="line">    result = estimator.evaluate(</span><br><span class="line">        input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)</span><br><span class="line"></span><br><span class="line">    output_eval_file = os.path.join(FLAGS.output_dir, <span class="string">"eval_results.txt"</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.gfile.GFile(output_eval_file, <span class="string">"w"</span>) <span class="keyword">as</span> writer:</span><br><span class="line">      tf.logging.info(<span class="string">"***** Eval results *****"</span>)</span><br><span class="line">      <span class="keyword">for</span> key <span class="keyword">in</span> sorted(result.keys()):</span><br><span class="line">        tf.logging.info(<span class="string">"  %s = %s"</span>, key, str(result[key]))</span><br><span class="line">        writer.write(<span class="string">"%s = %s\n"</span> % (key, str(result[key])))</span><br></pre></td></tr></table></figure><p>这里引出几个问题：</p><ul><li>模型的创建: <a href="#model_fn_builder">model_fn_builder</a></li><li>数据的解析: <a href="#input_fn_builder">input_fn_builder</a></li><li>模型的训练：estimator.train</li><li>验证集的测试: estimator.evaluate</li></ul><h3 id="input-fn-builder"><a href="#input-fn-builder" class="headerlink" title="input_fn_builder"></a>input_fn_builder</h3><p>从 tfrecord 中解析出 bert 的输入数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">input_fn_builder</span><span class="params">(input_files,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_seq_length,</span></span></span><br><span class="line"><span class="function"><span class="params">                     max_predictions_per_seq,</span></span></span><br><span class="line"><span class="function"><span class="params">                     is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_cpu_threads=<span class="number">4</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Creates an `input_fn` closure to be passed to TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">input_fn</span><span class="params">(params)</span>:</span></span><br><span class="line">    <span class="string">"""The actual input function."""</span></span><br><span class="line">    batch_size = params[<span class="string">"batch_size"</span>]</span><br><span class="line"></span><br><span class="line">    name_to_features = &#123;</span><br><span class="line">        <span class="string">"input_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"input_mask"</span>: <span class="comment"># input_mask=0表示是补齐的部分</span></span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"segment_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_seq_length], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_positions"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_ids"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.int64),</span><br><span class="line">        <span class="string">"masked_lm_weights"</span>:</span><br><span class="line">            tf.FixedLenFeature([max_predictions_per_seq], tf.float32),</span><br><span class="line">        <span class="string">"next_sentence_labels"</span>:</span><br><span class="line">            tf.FixedLenFeature([<span class="number">1</span>], tf.int64),</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># For training, we want a lot of parallel reading and shuffling.</span></span><br><span class="line">    <span class="comment"># For eval, we want no shuffling and parallel reading doesn't matter.</span></span><br><span class="line">    <span class="keyword">if</span> is_training:</span><br><span class="line">      <span class="comment"># 得到dataset对象</span></span><br><span class="line">      d = tf.data.Dataset.from_tensor_slices(tf.constant(input_files))</span><br><span class="line">      <span class="comment"># 如果 repeat 转换在 shuffle 转换之前应用，则迭代次数边界将变的不确定。</span></span><br><span class="line">      <span class="comment"># 也就是说，某些元素可以在其他元素出现之前重复一次。</span></span><br><span class="line">      <span class="comment"># 另一方面，如果在 repeat 转换之前应用 shuffle 转换，则在涉及 shuffle</span></span><br><span class="line">      <span class="comment"># 转换的内部状态初始化的每个迭代次数开始时性能可能会下降。</span></span><br><span class="line">      <span class="comment"># 换句话说，前者（在 shuffle 之前 repeat）提供了更好的性能，</span></span><br><span class="line">      <span class="comment"># 而后者（在 repeat 之前 shuffle）提供了更确定性的排序。</span></span><br><span class="line">      d = d.repeat()</span><br><span class="line">      d = d.shuffle(buffer_size=len(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `cycle_length` is the number of parallel files that get read.</span></span><br><span class="line">      cycle_length = min(num_cpu_threads, len(input_files))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># `sloppy` mode means that the interleaving is not exact. This adds</span></span><br><span class="line">      <span class="comment"># even more randomness to the training pipeline.</span></span><br><span class="line">      <span class="comment"># 这里的说明可以看https://tensorflow.juejin.im/performance/datasets_performance.html</span></span><br><span class="line">      <span class="comment"># sloppy设置为True，转换可能会偏离其确定性顺序，但是这样可以让训练数据更加随机化</span></span><br><span class="line">      d = d.apply(</span><br><span class="line">          tf.contrib.data.parallel_interleave(</span><br><span class="line">              tf.data.TFRecordDataset,</span><br><span class="line">              sloppy=is_training,</span><br><span class="line">              cycle_length=cycle_length))</span><br><span class="line">      d = d.shuffle(buffer_size=<span class="number">100</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># evaluate模式就无所谓顺序和并行了</span></span><br><span class="line">      d = tf.data.TFRecordDataset(input_files)</span><br><span class="line">      <span class="comment"># Since we evaluate for a fixed number of steps we don't want to encounter</span></span><br><span class="line">      <span class="comment"># out-of-range exceptions.</span></span><br><span class="line">      d = d.repeat()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We must `drop_remainder` on training because the TPU requires fixed</span></span><br><span class="line">    <span class="comment"># size dimensions. For eval, we assume we are evaluating on the CPU or GPU</span></span><br><span class="line">    <span class="comment"># and we *don't* want to drop the remainder, otherwise we wont cover</span></span><br><span class="line">    <span class="comment"># every sample.</span></span><br><span class="line">    d = d.apply(</span><br><span class="line">        <span class="comment"># 这个方法相当于先map，然后batch</span></span><br><span class="line">        tf.contrib.data.map_and_batch(</span><br><span class="line">            <span class="keyword">lambda</span> record: _decode_record(record, name_to_features),</span><br><span class="line">            batch_size=batch_size,</span><br><span class="line">            num_parallel_batches=num_cpu_threads,</span><br><span class="line">            drop_remainder=<span class="literal">True</span>))</span><br><span class="line">    <span class="comment"># 将解析后的值分成多组batch，作为模型的输入数据(model_fn中的features)。</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> input_fn</span><br></pre></td></tr></table></figure><h3 id="model-fn-builder"><a href="#model-fn-builder" class="headerlink" title="model_fn_builder"></a>model_fn_builder</h3><p>用于构造 Estimator 使用的 model_fn。包含了特征提取、模型创建、计算损失、加载 checkpoint、计算 acc，并返回一个 EstimatorSpec。</p><p>定义好了<code>get_masked_lm_output</code>和<code>get_next_sentence_output</code>两个训练任务后，就可以写出训练过程，之后将训练集传入自动训练。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_fn_builder</span><span class="params">(bert_config, init_checkpoint, learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">                     num_train_steps, num_warmup_steps, use_tpu,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings)</span>:</span></span><br><span class="line">  <span class="string">"""Returns `model_fn` closure for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">model_fn</span><span class="params">(features, labels, mode, params)</span>:</span>  <span class="comment"># pylint: disable=unused-argument</span></span><br><span class="line">    <span class="string">"""The `model_fn` for TPUEstimator."""</span></span><br><span class="line"></span><br><span class="line">    tf.logging.info(<span class="string">"*** Features ***"</span>)</span><br><span class="line">    <span class="keyword">for</span> name <span class="keyword">in</span> sorted(features.keys()):</span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s"</span> % (name, features[name].shape))</span><br><span class="line"></span><br><span class="line">    input_ids = features[<span class="string">"input_ids"</span>]</span><br><span class="line">    input_mask = features[<span class="string">"input_mask"</span>]</span><br><span class="line">    segment_ids = features[<span class="string">"segment_ids"</span>]</span><br><span class="line">    masked_lm_positions = features[<span class="string">"masked_lm_positions"</span>]</span><br><span class="line">    masked_lm_ids = features[<span class="string">"masked_lm_ids"</span>]</span><br><span class="line">    masked_lm_weights = features[<span class="string">"masked_lm_weights"</span>]</span><br><span class="line">    next_sentence_labels = features[<span class="string">"next_sentence_labels"</span>]</span><br><span class="line"></span><br><span class="line">    is_training = (mode == tf.estimator.ModeKeys.TRAIN)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 重点是这里的创建BERTModel，看下一部分的模型代码</span></span><br><span class="line">    model = modeling.BertModel(</span><br><span class="line">        config=bert_config,</span><br><span class="line">        is_training=is_training,</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        input_mask=input_mask,</span><br><span class="line">        token_type_ids=segment_ids,</span><br><span class="line">        use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算MaskedLM的损失</span></span><br><span class="line">    (masked_lm_loss,</span><br><span class="line">     masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">         bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">         masked_lm_positions, masked_lm_ids, masked_lm_weights)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算NSP的损失</span></span><br><span class="line">    (next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">     next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">         bert_config, model.get_pooled_output(), next_sentence_labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># MLM的损失和NSP损失相加</span></span><br><span class="line">    total_loss = masked_lm_loss + next_sentence_loss</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获得所有可训练的variable</span></span><br><span class="line">    tvars = tf.trainable_variables()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载checkpoint</span></span><br><span class="line">    initialized_variable_names = &#123;&#125;</span><br><span class="line">    scaffold_fn = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> init_checkpoint:</span><br><span class="line">      (assignment_map, initialized_variable_names</span><br><span class="line">      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)</span><br><span class="line">      <span class="keyword">if</span> use_tpu:</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">tpu_scaffold</span><span class="params">()</span>:</span></span><br><span class="line">          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line">          <span class="keyword">return</span> tf.train.Scaffold()</span><br><span class="line"></span><br><span class="line">        scaffold_fn = tpu_scaffold</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 打印log</span></span><br><span class="line">    tf.logging.info(<span class="string">"**** Trainable Variables ****"</span>)</span><br><span class="line">    <span class="keyword">for</span> var <span class="keyword">in</span> tvars:</span><br><span class="line">      init_string = <span class="string">""</span></span><br><span class="line">      <span class="keyword">if</span> var.name <span class="keyword">in</span> initialized_variable_names:</span><br><span class="line">        init_string = <span class="string">", *INIT_FROM_CKPT*"</span></span><br><span class="line">      tf.logging.info(<span class="string">"  name = %s, shape = %s%s"</span>, var.name, var.shape,</span><br><span class="line">                      init_string)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练过程，获得spec</span></span><br><span class="line">    output_spec = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">      <span class="comment"># 创建优化器optimizer</span></span><br><span class="line">      train_op = optimization.create_optimizer(</span><br><span class="line">          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br><span class="line"></span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          train_op=train_op,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">elif</span> mode == tf.estimator.ModeKeys.EVAL:</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 下面是计算损失和acc的函数</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">metric_fn</span><span class="params">(masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                    masked_lm_weights, next_sentence_example_loss,</span></span></span><br><span class="line"><span class="function"><span class="params">                    next_sentence_log_probs, next_sentence_labels)</span>:</span></span><br><span class="line">        <span class="string">"""Computes the loss and accuracy of the model."""</span></span><br><span class="line">        <span class="comment"># [batch_size*max_predictions_per_seq=640, vocab_size=30522]</span></span><br><span class="line">        masked_lm_log_probs = tf.reshape(masked_lm_log_probs,</span><br><span class="line">                                         [<span class="number">-1</span>, masked_lm_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        masked_lm_predictions = tf.argmax(</span><br><span class="line">            masked_lm_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        <span class="comment"># [batch_size*max_predictions_per_seq=640, ]</span></span><br><span class="line">        masked_lm_example_loss = tf.reshape(masked_lm_example_loss, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_ids = tf.reshape(masked_lm_ids, [<span class="number">-1</span>])</span><br><span class="line">        masked_lm_weights = tf.reshape(masked_lm_weights, [<span class="number">-1</span>])</span><br><span class="line">        <span class="comment"># tf.metrics.accuracy返回两个值，accuracy为到上一个batch为止的准确度，</span></span><br><span class="line">        <span class="comment"># update_op为更新本批次后的准确度。</span></span><br><span class="line">        <span class="comment"># masked_lm_weights用于标记哪些是真实值哪些是补全值</span></span><br><span class="line">        masked_lm_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=masked_lm_ids,</span><br><span class="line">            predictions=masked_lm_predictions,</span><br><span class="line">            weights=masked_lm_weights)</span><br><span class="line">        <span class="comment"># 就是per_example_loss交叉熵损失</span></span><br><span class="line">        masked_lm_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=masked_lm_example_loss, weights=masked_lm_weights)</span><br><span class="line"></span><br><span class="line">        next_sentence_log_probs = tf.reshape(</span><br><span class="line">            next_sentence_log_probs, [<span class="number">-1</span>, next_sentence_log_probs.shape[<span class="number">-1</span>]])</span><br><span class="line">        next_sentence_predictions = tf.argmax(</span><br><span class="line">            next_sentence_log_probs, axis=<span class="number">-1</span>, output_type=tf.int32)</span><br><span class="line">        next_sentence_labels = tf.reshape(next_sentence_labels, [<span class="number">-1</span>])</span><br><span class="line">        next_sentence_accuracy = tf.metrics.accuracy(</span><br><span class="line">            labels=next_sentence_labels, predictions=next_sentence_predictions)</span><br><span class="line">        next_sentence_mean_loss = tf.metrics.mean(</span><br><span class="line">            values=next_sentence_example_loss)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">            <span class="string">"masked_lm_accuracy"</span>: masked_lm_accuracy,</span><br><span class="line">            <span class="string">"masked_lm_loss"</span>: masked_lm_mean_loss,</span><br><span class="line">            <span class="string">"next_sentence_accuracy"</span>: next_sentence_accuracy,</span><br><span class="line">            <span class="string">"next_sentence_loss"</span>: next_sentence_mean_loss,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      eval_metrics = (metric_fn, [</span><br><span class="line">          masked_lm_example_loss, masked_lm_log_probs, masked_lm_ids,</span><br><span class="line">          masked_lm_weights, next_sentence_example_loss,</span><br><span class="line">          next_sentence_log_probs, next_sentence_labels</span><br><span class="line">      ])</span><br><span class="line">      output_spec = tf.contrib.tpu.TPUEstimatorSpec(</span><br><span class="line">          mode=mode,</span><br><span class="line">          loss=total_loss,</span><br><span class="line">          eval_metrics=eval_metrics,</span><br><span class="line">          scaffold_fn=scaffold_fn)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"Only TRAIN and EVAL modes are supported: %s"</span> % (mode))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output_spec</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> model_fn</span><br></pre></td></tr></table></figure><p>基于上述搭建好的模型结构及相应的损失函数，在训练阶段，利用相应的优化器(AdamWeightDecayOptimizer)优化损失函数，使其减小，并保存不同训练步数对应的模型参数，直到跑完所有步数，从而确定最终的模型结构与参数。</p><p>从这里引出几个问题：</p><ol><li>Bert 模型的创建，见<a href="#bertmodel">model = modeling.BertModel(···)</a></li><li>计算 MaskedLM 的损失，见<a href="#get_masked_lm_output">(masked_lm_loss, masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output()</a></li><li>计算 NSP 的损失，见<a href="#get_next_sentence_output">(next_sentence_loss, next_sentence_example_loss, next_sentence_log_probs) = get_next_sentence_output()</a></li><li>创建优化器，用来更新模型(权重)参数，见<a href="#create_optimizer">create_optimizer()</a></li></ol><h3 id="文件说明"><a href="#文件说明" class="headerlink" title="文件说明"></a>文件说明</h3><p>由于 BERT 在预训练中使用了 estimator 这种高级 API 形式，在训练完成后会自动生成 ckpt 格式的模型文件(结构和数据是分开的) 及可供 tensorboard 查看的事件文件。具体文件说明如下：</p><ol><li><code>checkpoint</code>: 记录了模型文件的路径信息列表，可以用来迅速查找最近一次的 ckpt 文件。(每个 ckpt 文件对应一个模型)其内容如下所示<ul><li>model_checkpoint_path: “model.ckpt-20”</li><li>all_model_checkpoint_paths: “model.ckpt-0”</li><li>all_model_checkpoint_paths: “model.ckpt-20”</li></ul></li><li><code>events.out.tfevents.1570029823.04c93f97d224</code>：事件文件，tensorboard 可加载显示</li><li><code>graph.pbtxt</code> : 以 Protobuffer 格式描述的模型结构文件(text 格式的图文件(.pbtext),二进制格式的图文件为(.pb))，记录了模型中所有的节点信息，内容大致如下：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">node &#123;</span><br><span class="line">  name: <span class="string">"global_step/Initializer/zeros"</span></span><br><span class="line">  op: <span class="string">"Const"</span></span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"_class"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        s: <span class="string">"loc:@global_step"</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"_output_shapes"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      list &#123;</span><br><span class="line">        shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"dtype"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      type: DT_INT64</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  attr &#123;</span><br><span class="line">    key: <span class="string">"value"</span></span><br><span class="line">    value &#123;</span><br><span class="line">      tensor &#123;</span><br><span class="line">        dtype: DT_INT64</span><br><span class="line">        tensor_shape &#123;</span><br><span class="line">        &#125;</span><br><span class="line">        int64_val: <span class="number">0</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li><code>model.ckpt-20.data-00000-of-00001</code> : 模型文件中的数据(the values of all variables)部分 (二进制文件)</li><li><code>model.ckpt-20.index</code> : 模型文件中的映射表( Each key is a name of a tensor and its value is a serialized BundleEntryProto. Each BundleEntryProto describes the metadata of a tensor: which of the “data” files contains the content of a tensor, the offset into that file, checksum, some auxiliary data, etc.)部分 (二进制文件)</li><li><code>model.ckpt-20.meta</code> : 模型文件中的(图)结构(由 GraphDef, SaverDef, MateInfoDef,SignatureDef,CollectionDef 等组成的 MetaGraphDef)部分 (二进制文件，内容和 graph.pbtxt 基本一样，其是一个序列化的 MetaGraphDef protocol buffer)</li></ol><p>在评估阶段，直接加载训练好的模型结构与参数，对预测样本进行预测即可。</p><h3 id="BertModel"><a href="#BertModel" class="headerlink" title="BertModel"></a>BertModel</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BertModel</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">"""BERT model ("Bidirectional Encoder Representations from Transformers").</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Example usage:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  # python</span></span><br><span class="line"><span class="string">  # Already been converted into WordPiece token ids</span></span><br><span class="line"><span class="string">  input_ids = tf.constant([[31, 51, 99], [15, 5, 0]])</span></span><br><span class="line"><span class="string">  input_mask = tf.constant([[1, 1, 1], [1, 1, 0]])</span></span><br><span class="line"><span class="string">  token_type_ids = tf.constant([[0, 0, 1], [0, 2, 0]])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  config = modeling.BertConfig(vocab_size=32000, hidden_size=512,</span></span><br><span class="line"><span class="string">    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  model = modeling.BertModel(config=config, is_training=True,</span></span><br><span class="line"><span class="string">    input_ids=input_ids, input_mask=input_mask, token_type_ids=token_type_ids)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  label_embeddings = tf.get_variable(...)</span></span><br><span class="line"><span class="string">  pooled_output = model.get_pooled_output()</span></span><br><span class="line"><span class="string">  logits = tf.matmul(pooled_output, label_embeddings)</span></span><br><span class="line"><span class="string">  ...</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               config,</span></span></span><br><span class="line"><span class="function"><span class="params">               is_training,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">               input_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               use_one_hot_embeddings=False,</span></span></span><br><span class="line"><span class="function"><span class="params">               scope=None)</span>:</span></span><br><span class="line">    <span class="string">"""Constructor for BertModel.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      config: `BertConfig` instance.</span></span><br><span class="line"><span class="string">      is_training: bool. true for training model, false for eval model. Controls</span></span><br><span class="line"><span class="string">        whether dropout will be applied.</span></span><br><span class="line"><span class="string">      input_ids: int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word</span></span><br><span class="line"><span class="string">        embeddings or tf.embedding_lookup() for the word embeddings.</span></span><br><span class="line"><span class="string">      scope: (optional) variable scope. Defaults to "bert".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: The config is invalid or one of the input tensor shapes</span></span><br><span class="line"><span class="string">        is invalid.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    config = copy.deepcopy(config)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_training:</span><br><span class="line">      config.hidden_dropout_prob = <span class="number">0.0</span></span><br><span class="line">      config.attention_probs_dropout_prob = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># get_shape_list</span></span><br><span class="line">    <span class="comment"># Returns a list of the shape of tensor, preferring static dimensions.</span></span><br><span class="line">    input_shape = get_shape_list(input_ids, expected_rank=<span class="number">2</span>)</span><br><span class="line">    batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">    seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> input_mask <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      input_mask = tf.ones(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      token_type_ids = tf.zeros(shape=[batch_size, seq_length], dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope, default_name=<span class="string">"bert"</span>):</span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"embeddings"</span>):</span><br><span class="line">        <span class="comment"># Perform embedding lookup on the word ids.</span></span><br><span class="line">        <span class="comment"># embedding_output 是input_ids对应的embedding输出</span></span><br><span class="line">        <span class="comment"># embedding_table就是整个的embedding table</span></span><br><span class="line">        (self.embedding_output, self.embedding_table) = embedding_lookup(</span><br><span class="line">            input_ids=input_ids,</span><br><span class="line">            vocab_size=config.vocab_size,</span><br><span class="line">            embedding_size=config.hidden_size,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            word_embedding_name=<span class="string">"word_embeddings"</span>,</span><br><span class="line">            use_one_hot_embeddings=use_one_hot_embeddings)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Add positional embeddings and token type embeddings, then layer</span></span><br><span class="line">        <span class="comment"># normalize and perform dropout.</span></span><br><span class="line">        <span class="comment"># 上面得到的embedding输出，先加上token_type_embedding，</span></span><br><span class="line">        <span class="comment"># 然后加上position_embedding。shape相同，对应位置相加</span></span><br><span class="line">        <span class="comment"># token_type_embedding和position_embedding都是通过构建一个look up table得到</span></span><br><span class="line">        <span class="comment"># 最后进行layer_norm_and_dropout</span></span><br><span class="line">        self.embedding_output = embedding_postprocessor(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            use_token_type=<span class="literal">True</span>,</span><br><span class="line">            token_type_ids=token_type_ids,</span><br><span class="line">            token_type_vocab_size=config.type_vocab_size,</span><br><span class="line">            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span><br><span class="line">            use_position_embeddings=<span class="literal">True</span>,</span><br><span class="line">            position_embedding_name=<span class="string">"position_embeddings"</span>,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            max_position_embeddings=config.max_position_embeddings,</span><br><span class="line">            dropout_prob=config.hidden_dropout_prob)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"encoder"</span>):</span><br><span class="line">        <span class="comment"># This converts a 2D mask of shape [batch_size, seq_length] to a 3D</span></span><br><span class="line">        <span class="comment"># mask of shape [batch_size, seq_length, seq_length] which is used</span></span><br><span class="line">        <span class="comment"># for the attention scores.</span></span><br><span class="line">        <span class="comment"># input_ids, input_mask: (32, 128)</span></span><br><span class="line">        <span class="comment"># attention_mask: (32, 128, 128)</span></span><br><span class="line">        attention_mask = create_attention_mask_from_input_mask(</span><br><span class="line">            input_ids, input_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run the stacked transformer.</span></span><br><span class="line">        <span class="comment"># `sequence_output` shape = [batch_size, seq_length, hidden_size].</span></span><br><span class="line">        <span class="comment"># encoder部分，由num_hidden_layers(12)个transformer encoder组成的</span></span><br><span class="line">        self.all_encoder_layers = transformer_model(</span><br><span class="line">            input_tensor=self.embedding_output,</span><br><span class="line">            attention_mask=attention_mask,</span><br><span class="line">            hidden_size=config.hidden_size,</span><br><span class="line">            num_hidden_layers=config.num_hidden_layers,</span><br><span class="line">            num_attention_heads=config.num_attention_heads,</span><br><span class="line">            intermediate_size=config.intermediate_size,</span><br><span class="line">            intermediate_act_fn=get_activation(config.hidden_act),</span><br><span class="line">            hidden_dropout_prob=config.hidden_dropout_prob,</span><br><span class="line">            attention_probs_dropout_prob=config.attention_probs_dropout_prob,</span><br><span class="line">            initializer_range=config.initializer_range,</span><br><span class="line">            do_return_all_layers=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">      self.sequence_output = self.all_encoder_layers[<span class="number">-1</span>]</span><br><span class="line">      <span class="comment"># The "pooler" converts the encoded sequence tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, seq_length, hidden_size] to a tensor of shape</span></span><br><span class="line">      <span class="comment"># [batch_size, hidden_size]. This is necessary for segment-level</span></span><br><span class="line">      <span class="comment"># (or segment-pair-level) classification tasks where we need a fixed</span></span><br><span class="line">      <span class="comment"># dimensional representation of the segment.</span></span><br><span class="line">      <span class="comment"># 两种输出，一种是最后一层transformer encoder的sequence_output，</span></span><br><span class="line">      <span class="comment"># token级别的embedding，用于masked LM任务训练</span></span><br><span class="line">      <span class="comment"># 另一种输出是取sequence_output的第一个token，然后接一个带有tanh的全连接层最为输出，</span></span><br><span class="line">      <span class="comment"># 句子级别的embedding，用于NSP任务的训练</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"pooler"</span>):</span><br><span class="line">        <span class="comment"># We "pool" the model by simply taking the hidden state corresponding</span></span><br><span class="line">        <span class="comment"># to the first token. We assume that this has been pre-trained</span></span><br><span class="line">        first_token_tensor = tf.squeeze(self.sequence_output[:, <span class="number">0</span>:<span class="number">1</span>, :], axis=<span class="number">1</span>)</span><br><span class="line">        self.pooled_output = tf.layers.dense(</span><br><span class="line">            first_token_tensor,</span><br><span class="line">            config.hidden_size,</span><br><span class="line">            activation=tf.tanh,</span><br><span class="line">            kernel_initializer=create_initializer(config.initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_pooled_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.pooled_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_sequence_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets final hidden layer of encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the final hidden of the transformer encoder.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.sequence_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_all_encoder_layers</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.all_encoder_layers</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_output</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""Gets output of the embedding lookup (i.e., input to the transformer).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding</span></span><br><span class="line"><span class="string">      to the output of the embedding layer, after summing the word</span></span><br><span class="line"><span class="string">      embeddings with the positional embeddings and the token type embeddings,</span></span><br><span class="line"><span class="string">      then performing layer normalization. This is the input to the transformer.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_output</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">get_embedding_table</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.embedding_table</span><br></pre></td></tr></table></figure><p>这里引出几个部分：</p><ol><li>embedding 表的构建，即<a href="#embedding_lookup">embedding_lookup</a></li><li>在 word_embeddings 的基础上增加 segment_id 和 position 信息，最后将叠加后 embedding 分别进行 layer_norm，batch_norm 和 dropout 操作。见<a href="#embedding_postprocessor">embedding_postprocessor</a></li><li>transformer 模型的构建，即<a href="#transformer_model">transformer_model</a></li><li>attention 自注意力层的构建，即<a href="#attention_layer">attention_layer</a></li></ol><h3 id="embedding-lookup"><a href="#embedding-lookup" class="headerlink" title="embedding_lookup"></a>embedding_lookup</h3><p>构建一个 embedding lookup 表，用于生成每个 token 的表示，同时返回 input_ids 对应的 embedding。</p><p>这里的 embedding 只包括 word_embedding，token embedding 和 position embedding 在 embedding_postprocessor 中处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_lookup</span><span class="params">(input_ids,</span></span></span><br><span class="line"><span class="function"><span class="params">                     vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">                     embedding_size=<span class="number">128</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     word_embedding_name=<span class="string">"word_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                     use_one_hot_embeddings=False)</span>:</span></span><br><span class="line">  <span class="string">"""Looks up words embeddings for id tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word</span></span><br><span class="line"><span class="string">      ids.</span></span><br><span class="line"><span class="string">    vocab_size: int. Size of the embedding vocabulary.</span></span><br><span class="line"><span class="string">    embedding_size: int. Width of the word embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Embedding initialization range.</span></span><br><span class="line"><span class="string">    word_embedding_name: string. Name of the embedding table.</span></span><br><span class="line"><span class="string">    use_one_hot_embeddings: bool. If True, use one-hot method for word</span></span><br><span class="line"><span class="string">      embeddings. If False, use `tf.gather()`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, embedding_size].</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># This function assumes that the input is of shape [batch_size, seq_length,</span></span><br><span class="line">  <span class="comment"># num_inputs].</span></span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="comment"># If the input is a 2D tensor of shape [batch_size, seq_length], we</span></span><br><span class="line">  <span class="comment"># reshape to [batch_size, seq_length, 1].</span></span><br><span class="line">  <span class="keyword">if</span> input_ids.shape.ndims == <span class="number">2</span>:</span><br><span class="line">    input_ids = tf.expand_dims(input_ids, axis=[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">  embedding_table = tf.get_variable(</span><br><span class="line">      name=word_embedding_name,</span><br><span class="line">      shape=[vocab_size, embedding_size],</span><br><span class="line">      initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># flat_input_ids shape: (4096, )</span></span><br><span class="line">  flat_input_ids = tf.reshape(input_ids, [<span class="number">-1</span>])</span><br><span class="line">  <span class="keyword">if</span> use_one_hot_embeddings:</span><br><span class="line">    one_hot_input_ids = tf.one_hot(flat_input_ids, depth=vocab_size)</span><br><span class="line">    output = tf.matmul(one_hot_input_ids, embedding_table)</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    output = tf.gather(embedding_table, flat_input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># input_shape = [32, 128, 1]</span></span><br><span class="line">  input_shape = get_shape_list(input_ids)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># output shape: [4096=32*128, 512] -&gt; [batch, seq_len, embedding_size=512]</span></span><br><span class="line">  output = tf.reshape(output,</span><br><span class="line">                      input_shape[<span class="number">0</span>:<span class="number">-1</span>] + [input_shape[<span class="number">-1</span>] * embedding_size])</span><br><span class="line">  <span class="keyword">return</span> (output, embedding_table)</span><br></pre></td></tr></table></figure><h3 id="embedding-postprocessor"><a href="#embedding-postprocessor" class="headerlink" title="embedding_postprocessor"></a>embedding_postprocessor</h3><p>在 word_embeddings 的基础上增加 segment_id 和 position 信息，最后将叠加后 embedding 分别进行 layer_norm(对每个样本的不同维度进行归一化操作)，batch_norm(是对不同样本的同一特征进行归一化操作)和 dropout(一个张量中某几个位置的值变成 0)操作。</p><p>token_type_table 与 full_position_embeddings 为模型待学习参数。它们和 word_embedding 是对应位置相加，不改变 shape</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">embedding_postprocessor</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_token_type=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_ids=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_vocab_size=<span class="number">16</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            token_type_embedding_name=<span class="string">"token_type_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            use_position_embeddings=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                            position_embedding_name=<span class="string">"position_embeddings"</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            max_position_embeddings=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                            dropout_prob=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">  <span class="string">"""Performs various post-processing on a word embedding tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      embedding_size].</span></span><br><span class="line"><span class="string">    use_token_type: bool. Whether to add embeddings for `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].</span></span><br><span class="line"><span class="string">      Must be specified if `use_token_type` is True.</span></span><br><span class="line"><span class="string">    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.</span></span><br><span class="line"><span class="string">    token_type_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for token type ids.</span></span><br><span class="line"><span class="string">    use_position_embeddings: bool. Whether to add position embeddings for the</span></span><br><span class="line"><span class="string">      position of each token in the sequence.</span></span><br><span class="line"><span class="string">    position_embedding_name: string. The name of the embedding table variable</span></span><br><span class="line"><span class="string">      for positional embeddings.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initialization.</span></span><br><span class="line"><span class="string">    max_position_embeddings: int. Maximum sequence length that might ever be</span></span><br><span class="line"><span class="string">      used with this model. This can be longer than the sequence length of</span></span><br><span class="line"><span class="string">      input_tensor, but cannot be shorter.</span></span><br><span class="line"><span class="string">    dropout_prob: float. Dropout probability applied to the final output tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float tensor with same shape as `input_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: One of the tensor shapes or input values is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  output = input_tensor</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_token_type:</span><br><span class="line">    <span class="keyword">if</span> token_type_ids <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">"`token_type_ids` must be specified if"</span></span><br><span class="line">                       <span class="string">"`use_token_type` is True."</span>)</span><br><span class="line">    token_type_table = tf.get_variable(</span><br><span class="line">        name=token_type_embedding_name,</span><br><span class="line">        shape=[token_type_vocab_size, width],</span><br><span class="line">        initializer=create_initializer(initializer_range))</span><br><span class="line">    <span class="comment"># This vocab will be small so we always do one-hot here, since it is always</span></span><br><span class="line">    <span class="comment"># faster for a small vocabulary.</span></span><br><span class="line">    flat_token_type_ids = tf.reshape(token_type_ids, [<span class="number">-1</span>])</span><br><span class="line">    one_hot_ids = tf.one_hot(flat_token_type_ids, depth=token_type_vocab_size)</span><br><span class="line">    token_type_embeddings = tf.matmul(one_hot_ids, token_type_table)</span><br><span class="line">    token_type_embeddings = tf.reshape(token_type_embeddings,</span><br><span class="line">                                       [batch_size, seq_length, width])</span><br><span class="line">    output += token_type_embeddings</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_position_embeddings:</span><br><span class="line">    assert_op = tf.assert_less_equal(seq_length, max_position_embeddings)</span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([assert_op]):</span><br><span class="line">      full_position_embeddings = tf.get_variable(</span><br><span class="line">          name=position_embedding_name,</span><br><span class="line">          shape=[max_position_embeddings, width],</span><br><span class="line">          initializer=create_initializer(initializer_range))</span><br><span class="line">      <span class="comment"># Since the position embedding table is a learned variable, we create it</span></span><br><span class="line">      <span class="comment"># using a (long) sequence length `max_position_embeddings`. The actual</span></span><br><span class="line">      <span class="comment"># sequence length might be shorter than this, for faster training of</span></span><br><span class="line">      <span class="comment"># tasks that do not have long sequences.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># So `full_position_embeddings` is effectively an embedding table</span></span><br><span class="line">      <span class="comment"># for position [0, 1, 2, ..., max_position_embeddings-1], and the current</span></span><br><span class="line">      <span class="comment"># sequence has positions [0, 1, 2, ... seq_length-1], so we can just</span></span><br><span class="line">      <span class="comment"># perform a slice.</span></span><br><span class="line">      position_embeddings = tf.slice(full_position_embeddings, [<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                                     [seq_length, <span class="number">-1</span>])</span><br><span class="line">      num_dims = len(output.shape.as_list())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Only the last two dimensions are relevant (`seq_length` and `width`), so</span></span><br><span class="line">      <span class="comment"># we broadcast among the first dimensions, which is typically just</span></span><br><span class="line">      <span class="comment"># the batch size.</span></span><br><span class="line">      position_broadcast_shape = []</span><br><span class="line">      <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_dims - <span class="number">2</span>):</span><br><span class="line">        position_broadcast_shape.append(<span class="number">1</span>)</span><br><span class="line">      position_broadcast_shape.extend([seq_length, width])</span><br><span class="line">      position_embeddings = tf.reshape(position_embeddings,</span><br><span class="line">                                       position_broadcast_shape)</span><br><span class="line">      output += position_embeddings</span><br><span class="line"></span><br><span class="line">  output = layer_norm_and_dropout(output, dropout_prob)</span><br><span class="line">  <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><h3 id="transformer-model"><a href="#transformer-model" class="headerlink" title="transformer_model"></a>transformer_model</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transformer_model</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_size=<span class="number">768</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_hidden_layers=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      num_attention_heads=<span class="number">12</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_size=<span class="number">3072</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      intermediate_act_fn=gelu,</span></span></span><br><span class="line"><span class="function"><span class="params">                      hidden_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      attention_probs_dropout_prob=<span class="number">0.1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                      do_return_all_layers=False)</span>:</span></span><br><span class="line">  <span class="string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is almost an exact implementation of the original Transformer encoder.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  See the original paper:</span></span><br><span class="line"><span class="string">  https://arxiv.org/abs/1706.03762</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Also see:</span></span><br><span class="line"><span class="string">  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].</span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,</span></span><br><span class="line"><span class="string">      seq_length], with 1 for positions that can be attended to and 0 in</span></span><br><span class="line"><span class="string">      positions that should not be.</span></span><br><span class="line"><span class="string">    hidden_size: int. Hidden size of the Transformer.</span></span><br><span class="line"><span class="string">    num_hidden_layers: int. Number of layers (blocks) in the Transformer.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads in the Transformer.</span></span><br><span class="line"><span class="string">    intermediate_size: int. The size of the "intermediate" (a.k.a., feed</span></span><br><span class="line"><span class="string">      forward) layer.</span></span><br><span class="line"><span class="string">    intermediate_act_fn: function. The non-linear activation function to apply</span></span><br><span class="line"><span class="string">      to the output of the intermediate/feed-forward layer.</span></span><br><span class="line"><span class="string">    hidden_dropout_prob: float. Dropout probability for the hidden layers.</span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: float. Dropout probability of the attention</span></span><br><span class="line"><span class="string">      probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the initializer (stddev of truncated</span></span><br><span class="line"><span class="string">      normal).</span></span><br><span class="line"><span class="string">    do_return_all_layers: Whether to also return all layers or just the final</span></span><br><span class="line"><span class="string">      layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, seq_length, hidden_size], the final</span></span><br><span class="line"><span class="string">    hidden layer of the Transformer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: A Tensor shape or parameter is invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># hidden size需要是注意力头个数的倍数</span></span><br><span class="line">  <span class="keyword">if</span> hidden_size % num_attention_heads != <span class="number">0</span>:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The hidden size (%d) is not a multiple of the number of attention "</span></span><br><span class="line">        <span class="string">"heads (%d)"</span> % (hidden_size, num_attention_heads))</span><br><span class="line"></span><br><span class="line">  attention_head_size = int(hidden_size / num_attention_heads)</span><br><span class="line">  input_shape = get_shape_list(input_tensor, expected_rank=<span class="number">3</span>)</span><br><span class="line">  batch_size = input_shape[<span class="number">0</span>]</span><br><span class="line">  seq_length = input_shape[<span class="number">1</span>]</span><br><span class="line">  input_width = input_shape[<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">  <span class="comment"># The Transformer performs sum residuals on all layers so the input needs</span></span><br><span class="line">  <span class="comment"># to be the same as the hidden size.</span></span><br><span class="line">  <span class="keyword">if</span> input_width != hidden_size:</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"The width of the input tensor (%d) != hidden size (%d)"</span> %</span><br><span class="line">                     (input_width, hidden_size))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span></span><br><span class="line">  <span class="comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span></span><br><span class="line">  <span class="comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span></span><br><span class="line">  <span class="comment"># help the optimizer.</span></span><br><span class="line">  prev_output = reshape_to_matrix(input_tensor)</span><br><span class="line"></span><br><span class="line">  all_layer_outputs = []</span><br><span class="line">  <span class="keyword">for</span> layer_idx <span class="keyword">in</span> range(num_hidden_layers):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"layer_%d"</span> % layer_idx):</span><br><span class="line">      layer_input = prev_output</span><br><span class="line"></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"attention"</span>):</span><br><span class="line">        attention_heads = []</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"self"</span>):</span><br><span class="line">          <span class="comment"># 这里引入attention_layer，输入是上一次的输出，输出是context Z矩阵</span></span><br><span class="line">          <span class="comment"># attention_head: [B*F, N*H] if do_return_2d_tensor=True</span></span><br><span class="line">          attention_head = attention_layer(</span><br><span class="line">              from_tensor=layer_input,</span><br><span class="line">              to_tensor=layer_input,</span><br><span class="line">              attention_mask=attention_mask,</span><br><span class="line">              num_attention_heads=num_attention_heads,</span><br><span class="line">              size_per_head=attention_head_size,</span><br><span class="line">              attention_probs_dropout_prob=attention_probs_dropout_prob,</span><br><span class="line">              initializer_range=initializer_range,</span><br><span class="line">              do_return_2d_tensor=<span class="literal">True</span>,</span><br><span class="line">              batch_size=batch_size,</span><br><span class="line">              from_seq_length=seq_length,</span><br><span class="line">              to_seq_length=seq_length)</span><br><span class="line">          attention_heads.append(attention_head)</span><br><span class="line"></span><br><span class="line">        attention_output = <span class="literal">None</span></span><br><span class="line">        <span class="comment"># 调试过程中确定每次len(attention_heads)都是1</span></span><br><span class="line">        <span class="keyword">if</span> len(attention_heads) == <span class="number">1</span>:</span><br><span class="line">          attention_output = attention_heads[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># In the case where we have other sequences, we just concatenate</span></span><br><span class="line">          <span class="comment"># them to the self-attention head before the projection.</span></span><br><span class="line">          <span class="comment"># 把所有的Z拼接在一起，然后使用Wo线性变换</span></span><br><span class="line">          attention_output = tf.concat(attention_heads, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run a linear projection of `hidden_size` then add a residual</span></span><br><span class="line">        <span class="comment"># with `layer_input`.</span></span><br><span class="line">        <span class="comment"># 上面是self-attention部分，下面是和layer_input进行残差连接</span></span><br><span class="line">        <span class="comment"># 使用线性变换到hidden_size维，然后dropout，</span></span><br><span class="line">        <span class="comment"># 然后加上layer_input之后layer_norm（对一个样本的特征进行归一化）</span></span><br><span class="line">        <span class="comment"># [B*F, N*H] -&gt; [B*F, hidden_size]</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">          attention_output = tf.layers.dense(</span><br><span class="line">              attention_output,</span><br><span class="line">              hidden_size,</span><br><span class="line">              kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">          attention_output = dropout(attention_output, hidden_dropout_prob)</span><br><span class="line">          attention_output = layer_norm(attention_output + layer_input)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># The activation is only applied to the "intermediate" hidden layer.</span></span><br><span class="line">      <span class="comment"># 再进行一个带激活函数的线性变换，激活函数只用在这，用的是smooth版的relu：gelu</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"intermediate"</span>):</span><br><span class="line">        intermediate_output = tf.layers.dense(</span><br><span class="line">            attention_output,</span><br><span class="line">            intermediate_size,</span><br><span class="line">            activation=intermediate_act_fn,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Down-project back to `hidden_size` then add the residual.</span></span><br><span class="line">      <span class="comment"># 再次有一个layer_output和attention_output的残差连接</span></span><br><span class="line">      <span class="keyword">with</span> tf.variable_scope(<span class="string">"output"</span>):</span><br><span class="line">        layer_output = tf.layers.dense(</span><br><span class="line">            intermediate_output,</span><br><span class="line">            hidden_size,</span><br><span class="line">            kernel_initializer=create_initializer(initializer_range))</span><br><span class="line">        layer_output = dropout(layer_output, hidden_dropout_prob)</span><br><span class="line">        layer_output = layer_norm(layer_output + attention_output)</span><br><span class="line">        prev_output = layer_output</span><br><span class="line">        all_layer_outputs.append(layer_output)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># bert的配置是True，返回所有层</span></span><br><span class="line">  <span class="keyword">if</span> do_return_all_layers:</span><br><span class="line">    final_outputs = []</span><br><span class="line">    <span class="keyword">for</span> layer_output <span class="keyword">in</span> all_layer_outputs:</span><br><span class="line">      <span class="comment"># [4096, 512] -&gt; [32, 128, 512]</span></span><br><span class="line">      final_output = reshape_from_matrix(layer_output, input_shape)</span><br><span class="line">      final_outputs.append(final_output)</span><br><span class="line">    <span class="keyword">return</span> final_outputs</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    final_output = reshape_from_matrix(prev_output, input_shape)</span><br><span class="line">    <span class="keyword">return</span> final_output</span><br></pre></td></tr></table></figure><h3 id="attention-layer"><a href="#attention-layer" class="headerlink" title="attention_layer"></a>attention_layer</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">attention_layer</span><span class="params">(from_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_mask=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    num_attention_heads=<span class="number">1</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    size_per_head=<span class="number">512</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    query_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    key_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    value_act=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    attention_probs_dropout_prob=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    initializer_range=<span class="number">0.02</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                    do_return_2d_tensor=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                    batch_size=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    from_seq_length=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                    to_seq_length=None)</span>:</span></span><br><span class="line">  <span class="string">"""Performs multi-headed attention from `from_tensor` to `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This is an implementation of multi-headed attention based on "Attention</span></span><br><span class="line"><span class="string">  is all you Need". If `from_tensor` and `to_tensor` are the same, then</span></span><br><span class="line"><span class="string">  this is self-attention. Each timestep in `from_tensor` attends to the</span></span><br><span class="line"><span class="string">  corresponding sequence in `to_tensor`, and returns a fixed-with vector.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  This function first projects `from_tensor` into a "query" tensor and</span></span><br><span class="line"><span class="string">  `to_tensor` into "key" and "value" tensors. These are (effectively) a list</span></span><br><span class="line"><span class="string">  of tensors of length `num_attention_heads`, where each tensor is of shape</span></span><br><span class="line"><span class="string">  [batch_size, seq_length, size_per_head].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Then, the query and key tensors are dot-producted and scaled. These are</span></span><br><span class="line"><span class="string">  softmaxed to obtain attention probabilities. The value tensors are then</span></span><br><span class="line"><span class="string">  interpolated by these probabilities, then concatenated back to a single</span></span><br><span class="line"><span class="string">  tensor and returned.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In practice, the multi-headed attention are done with transposes and</span></span><br><span class="line"><span class="string">  reshapes rather than actual separate tensors.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Args:</span></span><br><span class="line"><span class="string">    from_tensor: float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      from_width].</span></span><br><span class="line"><span class="string">    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    attention_mask: (optional) int32 Tensor of shape [batch_size,</span></span><br><span class="line"><span class="string">      from_seq_length, to_seq_length]. The values should be 1 or 0. The</span></span><br><span class="line"><span class="string">      attention scores will effectively be set to -infinity for any positions in</span></span><br><span class="line"><span class="string">      the mask that are 0, and will be unchanged for positions that are 1.</span></span><br><span class="line"><span class="string">    num_attention_heads: int. Number of attention heads.</span></span><br><span class="line"><span class="string">    size_per_head: int. Size of each attention head. 传入的是 int(hidden_size / num_attention_heads)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    query_act: (optional) Activation function for the query transform.</span></span><br><span class="line"><span class="string">    key_act: (optional) Activation function for the key transform.</span></span><br><span class="line"><span class="string">    value_act: (optional) Activation function for the value transform.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    attention_probs_dropout_prob: (optional) float. Dropout probability of the</span></span><br><span class="line"><span class="string">      attention probabilities.</span></span><br><span class="line"><span class="string">    initializer_range: float. Range of the weight initializer.</span></span><br><span class="line"><span class="string">    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size</span></span><br><span class="line"><span class="string">      * from_seq_length, num_attention_heads * size_per_head]. If False, the</span></span><br><span class="line"><span class="string">      output will be of shape [batch_size, from_seq_length, num_attention_heads</span></span><br><span class="line"><span class="string">      * size_per_head].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    batch_size: (Optional) int. If the input is 2D, this might be the batch size</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor` and `to_tensor`.</span></span><br><span class="line"><span class="string">    from_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `from_tensor`.</span></span><br><span class="line"><span class="string">    to_seq_length: (Optional) If the input is 2D, this might be the seq length</span></span><br><span class="line"><span class="string">      of the 3D version of the `to_tensor`.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns:</span></span><br><span class="line"><span class="string">    float Tensor of shape [batch_size, from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is</span></span><br><span class="line"><span class="string">      true, this will be of shape [batch_size * from_seq_length,</span></span><br><span class="line"><span class="string">      num_attention_heads * size_per_head]).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Raises:</span></span><br><span class="line"><span class="string">    ValueError: Any of the arguments or tensor shapes are invalid.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">transpose_for_scores</span><span class="params">(input_tensor, batch_size, num_attention_heads,</span></span></span><br><span class="line"><span class="function"><span class="params">                           seq_length, width)</span>:</span></span><br><span class="line">    output_tensor = tf.reshape(</span><br><span class="line">        input_tensor, [batch_size, seq_length, num_attention_heads, width])</span><br><span class="line"></span><br><span class="line">    output_tensor = tf.transpose(output_tensor, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">    <span class="keyword">return</span> output_tensor</span><br><span class="line"></span><br><span class="line">  from_shape = get_shape_list(from_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">  to_shape = get_shape_list(to_tensor, expected_rank=[<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) != len(to_shape):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(</span><br><span class="line">        <span class="string">"The rank of `from_tensor` must match the rank of `to_tensor`."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> len(from_shape) == <span class="number">3</span>:</span><br><span class="line">    batch_size = from_shape[<span class="number">0</span>]</span><br><span class="line">    from_seq_length = from_shape[<span class="number">1</span>]</span><br><span class="line">    to_seq_length = to_shape[<span class="number">1</span>]</span><br><span class="line">  <span class="keyword">elif</span> len(from_shape) == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">if</span> (batch_size <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> from_seq_length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> to_seq_length <span class="keyword">is</span> <span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">raise</span> ValueError(</span><br><span class="line">          <span class="string">"When passing in rank 2 tensors to attention_layer, the values "</span></span><br><span class="line">          <span class="string">"for `batch_size`, `from_seq_length`, and `to_seq_length` "</span></span><br><span class="line">          <span class="string">"must all be specified."</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Scalar dimensions referenced here:</span></span><br><span class="line">  <span class="comment">#   B = batch size (number of sequences)</span></span><br><span class="line">  <span class="comment">#   F = `from_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   T = `to_tensor` sequence length</span></span><br><span class="line">  <span class="comment">#   N = `num_attention_heads`</span></span><br><span class="line">  <span class="comment">#   H = `size_per_head`</span></span><br><span class="line">  <span class="comment">#   N * H 就相当于hidden size</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># from_tensor or to tensor: rank &gt;=2 -&gt; [batch * seq_len, width]</span></span><br><span class="line">  <span class="comment"># 相当于batch * seq_len的二维按行顺序排列成了一维</span></span><br><span class="line">  from_tensor_2d = reshape_to_matrix(from_tensor)</span><br><span class="line">  to_tensor_2d = reshape_to_matrix(to_tensor)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 下面是计算Q、K和V，实现不是原本的三维矩阵乘，而是压缩成了二维</span></span><br><span class="line">  <span class="comment"># 得到的结果相当于是每个attention-head的Q按照列拼接在一起，所以输出的unit</span></span><br><span class="line">  <span class="comment"># 是num_attention_heads*size_per_head，每个输出节点都有一组权重，相当于W^Q的一列</span></span><br><span class="line">  <span class="comment"># 返回的是一个tensor</span></span><br><span class="line">  <span class="comment"># `query_layer` = [B*F, width]-&gt;[B*F, N*H]</span></span><br><span class="line">  query_layer = tf.layers.dense(</span><br><span class="line">      from_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=query_act,</span><br><span class="line">      name=<span class="string">"query"</span>,</span><br><span class="line">      <span class="comment"># kernel_initializer用于初始化权重w</span></span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B*T, N*H]</span></span><br><span class="line">  key_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=key_act,</span><br><span class="line">      name=<span class="string">"key"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B*T, N*H]</span></span><br><span class="line">  value_layer = tf.layers.dense(</span><br><span class="line">      to_tensor_2d,</span><br><span class="line">      num_attention_heads * size_per_head,</span><br><span class="line">      activation=value_act,</span><br><span class="line">      name=<span class="string">"value"</span>,</span><br><span class="line">      kernel_initializer=create_initializer(initializer_range))</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `query_layer` = [B, N, F, H]</span></span><br><span class="line">  <span class="comment"># 先reshape到[B, F, N, H]，然后transpose到[B, N, F, H]</span></span><br><span class="line">  query_layer = transpose_for_scores(query_layer, batch_size,</span><br><span class="line">                                     num_attention_heads, from_seq_length,</span><br><span class="line">                                     size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `key_layer` = [B, N, T, H]</span></span><br><span class="line">  key_layer = transpose_for_scores(key_layer, batch_size, num_attention_heads,</span><br><span class="line">                                   to_seq_length, size_per_head)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">  <span class="comment"># attention scores.</span></span><br><span class="line">  <span class="comment"># `attention_scores` = [B, N, F, T]</span></span><br><span class="line">  <span class="comment"># transpose_b=True表示b=key_layer在乘法之前进行转置，其实是最后两个维度进行转置</span></span><br><span class="line">  <span class="comment"># tf.matmul()在高维矩阵乘法中，其实是对高维矩阵的每个二维矩阵相乘</span></span><br><span class="line">  <span class="comment"># 所以转制后变成[B, N, F, H] * [B, N, H, T] =&gt; [B, N, F, T]</span></span><br><span class="line">  attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># size_per_head就是score除以的\sqrt(d_k)</span></span><br><span class="line">  attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> attention_mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="comment"># `attention_mask` = [B, 1, F, T]</span></span><br><span class="line">    attention_mask = tf.expand_dims(attention_mask, axis=[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since attention_mask is 1.0 for positions we want to attend and 0.0 for</span></span><br><span class="line">    <span class="comment"># masked positions, this operation will create a tensor which is 0.0 for</span></span><br><span class="line">    <span class="comment"># positions we want to attend and -10000.0 for masked positions.</span></span><br><span class="line">    <span class="comment"># 如果mask显示为1，那么不改变score，否则会变得很小。这个mask是attention的可视域</span></span><br><span class="line">    <span class="comment"># 这个mask追溯上去得到的是全1的矩阵，对padding部分进行mask</span></span><br><span class="line">    adder = (<span class="number">1.0</span> - tf.cast(attention_mask, tf.float32)) * <span class="number">-10000.0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Since we are adding it to the raw scores before the softmax, this is</span></span><br><span class="line">    <span class="comment"># effectively the same as removing these entirely.</span></span><br><span class="line">    attention_scores += adder</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line">  <span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">  attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line">  <span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">  <span class="comment"># base bert_config中attention_probs_dropout_prob=0.1，所以实际上dropout了0.9的token</span></span><br><span class="line">  <span class="comment"># dropout函数输出的是output = tf.nn.dropout(input_tensor, 1.0 - dropout_prob)</span></span><br><span class="line">  attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, T, N, H]</span></span><br><span class="line">  value_layer = tf.reshape(</span><br><span class="line">      value_layer,</span><br><span class="line">      [batch_size, to_seq_length, num_attention_heads, size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `value_layer` = [B, N, T, H]</span></span><br><span class="line">  value_layer = tf.transpose(value_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, N, F, H]</span></span><br><span class="line">  context_layer = tf.matmul(attention_probs, value_layer)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># `context_layer` = [B, F, N, H]</span></span><br><span class="line">  <span class="comment"># context其实就是图解中的Z值</span></span><br><span class="line">  context_layer = tf.transpose(context_layer, [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> do_return_2d_tensor:</span><br><span class="line">    <span class="comment"># `context_layer` = [B*F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size * from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    <span class="comment"># `context_layer` = [B, F, N*H]</span></span><br><span class="line">    context_layer = tf.reshape(</span><br><span class="line">        context_layer,</span><br><span class="line">        [batch_size, from_seq_length, num_attention_heads * size_per_head])</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 返回的是整个的Z矩阵</span></span><br><span class="line">  <span class="keyword">return</span> context_layer</span><br></pre></td></tr></table></figure><h3 id="get-masked-lm-output"><a href="#get-masked-lm-output" class="headerlink" title="get_masked_lm_output"></a>get_masked_lm_output</h3><p>从 BertModel 部分返回到<a href="#model_fn_builder">model_fn_builder</a>。</p><p>搞定了<code>modeling.BertModel</code>，下面开始计算 Masked LM 和 NSP 任务。</p><p>两个任务的本质都是分类任务，一个是二分类，即两个 segment 是否是连贯的；一个是多分类，即输入序列中被 mask 的 token 为词表中某个 token 的概率。它们的损失函数都是<strong>交叉熵损失</strong>。</p><p>NSP 问题中 0 是连续的，1 是随机的。</p><p>在<code>model_fn_builder</code>中是通过下面代码进行调用的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(masked_lm_loss,</span><br><span class="line">  masked_lm_example_loss, masked_lm_log_probs) = get_masked_lm_output(</span><br><span class="line">      bert_config, model.get_sequence_output(), model.get_embedding_table(),</span><br><span class="line">      masked_lm_positions, masked_lm_ids, masked_lm_weights</span><br></pre></td></tr></table></figure><p>源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_masked_lm_output</span><span class="params">(bert_config, input_tensor, output_weights, positions,</span></span></span><br><span class="line"><span class="function"><span class="params">                         label_ids, label_weights)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the masked LM."""</span></span><br><span class="line">  <span class="comment"># input_tensor是model.get_sequence_output()得到，是encoder layer的最后一层，shape=(32, 128, 512)</span></span><br><span class="line">  <span class="comment"># output_weights是emebdding table</span></span><br><span class="line">  <span class="comment"># position是masked_lm_positions，ids和weights都是masked_</span></span><br><span class="line">  <span class="comment"># gather_indexes是从input_tensor中取出positions位置的tensor</span></span><br><span class="line">  <span class="comment"># 此时input_tensor.shape = （4096, 512)-&gt;(640, 512)，即</span></span><br><span class="line">  <span class="comment"># [batch_size*max_predictions_per_seq=20, hidden_size]</span></span><br><span class="line">  input_tensor = gather_indexes(input_tensor, positions)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/predictions"</span>):</span><br><span class="line">    <span class="comment"># We apply one more non-linear transformation before the output layer.</span></span><br><span class="line">    <span class="comment"># This matrix is not used after pre-training.</span></span><br><span class="line">    <span class="comment"># 对input做了一个fc+ac+norm的操作</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"transform"</span>):</span><br><span class="line">      input_tensor = tf.layers.dense(</span><br><span class="line">          input_tensor,</span><br><span class="line">          units=bert_config.hidden_size,</span><br><span class="line">          activation=modeling.get_activation(bert_config.hidden_act),</span><br><span class="line">          kernel_initializer=modeling.create_initializer(</span><br><span class="line">              bert_config.initializer_range))</span><br><span class="line">      input_tensor = modeling.layer_norm(input_tensor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The output weights are the same as the input embeddings, but there is</span></span><br><span class="line">    <span class="comment"># an output-only bias for each token.</span></span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>,</span><br><span class="line">        shape=[bert_config.vocab_size],</span><br><span class="line">        initializer=tf.zeros_initializer())</span><br><span class="line">    <span class="comment"># output_bias: [30522]</span></span><br><span class="line">    <span class="comment"># input_tensor: [640, 512]</span></span><br><span class="line">    <span class="comment"># output_weights: [30522, 512]</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># transpose_b表示转置一下</span></span><br><span class="line">    <span class="comment"># logits: [batch_size*max_predictions_per_seq=640, vocab_size=30522]</span></span><br><span class="line">    <span class="comment"># log_probs: [batch_size*max_predictions_per_seq, vocab_size]</span></span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># label_ids: [640, ]</span></span><br><span class="line">    <span class="comment"># label_weights: [640, ]</span></span><br><span class="line">    label_ids = tf.reshape(label_ids, [<span class="number">-1</span>])</span><br><span class="line">    label_weights = tf.reshape(label_weights, [<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># [batch_size*max_predictions_per_seq, vocab_size]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(</span><br><span class="line">        label_ids, depth=bert_config.vocab_size, dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># The `positions` tensor might be zero-padded (if the sequence is too</span></span><br><span class="line">    <span class="comment"># short to have the maximum number of predictions). The `label_weights`</span></span><br><span class="line">    <span class="comment"># tensor has a value of 1.0 for every real prediction and 0.0 for the</span></span><br><span class="line">    <span class="comment"># padding predictions.</span></span><br><span class="line">    <span class="comment"># [640, 30522]*[640, 30522]</span></span><br><span class="line">    <span class="comment"># reduce_sum: 调用reduce_sum(arg1, arg2)时，arg1为要求和的数据，arg2为0或1，</span></span><br><span class="line">    <span class="comment"># 通常用reduction_indices=[0]或reduction_indices=[1]来传递。</span></span><br><span class="line">    <span class="comment"># arg2 = 0时，是纵向求和，当arg2 = 1时，是横向求和；省略arg2参数时，默认对所有元素进行求和。</span></span><br><span class="line">    <span class="comment"># reduce就是“对矩阵降维”的含义，在reduce_sum()中就是按照求和的方式对矩阵降维。</span></span><br><span class="line">    <span class="comment"># 那么其他reduce前缀的函数也举一反三了，比如reduce_mean()就是按照某个维度求平均值，等等。</span></span><br><span class="line">    <span class="comment"># per_example_loss-&gt;reduce_sum-&gt;[640, ]，每个位置预测对的log概率*(-1)</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(log_probs * one_hot_labels, axis=[<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># label_weights即masked_lm_weights</span></span><br><span class="line">    <span class="comment"># 和input_mask的作用一样，标注masked_lm_ids 和 masked_lm_positions 哪些是真实值，哪些是补全值</span></span><br><span class="line">    <span class="comment"># label_weights: [640, ]</span></span><br><span class="line">    numerator = tf.reduce_sum(label_weights * per_example_loss)</span><br><span class="line">    <span class="comment"># 计算非0的真实位置个数，+1e-5防止分子为0，</span></span><br><span class="line">    denominator = tf.reduce_sum(label_weights) + <span class="number">1e-5</span></span><br><span class="line">    loss = numerator / denominator</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> (loss, per_qiumple_loss, log_probs)</span><br></pre></td></tr></table></figure><h3 id="get-next-sentence-output"><a href="#get-next-sentence-output" class="headerlink" title="get_next_sentence_output"></a>get_next_sentence_output</h3><p><code>model_fn_builder</code>中计算完了<code>get_masked_lm_output</code>之后，计算<code>get_next_sentence_output</code>。</p><p>调用方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(next_sentence_loss, next_sentence_example_loss,</span><br><span class="line">  next_sentence_log_probs) = get_next_sentence_output(</span><br><span class="line">      bert_config, model.get_pooled_output(), next_sentence_labels)</span><br></pre></td></tr></table></figure><p>源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_next_sentence_output</span><span class="params">(bert_config, input_tensor, labels)</span>:</span></span><br><span class="line">  <span class="string">"""Get loss and log probs for the next sentence prediction."""</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Simple binary classification. Note that 0 is "next sentence" and 1 is</span></span><br><span class="line">  <span class="comment"># "random sentence". This weight matrix is not used after pre-training.</span></span><br><span class="line">  <span class="comment"># input_tensor: [batch_size, hidden_size]</span></span><br><span class="line">  <span class="comment"># labels: [batch_size, 1]</span></span><br><span class="line">  <span class="keyword">with</span> tf.variable_scope(<span class="string">"cls/seq_relationship"</span>):</span><br><span class="line">    output_weights = tf.get_variable(</span><br><span class="line">        <span class="string">"output_weights"</span>,</span><br><span class="line">        shape=[<span class="number">2</span>, bert_config.hidden_size],</span><br><span class="line">        initializer=modeling.create_initializer(bert_config.initializer_range))</span><br><span class="line">    output_bias = tf.get_variable(</span><br><span class="line">        <span class="string">"output_bias"</span>, shape=[<span class="number">2</span>], initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># logits: [batch_size, 2]，相当于全连接层+softmax分类</span></span><br><span class="line">    logits = tf.matmul(input_tensor, output_weights, transpose_b=<span class="literal">True</span>)</span><br><span class="line">    logits = tf.nn.bias_add(logits, output_bias)</span><br><span class="line">    log_probs = tf.nn.log_softmax(logits, axis=<span class="number">-1</span>)</span><br><span class="line">    labels = tf.reshape(labels, [<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># one_hot_labels: [batch_size, 2]</span></span><br><span class="line">    one_hot_labels = tf.one_hot(labels, depth=<span class="number">2</span>, dtype=tf.float32)</span><br><span class="line">    <span class="comment"># 交叉熵损失：-\frac&#123;1&#125;&#123;N&#125; \sum_i (y_i log(p_i) + (1-y_i) log(1-p_i))</span></span><br><span class="line">    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=<span class="number">-1</span>)</span><br><span class="line">    loss = tf.reduce_mean(per_example_loss)</span><br><span class="line">    <span class="keyword">return</span> (loss, per_example_loss, log_probs)</span><br></pre></td></tr></table></figure><h3 id="create-optimizer"><a href="#create-optimizer" class="headerlink" title="create_optimizer"></a>create_optimizer</h3><p>调用部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == tf.estimator.ModeKeys.TRAIN:</span><br><span class="line">  <span class="comment"># 创建优化器optimizer</span></span><br><span class="line">  train_op = optimization.create_optimizer(</span><br><span class="line">      total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)</span><br></pre></td></tr></table></figure><p>实现部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_optimizer</span><span class="params">(loss, init_lr, num_train_steps, num_warmup_steps, use_tpu)</span>:</span></span><br><span class="line">  <span class="string">"""Creates an optimizer training op."""</span></span><br><span class="line">  global_step = tf.train.get_or_create_global_step()</span><br><span class="line"></span><br><span class="line">  learning_rate = tf.constant(value=init_lr, shape=[], dtype=tf.float32)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear decay of the learning rate.</span></span><br><span class="line">  learning_rate = tf.train.polynomial_decay(</span><br><span class="line">      learning_rate,</span><br><span class="line">      global_step,</span><br><span class="line">      num_train_steps,</span><br><span class="line">      end_learning_rate=<span class="number">0.0</span>,</span><br><span class="line">      power=<span class="number">1.0</span>,</span><br><span class="line">      cycle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Implements linear warmup. I.e., if global_step &lt; num_warmup_steps, the</span></span><br><span class="line">  <span class="comment"># learning rate will be `global_step/num_warmup_steps * init_lr`.</span></span><br><span class="line">  <span class="keyword">if</span> num_warmup_steps:</span><br><span class="line">    global_steps_int = tf.cast(global_step, tf.int32)</span><br><span class="line">    warmup_steps_int = tf.constant(num_warmup_steps, dtype=tf.int32)</span><br><span class="line"></span><br><span class="line">    global_steps_float = tf.cast(global_steps_int, tf.float32)</span><br><span class="line">    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)</span><br><span class="line"></span><br><span class="line">    warmup_percent_done = global_steps_float / warmup_steps_float</span><br><span class="line">    warmup_learning_rate = init_lr * warmup_percent_done</span><br><span class="line"></span><br><span class="line">    is_warmup = tf.cast(global_steps_int &lt; warmup_steps_int, tf.float32)</span><br><span class="line">    <span class="comment"># 小trick，is_warmup为1时，算后面的</span></span><br><span class="line">    learning_rate = (</span><br><span class="line">        (<span class="number">1.0</span> - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># It is recommended that you use this optimizer for fine tuning, since this</span></span><br><span class="line">  <span class="comment"># is how the model was trained (note that the Adam m/v variables are NOT</span></span><br><span class="line">  <span class="comment"># loaded from init_checkpoint.)</span></span><br><span class="line">  optimizer = AdamWeightDecayOptimizer(</span><br><span class="line">      learning_rate=learning_rate,</span><br><span class="line">      weight_decay_rate=<span class="number">0.01</span>,</span><br><span class="line">      beta_1=<span class="number">0.9</span>,</span><br><span class="line">      beta_2=<span class="number">0.999</span>,</span><br><span class="line">      epsilon=<span class="number">1e-6</span>,</span><br><span class="line">      exclude_from_weight_decay=[<span class="string">"LayerNorm"</span>, <span class="string">"layer_norm"</span>, <span class="string">"bias"</span>])</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> use_tpu:</span><br><span class="line">    optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)</span><br><span class="line"></span><br><span class="line">  tvars = tf.trainable_variables()</span><br><span class="line">  grads = tf.gradients(loss, tvars)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># This is how the model was pre-trained.</span></span><br><span class="line">  (grads, _) = tf.clip_by_global_norm(grads, clip_norm=<span class="number">1.0</span>)</span><br><span class="line"></span><br><span class="line">  train_op = optimizer.apply_gradients(</span><br><span class="line">      zip(grads, tvars), global_step=global_step)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Normally the global step update is done inside of `apply_gradients`.</span></span><br><span class="line">  <span class="comment"># However, `AdamWeightDecayOptimizer` doesn't do this. But if you use</span></span><br><span class="line">  <span class="comment"># a different optimizer, you should probably take this line out.</span></span><br><span class="line">  new_global_step = global_step + <span class="number">1</span></span><br><span class="line">  train_op = tf.group(train_op, [global_step.assign(new_global_step)])</span><br><span class="line">  <span class="keyword">return</span> train_op</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AdamWeightDecayOptimizer</span><span class="params">(tf.train.Optimizer)</span>:</span></span><br><span class="line">  <span class="string">"""A basic Adam optimizer that includes "correct" L2 weight decay."""</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,</span></span></span><br><span class="line"><span class="function"><span class="params">               learning_rate,</span></span></span><br><span class="line"><span class="function"><span class="params">               weight_decay_rate=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               beta_1=<span class="number">0.9</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               beta_2=<span class="number">0.999</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               epsilon=<span class="number">1e-6</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">               exclude_from_weight_decay=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               name=<span class="string">"AdamWeightDecayOptimizer"</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Constructs a AdamWeightDecayOptimizer."""</span></span><br><span class="line">    super(AdamWeightDecayOptimizer, self).__init__(<span class="literal">False</span>, name)</span><br><span class="line"></span><br><span class="line">    self.learning_rate = learning_rate</span><br><span class="line">    self.weight_decay_rate = weight_decay_rate</span><br><span class="line">    self.beta_1 = beta_1</span><br><span class="line">    self.beta_2 = beta_2</span><br><span class="line">    self.epsilon = epsilon</span><br><span class="line">    self.exclude_from_weight_decay = exclude_from_weight_decay</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply_gradients</span><span class="params">(self, grads_and_vars, global_step=None, name=None)</span>:</span></span><br><span class="line">    <span class="string">"""See base class."""</span></span><br><span class="line">    assignments = []</span><br><span class="line">    <span class="keyword">for</span> (grad, param) <span class="keyword">in</span> grads_and_vars:</span><br><span class="line">      <span class="keyword">if</span> grad <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> param <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">      param_name = self._get_variable_name(param.name)</span><br><span class="line"></span><br><span class="line">      m = tf.get_variable(</span><br><span class="line">          name=param_name + <span class="string">"/adam_m"</span>,</span><br><span class="line">          shape=param.shape.as_list(),</span><br><span class="line">          dtype=tf.float32,</span><br><span class="line">          trainable=<span class="literal">False</span>,</span><br><span class="line">          initializer=tf.zeros_initializer())</span><br><span class="line">      v = tf.get_variable(</span><br><span class="line">          name=param_name + <span class="string">"/adam_v"</span>,</span><br><span class="line">          shape=param.shape.as_list(),</span><br><span class="line">          dtype=tf.float32,</span><br><span class="line">          trainable=<span class="literal">False</span>,</span><br><span class="line">          initializer=tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Standard Adam update.</span></span><br><span class="line">      next_m = (</span><br><span class="line">          tf.multiply(self.beta_1, m) + tf.multiply(<span class="number">1.0</span> - self.beta_1, grad))</span><br><span class="line">      next_v = (</span><br><span class="line">          tf.multiply(self.beta_2, v) + tf.multiply(<span class="number">1.0</span> - self.beta_2,</span><br><span class="line">                                                    tf.square(grad)))</span><br><span class="line"></span><br><span class="line">      update = next_m / (tf.sqrt(next_v) + self.epsilon)</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Just adding the square of the weights to the loss function is *not*</span></span><br><span class="line">      <span class="comment"># the correct way of using L2 regularization/weight decay with Adam,</span></span><br><span class="line">      <span class="comment"># since that will interact with the m and v parameters in strange ways.</span></span><br><span class="line">      <span class="comment">#</span></span><br><span class="line">      <span class="comment"># Instead we want ot decay the weights in a manner that doesn't interact</span></span><br><span class="line">      <span class="comment"># with the m/v parameters. This is equivalent to adding the square</span></span><br><span class="line">      <span class="comment"># of the weights to the loss with plain (non-momentum) SGD.</span></span><br><span class="line">      <span class="keyword">if</span> self._do_use_weight_decay(param_name):</span><br><span class="line">        update += self.weight_decay_rate * param</span><br><span class="line"></span><br><span class="line">      update_with_lr = self.learning_rate * update</span><br><span class="line"></span><br><span class="line">      next_param = param - update_with_lr</span><br><span class="line"></span><br><span class="line">      assignments.extend(</span><br><span class="line">          [param.assign(next_param),</span><br><span class="line">           m.assign(next_m),</span><br><span class="line">           v.assign(next_v)])</span><br><span class="line">    <span class="keyword">return</span> tf.group(*assignments, name=name)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_do_use_weight_decay</span><span class="params">(self, param_name)</span>:</span></span><br><span class="line">    <span class="string">"""Whether to use L2 weight decay for `param_name`."""</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.weight_decay_rate:</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">if</span> self.exclude_from_weight_decay:</span><br><span class="line">      <span class="keyword">for</span> r <span class="keyword">in</span> self.exclude_from_weight_decay:</span><br><span class="line">        <span class="keyword">if</span> re.search(r, param_name) <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">_get_variable_name</span><span class="params">(self, param_name)</span>:</span></span><br><span class="line">    <span class="string">"""Get the variable name from the tensor name."""</span></span><br><span class="line">    m = re.match(<span class="string">"^(.*):\\d+$"</span>, param_name)</span><br><span class="line">    <span class="keyword">if</span> m <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">      param_name = m.group(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> param_name</span><br></pre></td></tr></table></figure><p>首先是学习率部分，将学习率设置为线性衰减的形式，接着根据global_step是否达到num_warmup_steps，在原来线性衰减的基础上将学习率进一步分成warmup_learning_rate和learning_rate两种方式。然后是优化器的构建。</p><p>先是实例化AdamWeightDecayOptimizer(其是梯度下降法的一种变种，也由待更新参数、学习率和参数更新方向三大要素组成)，接着通过tvars = tf.trainable_variables()解析出模型中所有待训练的参数变量，并给出loss关于所有参数变量的梯度表示grads = tf.gradients(loss, tvars)，同时限制梯度的大小。最后基于上述描述的梯度与变量，进行参数更新操作。更新时，依此遍历每一个待更新的参数，根据标准的Adam更新公式(参考Adam和学习率衰减（learning rate decay）)，先确定参数更新方向，接着在方向的基础上增加衰减参数(这个操作叫纠正的L2 weight decay)，然后在纠正后的方向上移动一定距离(learning_rate * update)后，更新现有的参数。 以上更新步骤随着训练步数不断进行，直到走完所有训练步数。</p><h3 id="Estimator-类"><a href="#Estimator-类" class="headerlink" title="Estimator 类"></a>Estimator 类</h3><p>tf 提供了很多预创建的 Estimator，也可以自己定义 Estimator 类。但都是基于<code>tf.estimator.Estimator</code>。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>Estimator 类，用来训练和验证 TensorFlow 模型：</p><ul><li>Estimator 对象包含了一个模型 model_fn，这个模型给定输入和参数，会返回训练、验证或者预测等所需要的操作节点。</li><li>所有的输出（检查点、事件文件等）会写入到 model_dir，或者其子文件夹中。如果 model_dir 为空，则默认为临时目录。</li><li>config 参数为 tf.estimator.RunConfig 对象，包含了执行环境的信息。如果没有传递 config，则它会被 Estimator 实例化，使用的是默认配置。</li><li>params 包含了超参数。Estimator 只传递超参数，不会检查超参数，因此 params 的结构完全取决于开发者。</li><li>Estimator 的所有方法都不能被子类覆盖（它的构造方法强制决定的）。子类应该使用 model_fn 来配置母类，或者增添方法来实现特殊的功能。</li><li>Estimator 不支持 Eager Execution（eager execution 能够使用 Python 的 debug 工具、数据结构与控制流。并且无需使用 placeholder、session，计算结果能够立即得出）。</li></ul><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p><code>__init__(self, model_fn, model_dir=None, config=None, params=None, warm_start_from=None)</code></p><p>构造一个 Estimator 的实例。</p><p>参数：</p><ol><li>model_fn: 模型函数。<ol><li>参数：<ol><li>features: 这是 input_fn 返回的第一项（input_fn 是 train, evaluate 和 predict 的参数）。类型应该是单一的 Tensor 或者 dict。</li><li>labels: 这是 input_fn 返回的第二项。类型应该是单一的 Tensor 或者 dict。如果 mode 为 ModeKeys.PREDICT，则会默认为 labels=None。如果 model_fn 不接受 mode，model_fn 应该仍然可以处理 labels=None。</li><li>mode: 可选。指定是训练、验证还是测试。参见 ModeKeys。</li><li>params: 可选，超参数的 dict。 可以从超参数调整中配置 Estimators。</li><li>config: 可选，配置。如果没有传则为默认值。可以根据 num_ps_replicas 或 model_dir 等配置更新 model_fn。</li></ol></li><li>返回：EstimatorSpec</li></ol></li><li>model_dir:<ol><li>保存模型参数、图等的地址，也可以用来将路径中的检查点加载至 estimator 中来继续训练之前保存的模型。</li><li>如果是 PathLike， 那么路径就固定为它了。</li><li>如果是 None，那么 config 中的 model_dir 会被使用（如果设置了的话）</li><li>如果两个都设置了，那么必须相同；如果两个都是 None，则会使用临时目录。</li></ol></li><li>config: 配置类。</li><li>params: 超参数的 dict，会被传递到 model_fn。keys 是参数的名称，values 是基本 python 类型。</li><li>warm_start_from:<ol><li>可选，字符串，检查点的文件路径，用来指示从哪里开始热启动。</li><li>或者是 tf.estimator.WarmStartSettings 类来全部配置热启动。</li><li>如果是字符串路径，则所有的变量都是热启动，并且需要 Tensor 和词汇的名字都没有变。</li></ol></li><li>异常：<ol><li>RuntimeError： 开启了 eager execution</li><li>ValueError：model_fn 的参数与 params 不匹配</li><li>ValueError：这个函数被 Estimator 的子类所覆盖</li></ol></li></ol><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><p><code>train(self, input_fn, hooks=None, steps=None, max_steps=None, saving_listeners=None)</code></p><p>根据所给数据 input_fn， 对模型进行训练。</p><p>参数：</p><ol><li>input_fn：一个函数，提供由小 batches 组成的数据， 供训练使用。必须返回以下之一：<ul><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出必须是一个元组 (features, labels)，元组要求如下。</li><li>一个元组 (features, labels)：features 是一个 Tensor 或者一个字典（特征名为 Tensor），labels 是一个 Tensor 或者一个字典（特征名为 Tensor）。features 和 labels 都被 model_fn 所使用，应该符合 model_fn 输入的要求。</li></ul></li><li>hooks：SessionRunHook 子类实例的列表。用于在训练循环内部执行。</li><li>steps：模型训练的步数。<ol><li>如果是 None， 则一直训练，直到 input_fn 抛出了超过界限的异常。</li><li>steps 是递进式进行的。如果执行了两次训练（steps=10），则总共训练了 20 次。如果中途抛出了越界异常，则训练在 20 次之前就会停止。</li><li>如果你不想递进式进行，请换为设置 max_steps。如果设置了 steps，则 max_steps 必须是 None。</li></ol></li><li>max_steps：模型训练的最大步数。<ol><li>如果为 None，则一直训练，直到 input_fn 抛出了超过界限的异常。</li><li>如果设置了 max_steps， 则 steps 必须是 None。</li><li>如果中途抛出了越界异常，则训练在 max_steps 次之前就会停止。</li><li>执行两次 train(steps=100) 意味着 200 次训练；但是，执行两次 train(max_steps=100) 意味着第二次执行不会进行任何训练，因为第一次执行已经做完了所有的 100 次。</li></ol></li><li>saving_listeners：CheckpointSaverListener 对象的列表。用于在保存检查点之前或之后立即执行的回调函数。<br>返回：self：为了链接下去。</li></ol><p>异常：</p><ul><li>ValueError：steps 和 max_steps 都不是 None</li><li>ValueError：steps 或 max_steps &lt;= 0</li></ul><h4 id="evaluate"><a href="#evaluate" class="headerlink" title="evaluate"></a>evaluate</h4><p><code>evaluate(self, input_fn, steps=None, hooks=None, checkpoint_path=None, name=None)</code></p><p>根据所给数据 input_fn， 对模型进行验证。</p><p>对于每一步，执行 input_fn（返回数据的一个 batch）。一直进行验证，直到：</p><ul><li>steps 个 batches 进行完毕，或者</li><li>input_fn 抛出了越界异常（OutOfRangeError 或 StopIteration）</li></ul><p>参数：</p><ol><li>input_fn：一个函数，构造了验证所需的输入数据，必须返回以下之一：<ol><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出必须是一个元组 (features, labels)，元组要求如下。</li><li>一个元组 (features, labels)：features 是一个 Tensor 或者一个字典（特征名为 Tensor），labels 是一个 Tensor 或者一个字典（特征名为 Tensor）。features 和 labels 都被 model_fn 所使用，应该符合 model_fn 输入的要求。</li></ol></li><li>steps：模型验证的步数。如果是 None， 则一直验证，直到 input_fn 抛出了超过界限的异常。</li><li>hooks：SessionRunHook 子类实例的列表。用于在验证内部执行。</li><li>checkpoint_path： 用于验证的检查点路径。如果是 None， 则使用 model_dir 中最新的检查点。</li><li>name：验证的名字。使用者可以针对不同的数据集运行多个验证操作，比如训练集 vs 测试集。不同验证的结果被保存在不同的文件夹中，且分别出现在 tensorboard 中。</li></ol><p>返回：</p><ul><li>返回一个字典，包括 model_fn 中指定的评价指标、global_step（包含验证进行的全局步数）</li></ul><p>异常：</p><ul><li>ValueError：如果 step 小于等于 0</li><li>ValueError：如果 model_dir 指定的模型没有被训练，或者指定的 checkpoint_path 为空。</li></ul><h4 id="predict"><a href="#predict" class="headerlink" title="predict"></a>predict</h4><p><code>predict(self, input_fn, predict_keys=None, hooks=None, checkpoint_path=None, yield_single_examples=True)</code></p><p>对给出的特征进行预测。</p><p>参数：</p><ol><li>input_fn：一个函数，构造特征。预测一直进行下去，直到 input_fn 抛出了越界异常（OutOfRangeError 或 StopIteration）。函数必须返回以下之一：<ol><li>一个 ‘tf.data.Dataset’对象：Dataset 的输出和以下的限制相同。</li><li>features：一个 Tensor 或者一个字典（特征名为 Tensor）。features 被 model_fn 所使用，应该符合 model_fn 输入的要求。</li><li>一个元组，其中第一项为 features。</li></ol></li><li>predict_keys：字符串列表，要预测的键值。当 EstimatorSpec.predictions 是一个 dict 时使用。如果使用了 predict_keys， 那么剩下的预测值会从字典中过滤掉。如果是 None，则返回全部。</li><li>hooks：SessionRunHook 子类实例的列表。用于在预测内部回调。</li><li>checkpoint_path： 用于预测的检查点路径。如果是 None， 则使用 model_dir 中最新的检查点。</li><li>yield_single_examples：If False, yield the whole batch as returned by the model_fn instead of decomposing the batch into individual elements. This is useful if model_fn returns some tensors whose first dimension is not equal to the batch size.</li></ol><p>返回：</p><ul><li>predictions tensors 的值</li></ul><p>异常：</p><ul><li>ValueError：model_dir 中找不到训练好的模型。</li><li>ValueError：预测值的 batch 长度不同，且 yield_single_examples 为 True。</li><li>ValueError：predict_keys 和 predictions 之间有冲突。例如，predict_keys 不是 None，但是 EstimatorSpec.predictions 不是一个 dict。</li></ul><h3 id="estimator-train-evaluate"><a href="#estimator-train-evaluate" class="headerlink" title="estimator.train/evaluate"></a>estimator.train/evaluate</h3><p>了解了 Estimator 可以理解上面的<code>estimator.train</code>和<code>estimator.evaluate</code>了</p><p>搞定了上面的部分就可以完成<a href="#model_fn_builder">model_fn_builder</a>部分的理解，然后返回<a href="#main-of-pretrain">main</a>。这下只剩<code>estimator.train</code>和<code>estimator.evaluate</code>了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">estimator = tf.contrib.tpu.TPUEstimator(</span><br><span class="line">    use_tpu=FLAGS.use_tpu,</span><br><span class="line">    model_fn=model_fn,</span><br><span class="line">    config=run_config,</span><br><span class="line">    train_batch_size=FLAGS.train_batch_size,</span><br><span class="line">    eval_batch_size=FLAGS.eval_batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> FLAGS.do_train:</span><br><span class="line">  tf.logging.info(<span class="string">"***** Running training *****"</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.train_batch_size)</span><br><span class="line">  <span class="comment"># 从tfrecord解析出BERT的输入数据</span></span><br><span class="line">  train_input_fn = input_fn_builder(</span><br><span class="line">      input_files=input_files,</span><br><span class="line">      max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">      max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">      is_training=<span class="literal">True</span>)</span><br><span class="line">  estimator.train(input_fn=train_input_fn, max_steps=FLAGS.num_train_steps)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> FLAGS.do_eval:</span><br><span class="line">  tf.logging.info(<span class="string">"***** Running evaluation *****"</span>)</span><br><span class="line">  tf.logging.info(<span class="string">"  Batch size = %d"</span>, FLAGS.eval_batch_size)</span><br><span class="line">  <span class="comment"># 如果不训练只是eval，输入也是一样的tfrecord</span></span><br><span class="line">  eval_input_fn = input_fn_builder(</span><br><span class="line">      input_files=input_files,</span><br><span class="line">      max_seq_length=FLAGS.max_seq_length,</span><br><span class="line">      max_predictions_per_seq=FLAGS.max_predictions_per_seq,</span><br><span class="line">      is_training=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># evaluate操作得到结果</span></span><br><span class="line">  result = estimator.evaluate(</span><br><span class="line">      input_fn=eval_input_fn, steps=FLAGS.max_eval_steps)</span><br><span class="line">`</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;BERT 作为一个里程碑式的预训练模型，很多时候我们都是直接用训练好的 model 直接 fine-tune，对它的理解只停留在 MLM 和 NSP 上。后续的很多 SOTA 模型都是在 BERT 的基础上发展来，比如 ALBERT、RoBERTa、XLNet 之类。&lt;/p&gt;&lt;p&gt;这里对 BERT 创建预训练数据的源码：&lt;code&gt;create_pretraining_data.py&lt;/code&gt;和&lt;code&gt;run_pretrain.py&lt;/code&gt;进行分析和理解。&lt;/p&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/google-research/bert/tree/eedf5716ce1268e56f0a50264a88cafad334ac61&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;当前 BERT 对应的 commit&lt;/a&gt;&lt;/p&gt;&lt;p&gt;部分参考&lt;a href=&quot;https://carlos9310.github.io/2019/09/30/pre-trained-bert/&quot; target=&quot;_blank&quot; rel=&quot;external nofollow noopener noreferrer&quot;&gt;预训练模型-BERT预训练源码解读笔记&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="DeepLearning" scheme="https://hanielxx.com/tags/DeepLearning/"/>
    
    <category term="BERT" scheme="https://hanielxx.com/tags/BERT/"/>
    
    <category term="Pretrain" scheme="https://hanielxx.com/tags/Pretrain/"/>
    
  </entry>
  
  <entry>
    <title>GBM和GridSearch</title>
    <link href="https://hanielxx.com/MachineLearning/2021-01-12-gbm-gridsearch"/>
    <id>https://hanielxx.com/MachineLearning/2021-01-12-gbm-gridsearch</id>
    <published>2021-01-12T03:17:25.000Z</published>
    <updated>2021-05-17T15:34:23.028Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>sklearn中<code>sklearn.ensemble.GradientBoostingRegressor</code>和<code>sklearn.model_selection.GridSearchCV</code>的使用</p></div><a id="more"></a><h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>sklearn中实现了<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" target="_blank" rel="external nofollow noopener noreferrer">sklearn.ensemble.GradientBoostingRegressor</a>和<a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" target="_blank" rel="external nofollow noopener noreferrer">sklearn.ensemble.GradientBoostingClassifier</a>，分别用于回归和分类，具体的参数看官网说明。</p><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p><a href="https://hanielxx.com/Notes/2020-12-08-ucas-big-data-analysis-review.html#Gradient-Boost-Decision-Tree-GBDT">GDBT梯度提升决策树</a>可以参考以前的笔记。</p><p>提一下Adaboost，<a href="https://en.wikipedia.org/wiki/AdaBoost" target="_blank" rel="external nofollow noopener noreferrer">wikipedia</a>讲的比较清楚，也可以看我之前的笔记：<a href="https://hanielxx.com/Notes/2020-12-14-ucas-prml-review.html#AdaBoost">UCAS模式识别-Adaboost</a></p><h2 id="GridSearchCV"><a href="#GridSearchCV" class="headerlink" title="GridSearchCV"></a>GridSearchCV</h2><p>参考链接：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/130747955" target="_blank" rel="external nofollow noopener noreferrer">GBM调参汇总</a></li><li><a href="https://www.cnblogs.com/pinard/p/6143927.html" target="_blank" rel="external nofollow noopener noreferrer">刘建平-scikit-learn 梯度提升树(GBDT)调参小结</a></li><li><a href="https://www.cnblogs.com/wj-1314/p/10422159.html" target="_blank" rel="external nofollow noopener noreferrer">Python机器学习笔记：Grid SearchCV（网格搜索)</a></li></ul><p>GBDT搜索参数代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.utils <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gbdt_regression_gridsearchcv</span><span class="params">(x_train, y_train)</span>:</span></span><br><span class="line">    <span class="string">'''使用网格搜索GBDT参数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 固定learning rate=0.1, subsample=0.8, min_samples_split=1%*N=24, min_samples_leaf=default=1, max_depth=8, max_features='sqrt', random_state=SEED，得到结果n_estimaors=90最佳</span></span><br><span class="line">    param_test1 = {</span><br><span class="line">        <span class="string">'n_estimators'</span>: range(<span class="number">20</span>, <span class="number">250</span>, <span class="number">10</span>),</span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 固定其他，调整max_depth和min_sample_split，得到结果为max_depth=5, min_samples_split=44</span></span><br><span class="line">    param_test2 = {</span><br><span class="line">        <span class="string">'max_depth'</span>: range(<span class="number">3</span>, <span class="number">16</span>, <span class="number">2</span>),</span><br><span class="line">        <span class="string">'min_samples_split'</span>: range(<span class="number">12</span>, <span class="number">48</span>, <span class="number">4</span>),  <span class="comment"># 观测样本的0.5%-2%</span></span><br><span class="line">    }</span><br><span class="line">    <span class="comment"># 调整min_samples_split和min_samples_leaf，得到结果min_samples_split=44,min_samples_leaf=7</span></span><br><span class="line">    param_test3 = {</span><br><span class="line">        <span class="string">'min_samples_split'</span>: range(<span class="number">12</span>, <span class="number">48</span>, <span class="number">4</span>),</span><br><span class="line">        <span class="string">'min_samples_leaf'</span>: range(<span class="number">1</span>, <span class="number">11</span>),</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    gsearch1 = GridSearchCV(</span><br><span class="line">        estimator=GradientBoostingRegressor(learning_rate=<span class="number">0.1</span>,</span><br><span class="line">                                            n_estimators=<span class="number">90</span>,</span><br><span class="line">                                            subsample=<span class="number">0.8</span>,</span><br><span class="line">                                            max_depth=<span class="number">5</span>,</span><br><span class="line">                                            min_samples_split=<span class="number">44</span>,</span><br><span class="line">                                            min_samples_leaf=<span class="number">7</span>,</span><br><span class="line">                                            max_features=<span class="string">'sqrt'</span>,</span><br><span class="line">                                            random_state=SEED),</span><br><span class="line">        param_grid=param_test3,  <span class="comment"># 修改要搜索的参数</span></span><br><span class="line">        scoring=<span class="string">'neg_mean_squared_error'</span>,</span><br><span class="line">        cv=<span class="number">5</span>)</span><br><span class="line">    gsearch1.fit(x_train, y_train)</span><br><span class="line">    print(gsearch1.scorer_, gsearch1.best_params_, gsearch1.best_score_)</span><br></pre></td></tr></table></figure><h2 id="k-fold交叉验证"><a href="#k-fold交叉验证" class="headerlink" title="k-fold交叉验证"></a>k-fold交叉验证</h2><h3 id="sklearn代码"><a href="#sklearn代码" class="headerlink" title="sklearn代码"></a>sklearn代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kfold_regression</span><span class="params">(model, x_train, y_train, x_test, y_test, random_seed, K=<span class="number">5</span>)</span>:</span></span><br><span class="line">    <span class="string">'''使用k折交叉验证对model进行训练，得到最后的平均pearsonr相关系数</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># k折交叉验证，这里要记得shuffle=true，在分之前随机打乱</span></span><br><span class="line">    kf = KFold(n_splits=K, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    avg_r = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> kf.split(y_data):</span><br><span class="line">        x_train, x_test = x_data.iloc[train_index, :], x_data.iloc[test_index, :]</span><br><span class="line">        y_train, y_test = y_data[train_index], y_data[test_index]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 下面的参数是通过girdsearch得到</span></span><br><span class="line">        model.fit(x_train, y_train)</span><br><span class="line">        y_predict = model.predict(x_test)</span><br><span class="line"></span><br><span class="line">        r, _ = scipy.stats.pearsonr(y_predict, y_test)</span><br><span class="line">        avg_r += r</span><br><span class="line">        print(<span class="string">"current r = {}"</span>.format(r))</span><br><span class="line">        <span class="comment"># print("current split r = {}, oob_score = {}".format(r, model.oob_score_))</span></span><br><span class="line">    avg_r /= K</span><br><span class="line">    print(<span class="string">"average pcc: {}"</span>.format(avg_r))</span><br><span class="line">    <span class="keyword">return</span> avg_r</span><br><span class="line"></span><br><span class="line">gbdt_model = GradientBoostingRegressor(learning_rate=<span class="number">0.1</span>,</span><br><span class="line">                                        n_estimators=<span class="number">90</span>,</span><br><span class="line">                                        subsample=<span class="number">0.8</span>,</span><br><span class="line">                                        max_depth=<span class="number">5</span>,</span><br><span class="line">                                        min_samples_split=<span class="number">44</span>,</span><br><span class="line">                                        min_samples_leaf=<span class="number">7</span>,</span><br><span class="line">                                        max_features=<span class="string">'sqrt'</span>,</span><br><span class="line">                                        random_state=SEED)</span><br><span class="line"></span><br><span class="line">kfold_regression(gbdt_model, x_train, y_train, x_test, y_test, SEED, K)</span><br></pre></td></tr></table></figure><hr>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;sklearn中&lt;code&gt;sklearn.ensemble.GradientBoostingRegressor&lt;/code&gt;和&lt;code&gt;sklearn.model_selection.GridSearchCV&lt;/code&gt;的使用&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="MachineLearning" scheme="https://hanielxx.com/categories/MachineLearning/"/>
    
    
    <category term="GBM" scheme="https://hanielxx.com/tags/GBM/"/>
    
    <category term="GridSearch" scheme="https://hanielxx.com/tags/GridSearch/"/>
    
    <category term="GBDT" scheme="https://hanielxx.com/tags/GBDT/"/>
    
    <category term="K-Fold" scheme="https://hanielxx.com/tags/K-Fold/"/>
    
    <category term="ML" scheme="https://hanielxx.com/tags/ML/"/>
    
  </entry>
  
  <entry>
    <title>UCAS并行与分布式计算</title>
    <link href="https://hanielxx.com/Notes/2020-12-29-ucas-parallel-and-distributed-computing"/>
    <id>https://hanielxx.com/Notes/2020-12-29-ucas-parallel-and-distributed-computing</id>
    <published>2020-12-29T03:42:24.000Z</published>
    <updated>2021-05-17T15:34:23.082Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>UCAS并行与分布式计算期末复习</p></div><a id="more"></a><h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><h3 id="Scalability"><a href="#Scalability" class="headerlink" title="Scalability"></a>Scalability</h3><p>可伸缩性，三个尺度：</p><ul><li>在规模上可伸缩/vertical scalability：可以增加更多的用户和资源</li><li>在地理上可伸缩/horizontal scalability：用户、资源都可以相距很远</li><li>在管理上可伸缩：能够很容易地管理相互独立的组织。</li></ul><h3 id="Epidemic-protocols"><a href="#Epidemic-protocols" class="headerlink" title="Epidemic protocols"></a>Epidemic protocols</h3><p>流行病协议</p><ul><li>一种快速传播信息的方法仅使用本地的大型分布式系统信息</li><li>用于故障检测，数据聚合，资源发现和监视和数据库复制</li><li>节点状态：<ul><li>易感；已感染；已移除</li><li>简单的流行病广播算法/永久感染模型：节点始终是易感性或传染性的</li></ul></li><li>传播模型<br>– 反熵<br>– 谣言传播：谣言传播/闲聊</li><li>优势：高可扩展性和可靠性</li></ul><h4 id="Anti-Entropy"><a href="#Anti-Entropy" class="headerlink" title="Anti-Entropy"></a>Anti-Entropy</h4><p>反熵算法</p><ol><li>节点P随机选择另一个Q，然后通过称为push，pull，和push-pull的三种方式之一与Q交换更新。<ul><li>push：P给Q自己的更新</li><li>pull：P只从Q处拉取更新</li><li>push-pull：P和Q互相发送更新</li></ul></li><li>轮：将轮定义为一个周期，在该周期中，每个节点将至少一次主动与随机选择的其他节点交换更新。</li><li>性能：传播一个更新到所有的节点需要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="9.455ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 4179 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(1450, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1935, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(2412, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(2801, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(3401, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3790, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container>轮，n是系统中节点数<ol><li>假设感染进程每轮试图污染 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex" xmlns="http://www.w3.org/2000/svg" width="1.244ex" height="2.059ex" role="img" focusable="false" viewBox="0 -705 550 910"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g></g></g></svg></mjx-container> 个其他进程<ul><li>r回合后被感染成员的预期比例为1</li><li>感染整个系统所需的回合数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.667ex" xmlns="http://www.w3.org/2000/svg" width="32.233ex" height="2.364ex" role="img" focusable="false" viewBox="0 -750 14247 1045"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"></path></g><g data-mml-node="mo" transform="translate(1036.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(2092.6, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(2390.6, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="msub" transform="translate(2875.6, 0)"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="TeXAtom" transform="translate(477, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(550, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1328, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(4695.1, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5084.1, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(5684.1, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(6295.4, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(7295.6, 0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g><g data-mml-node="mi" transform="translate(7593.6, 0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(8078.6, 0)"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"></path></g><g data-mml-node="mo" transform="translate(8555.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(8944.6, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(9544.6, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(9933.6, 0)"><g data-mml-node="mo"><path data-c="2F" d="M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"></path></g></g><g data-mml-node="mi" transform="translate(10433.6, 0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(11205.8, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(12206, 0)"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(12969, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(13358, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(13858, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ul></li></ol></li><li>流行病理论的基本结果是，简单的流行病最终会感染整个系统。</li><li>push和pull的选择<ol><li>假设 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.803ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 797 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>表示节点在第i轮后保持未感染的概率。</li><li>对于<em>pull</em>，假设节点在第i轮后是未感染的，如果节点在第i+1轮后还是未感染，并且它在第i+1轮和一个未感染的节点进行了通信。那么 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="11.341ex" height="2.452ex" role="img" focusable="false" viewBox="0 -833.9 5012.7 1083.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(503, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1978.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(3034.2, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="msub" transform="translate(3423.2, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="msup" transform="translate(4220.1, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mn" transform="translate(389, 363) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></g></svg></mjx-container></li><li>对于<em>push</em>，假设节点在第i轮后是未感染的，如果节在第i+1轮后还是未感染点，并且没有已经感染的节点选择在第i+1轮和它通信，那么 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex" xmlns="http://www.w3.org/2000/svg" width="33.218ex" height="2.819ex" role="img" focusable="false" viewBox="0 -893.3 14682.5 1246.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="TeXAtom" transform="translate(503, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1978.4, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3034.2, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(4053.4, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="mo" transform="translate(4553.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(4942.6, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(5664.8, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(6665, 0)"><g data-mml-node="mn" transform="translate(255.4, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="msup" transform="translate(7529.3, 0)"><g data-mml-node="mo"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="TeXAtom" transform="translate(389, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(600, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(989, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(1489, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2267, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3064, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g><g data-mml-node="mo" transform="translate(10687.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(11743.5, 0)"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(12762.6, 0)"><path data-c="22C5" d="M78 250Q78 274 95 292T138 310Q162 310 180 294T199 251Q199 226 182 208T139 190T96 207T78 250Z"></path></g><g data-mml-node="msup" transform="translate(13262.9, 0)"><g data-mml-node="mi"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="TeXAtom" transform="translate(466, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(778, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container></li><li><strong>pull或push-pull都比push更好</strong></li></ol></li></ol><h4 id="Gossiping"><a href="#Gossiping" class="headerlink" title="Gossiping"></a>Gossiping</h4><p>闲聊算法</p><ol><li>n个人，最初不活跃（易感）。</li><li>然后设置一个active（感染）的人一起散布谣言，随机打给其他人并分享谣言</li><li>听到谣言的每个人也变得active，同样也分享谣言的活跃</li><li>当活动个体打出不必要的电话（即接收者已经知道谣言）时，活动个体以 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.798ex" xmlns="http://www.w3.org/2000/svg" width="1.829ex" height="2.755ex" role="img" focusable="false" viewBox="0 -864.9 808.4 1217.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mn" transform="translate(227.4, 394) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mi" transform="translate(220, -345) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><rect width="568.4" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container> 的概率失去了分享谣言的兴趣<ul><li>该个体被removed</li></ul></li><li>如果未感染人数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="17.055ex" height="2.46ex" role="img" focusable="false" viewBox="0 -893.3 7538.3 1087.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(746.8, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(1802.6, 0)"><g data-mml-node="mi"><path data-c="65" d="M28 218Q28 273 48 318T98 391T163 433T229 448Q282 448 320 430T378 380T406 316T415 245Q415 238 408 231H126V216Q126 68 226 36Q246 30 270 30Q312 30 342 62Q359 79 369 104L379 128Q382 131 395 131H398Q415 131 415 121Q415 117 412 108Q393 53 349 21T250 -11Q155 -11 92 58T28 218ZM333 275Q322 403 238 411H236Q228 411 220 410T195 402T166 381T143 340T127 274V267H333V275Z"></path><path data-c="78" d="M201 0Q189 3 102 3Q26 3 17 0H11V46H25Q48 47 67 52T96 61T121 78T139 96T160 122T180 150L226 210L168 288Q159 301 149 315T133 336T122 351T113 363T107 370T100 376T94 379T88 381T80 383Q74 383 44 385H16V431H23Q59 429 126 429Q219 429 229 431H237V385Q201 381 201 369Q201 367 211 353T239 315T268 274L272 270L297 304Q329 345 329 358Q329 364 327 369T322 376T317 380T310 384L307 385H302V431H309Q324 428 408 428Q487 428 493 431H499V385H492Q443 385 411 368Q394 360 377 341T312 257L296 236L358 151Q424 61 429 57T446 50Q464 46 499 46H516V0H510H502Q494 1 482 1T457 2T432 2T414 3Q403 3 377 3T327 1L304 0H295V46H298Q309 46 320 51T331 63Q331 65 291 120L250 175Q249 174 219 133T185 88Q181 83 181 74Q181 63 188 55T206 46Q208 46 208 23V0H201Z" transform="translate(444, 0)"></path><path data-c="70" d="M36 -148H50Q89 -148 97 -134V-126Q97 -119 97 -107T97 -77T98 -38T98 6T98 55T98 106Q98 140 98 177T98 243T98 296T97 335T97 351Q94 370 83 376T38 385H20V408Q20 431 22 431L32 432Q42 433 61 434T98 436Q115 437 135 438T165 441T176 442H179V416L180 390L188 397Q247 441 326 441Q407 441 464 377T522 216Q522 115 457 52T310 -11Q242 -11 190 33L182 40V-45V-101Q182 -128 184 -134T195 -145Q216 -148 244 -148H260V-194H252L228 -193Q205 -192 178 -192T140 -191Q37 -191 28 -194H20V-148H36ZM424 218Q424 292 390 347T305 402Q234 402 182 337V98Q222 26 294 26Q345 26 384 80T424 218Z" transform="translate(972, 0)"></path></g><g data-mml-node="TeXAtom" transform="translate(1528, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mo" transform="translate(778, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1167, 0)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g><g data-mml-node="mo" transform="translate(1688, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(2466, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(2966, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(3355, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(3744, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(4244, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(5022, 0)"><path data-c="1D460" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path></g><g data-mml-node="mo" transform="translate(5491, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></g></g></svg></mjx-container>，感染的人数可能会降到0，未感染人数s是随着k指数下降的。</li><li>因此增加 k 值是一个能确保所有人都听到谣言的高效的方法。也就是降低不分享谣言的概率。</li><li>闲聊算法不能确保所有节点都被更新。</li></ol><h3 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h3><p>RPC远程程序调用</p><p>Middleware Communication Protocols，即中间件通信协议。</p><p>中间件是通用的用于给上层提供服务，并屏蔽下层细节的叫中间件。</p><ol><li>RPC图示<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/VaVh2C.png" alt="VaVh2C"></li></ol></li><li>从常规过程调用中推断出的想法，实现透明度<ul><li>允许程序调用其他计算机上的过程</li></ul></li><li>如何实现呼叫？<ol><li>客户端程序以常规方式调用client stub</li><li>client stub生成消息，调用本地OS</li><li>客户端的OS向远程OS发送消息</li><li>远程OS向server stub提供消息</li><li>server stub解包参数，调用服务器</li><li>服务器完成任务，将结果返回到 stub</li><li>server stub将结果打包在消息中，调用本地OS</li><li>服务器的OS向客户端OS 9发送消息</li><li>客户端OS向 client stub 发送消息</li><li>stub 将结果解压缩，返回给客户端</li></ol></li><li>参数传递机制？<ol><li>不同编码：EBCDIC和ASCII</li><li>不同的字节序</li><li>引用参数传递：call-by-reference已被copy/restore替换</li></ol></li><li>RPC语义？<ol><li>At-least-once：能够重发请求消息、重新执行程序</li><li>At-most-once：能够重发请求消息、重新发送答复</li><li>Maybe：都不能确保</li></ol></li><li>RPC系统可能的故障和解决方案<ol><li>客户端无法找到服务器。<ol><li>让这个错误引发异常（例如Java）</li><li>使用信号处理程序（例如，定义信号类型SIG-NOSERVER）</li><li>缺点：并非每种语言都有异常或信号处理程序，会破坏透明性。</li></ol></li><li>从客户端到服务器的请求消息丢失。<ol><li>在client stub上启动计时器，如果计时器在答复或确认返回之前到期，则再次发送消息。</li></ol></li><li>服务器在收到请求后崩溃<ol><li>客户端设置计时器，计时器到期后，其解决方案取决于RPC语义：<ul><li>at least once语义：client stub重新发出请求，服务器重新执行</li><li>at most once语义：client stub立即放弃并报告失败</li><li>maybe语义：不做任何保证</li></ul></li></ol></li><li>从服务器到客户端的回复消息丢失<ol><li>client stub设置计时器，计时器到期后</li><li>再次发送请求<ol><li>幂等和非幂等请求</li><li>客户为每个请求分配一个序列号</li></ol></li><li>at-least-once语义：重新发送请求，然后重新执行</li><li>at-most-once语义：重新发送请求，过滤重复的请求，重新发送回复（如果有）/重新执行</li></ol></li><li>客户端在发送请求后崩溃。<ol><li>孤儿和隔代孤儿：(调用了RPC又去fork了其他进程，调用别的区了)计算处于活动状态，并且没有parent在等待结果。</li><li>灭绝：客户端存根在发送RPC消息之前进行日志输入，并在重新启动后杀死该孤儿（代价高，并且要在client端发送请求傻屌服务器进程，对权限要求也高，也不一定能杀干净）</li><li>轮回：客户端在重新启动后广播消息，因此<strong>服务器代表该客户端</strong>终止所有远程计算</li><li>温和的轮回：客户端广播一条消息，因此服务器尝试找到其所有者并在找不到所有者的情况下将其杀死。(Server找client，找不到就kill了)</li><li>到期：每个RPC都有标准的时间片，如果它无法在指定的时间内完成，则必须明确请求另一个时间片</li><li>实际上，这些方法都不可取</li></ol></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wYgtgq.png" alt="wYgtgq"></li></ol></li><li>RPC扩展：异步RPC模型<ol><li>传统RPC中客户端和服务器之间的互连</li><li>异步RPC，在server收到RPC请求后，会立即将答复发送回客户端</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/jT4EO3.png" alt="jT4EO3"></li><li>延迟同步RPC：客户端和服务器通过两个异步RPC进行交互</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/SJ9Jwl.png" alt="SJ9Jwl"></li></ol></li></ol><h3 id="Maekawa’s-voting-algorithm"><a href="#Maekawa’s-voting-algorithm" class="headerlink" title="Maekawa’s voting algorithm"></a>Maekawa’s voting algorithm</h3><p>Maekawa投票算法，进程是否能进入临界区的投票算法。</p><ol><li>每个进程 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="1.803ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 797 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container> 关联一个选举集 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.984ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 877 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container><ol><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="6.553ex" height="1.984ex" role="img" focusable="false" viewBox="0 -683 2896.5 877"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1074.7, 0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2019.5, 0)"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container></li><li>任意两个选举集至少一个公共成员</li><li>每个进程的选举集大小相同</li><li>每个进程 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="1.91ex" height="1.666ex" role="img" focusable="false" viewBox="0 -442 844.3 736.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mi" transform="translate(503, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></svg></mjx-container>被包括在M个选举集中</li><li>找到 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.984ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 877 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>的集合等价于找出一个有限的N点投影平面。</li></ol></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hhhmRb.png" alt="hhhmRb"></li><li>如何找到 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex" xmlns="http://www.w3.org/2000/svg" width="1.984ex" height="1.902ex" role="img" focusable="false" viewBox="0 -683 877 840.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mi" transform="translate(583, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>的集合？<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/x7wiDQ.png" alt="x7wiDQ"></li></ol></li><li>容易死锁</li><li>满足安全性ME1</li><li>优化：如果按照发生在先的顺序对待回答的请求排队，就没有死锁，并且满足ME3</li><li>每次进临界区要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.206ex" xmlns="http://www.w3.org/2000/svg" width="5.07ex" height="2.398ex" role="img" focusable="false" viewBox="0 -969 2241 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="msqrt" transform="translate(500, 0)"><g transform="translate(853, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 109)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="888" height="60" x="853" y="849"></rect></g></g></g></svg></mjx-container>个消息，每次退出要 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.206ex" xmlns="http://www.w3.org/2000/svg" width="3.939ex" height="2.398ex" role="img" focusable="false" viewBox="0 -969 1741 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 109)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="888" height="60" x="853" y="849"></rect></g></g></g></svg></mjx-container>个消息。如果 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex" xmlns="http://www.w3.org/2000/svg" width="6.157ex" height="1.636ex" role="img" focusable="false" viewBox="0 -683 2721.6 723"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1165.8, 0)"><path data-c="3E" d="M84 520Q84 528 88 533T96 539L99 540Q106 540 253 471T544 334L687 265Q694 260 694 250T687 235Q685 233 395 96L107 -40H101Q83 -38 83 -20Q83 -19 83 -17Q82 -10 98 -1Q117 9 248 71Q326 108 378 132L626 250L378 368Q90 504 86 509Q84 513 84 520Z"></path></g><g data-mml-node="mn" transform="translate(2221.6, 0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g></g></g></svg></mjx-container>，那么 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.206ex" xmlns="http://www.w3.org/2000/svg" width="5.07ex" height="2.398ex" role="img" focusable="false" viewBox="0 -969 2241 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="msqrt" transform="translate(500, 0)"><g transform="translate(853, 0)"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g><g data-mml-node="mo" transform="translate(0, 109)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="888" height="60" x="853" y="849"></rect></g></g></g></svg></mjx-container>的总值要优于Ricart-Agrawala算法的 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="8.797ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3888.4 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mo" transform="translate(500, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(889, 0)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g><g data-mml-node="mo" transform="translate(1999.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(2999.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(3499.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li><li>同步延迟是一个往返时间，不是单个消息传播时间。</li></ol><h3 id="Impossibility-in-Asynchronous-Systems"><a href="#Impossibility-in-Asynchronous-Systems" class="headerlink" title="Impossibility in Asynchronous Systems"></a>Impossibility in Asynchronous Systems</h3><p>异步系统的不可能性</p><ul><li>在一个异步系统中，即使是只有一个进程出现崩溃故障，也<strong>没有算法能够保证达到共识</strong></li><li>在异步系统中，<strong>没有可以确保的方法</strong>来解决拜占庭将军问题、交互一致性问题或者全排序可靠组播问题</li><li>绕过不可能性结论的三个方法<ul><li>故障屏蔽<ul><li>事务系统使用持久储存保存信息</li></ul></li><li>利用故障检测器达到共识<ul><li>即使是使用不可靠的故障检测器，只要通信是可靠的，崩溃的进程不超过N / 2，那么异步系统中的共 识是可以解决的</li></ul></li><li>随机化进程各方面的行为，使得破坏进程者不能有效地实施他们的阻碍战术</li></ul></li></ul><h3 id="Checkpointing"><a href="#Checkpointing" class="headerlink" title="Checkpointing"></a>Checkpointing</h3><p>属于恢复算法中的后向恢复算法：把系统从当前的错误状态带到之前的正确状态</p><p>后向算法的优缺点：</p><ul><li>优势：普遍适用的机制</li><li>弱点：相对较高的成本，恢复循环，有些状态永远无法回滚到</li></ul><p>算法依赖于<strong>检查点的时间和频率</strong>以及<strong>检查点中保存的信息量</strong>，有不同类型的检查点算法。</p><ul><li>同步的检查点，即协调的检查点，所有进程同步地将其状态写入本地稳定存储。<ul><li>使用<strong>两阶段锁协议</strong>：<strong>协调者多播checkpoint_request</strong>，将它们正在执行的应用程序传递给它们的任何后续消息使用队列存储，然后将确认发送给协调者，<strong>协调者多播checkpoint_done</strong></li><li>使用非阻塞方法<br>– 分布式快照算法（例如Chandy-Lamport的算法）可用于协调检查点</li><li><strong>增量式快照算法</strong><ul><li>在<strong>两阶段锁协议</strong>中，协调者将<strong>checkpoint_request</strong>多播到那些依赖于协调者恢复的进程，即，协调器仅将检查点请求多播到<strong>它自上次使用检查点以来向其发送消息的那些进程</strong>。</li><li>当进程P<strong>接收</strong>到这样的请求时，它将转发给自最后一个检查点以来，<strong>P本身已向其发送消息的所有那些进程</strong>，依此类推。就是下面的p1第二次发给了p2和p4。</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WnsfHX.png" alt="WnsfHX"></li></ul></li></ul></li><li>异步的检查点：每个进程独立的检查点。<ul><li>不协调：完全独立的行为</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nG2eeG.png" alt="nG2eeG"></li><li>出现一个故障后，failed的进程Q通过恢复其最近的检查点来recover<ul><li>如果Q在其最近的检查点之后未发出任何消息，则恢复完成</li><li>如果Q向进程P发送消息m，则消息m已被P接收，但它之前未被Q发送（在Q恢复其检查点之后）。 这是不一致的。 因此，Q必须向已发送消息的所有进程发送一条消息，并要求它们roll back。</li><li>当请求P回滚时，它通过还原到其最新检查点来回滚。</li><li>如果P已将消息发送到其他进程，则这些其他受影响的进程也必须回滚。</li></ul></li><li>Domino多米诺效应难以避免。</li></ul></li></ul><h3 id="Stabilizing-algorithms"><a href="#Stabilizing-algorithms" class="headerlink" title="Stabilizing algorithms"></a>Stabilizing algorithms</h3><ol><li>分布式系统的所有可能配置或行为的集合可以分为两类：合法，非法。</li></ol><ul><li>非反应系统的合法配置通常由系统整体状态的不变性表示。</li><li>在无功系统中，合法配置不仅由状态谓词决定，而且还由行为决定。</li></ul><ol start="2"><li>行为良好的系统始终处于合法配置，但是由于以下原因，此类系统可能会切换到非法配置：瞬时故障，拓扑更改，环境更改。</li><li>当以下两个条件成立时，系统称为稳定化：</li></ol><ul><li>收敛：无论初始状态如何，以及无论在每个步骤中选择执行的合格操作如何，系统<strong>最终都会返回到合法配置</strong>。</li><li>关闭: 一旦处于合法配置，除非故障或干扰破坏了数据存储器，否则<strong>系统将继续处于合法配置</strong>。</li></ul><p>建立一棵生成树的稳定算法</p><ol><li>假设故障不会对网络进行分区，而是现在从连接的无向图G =（V，E）（其中| V | = n）构造一个以r为根的生成树，并且每个节点都知道其在树中的级别。</li><li>除根r之外的每个节点i都维护两个局部变量：L（i）：i的级别； P（i）：i的父亲。 根节点r具有L（i）= 0，并且没有父变量。</li><li>合法状态：如果父指针构成了以根为根的G的生成树，则除根之外的每个节点的级别都等于其父级的级别加1。</li><li>如果以下谓词为真，则系统达到合法状态：<ul><li><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="47.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 21146.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D43A" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q492 659 471 656T418 643T357 615T294 567T236 496T189 394T158 260Q156 242 156 221Q156 173 170 136T206 79T256 45T308 28T353 24Q407 24 452 47T514 106Q517 114 529 161T541 214Q541 222 528 224T468 227H431Q425 233 425 235T427 254Q431 267 437 273H454Q494 271 594 271Q634 271 659 271T695 272T707 272Q721 272 721 263Q721 261 719 249Q714 230 709 228Q706 227 694 227Q674 227 653 224Q646 221 643 215T629 164Q620 131 614 108Q589 6 586 3Q584 1 581 1Q571 1 553 21T530 52Q530 53 528 52T522 47Q448 -22 322 -22Q201 -22 126 55T50 252Z"></path></g><g data-mml-node="mi" transform="translate(786, 0)"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g><g data-mml-node="mi" transform="translate(1431, 0)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g><g data-mml-node="mo" transform="translate(2412.8, 0)"><path data-c="2261" d="M56 444Q56 457 70 464H707Q722 456 722 444Q722 430 706 424H72Q56 429 56 444ZM56 237T56 250T70 270H707Q722 262 722 250T707 230H70Q56 237 56 250ZM56 56Q56 71 72 76H706Q722 70 722 56Q722 44 707 36H70Q56 43 56 56Z"></path></g><g data-mml-node="mo" transform="translate(3468.6, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(3857.6, 0)"><path data-c="2200" d="M0 673Q0 684 7 689T20 694Q32 694 38 680T82 567L126 451H430L473 566Q483 593 494 622T512 668T519 685Q524 694 538 694Q556 692 556 674Q556 670 426 329T293 -15Q288 -22 278 -22T263 -15Q260 -11 131 328T0 673ZM414 410Q414 411 278 411T142 410L278 55L414 410Z"></path></g><g data-mml-node="mi" transform="translate(4413.6, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(4758.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(5203.2, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(5984, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(6539.8, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(7162.6, 0)"><path data-c="2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path></g><g data-mml-node="mi" transform="translate(8218.3, 0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(8891.6, 0)"><path data-c="2227" d="M318 591Q325 598 333 598Q344 598 348 591Q349 590 414 445T545 151T611 -4Q609 -22 591 -22Q588 -22 586 -21T581 -20T577 -17T575 -13T572 -9T570 -4L333 528L96 -4Q87 -20 80 -21Q78 -22 75 -22Q57 -22 55 -4Q55 2 120 150T251 444T318 591Z"></path></g><g data-mml-node="mi" transform="translate(9780.8, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(10561.6, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(11617.3, 0)"><path data-c="1D443" d="M287 628Q287 635 230 637Q206 637 199 638T192 648Q192 649 194 659Q200 679 203 681T397 683Q587 682 600 680Q664 669 707 631T751 530Q751 453 685 389Q616 321 507 303Q500 302 402 301H307L277 182Q247 66 247 59Q247 55 248 54T255 50T272 48T305 46H336Q342 37 342 35Q342 19 335 5Q330 0 319 0Q316 0 282 1T182 2Q120 2 87 2T51 1Q33 1 33 11Q33 13 36 25Q40 41 44 43T67 46Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628ZM645 554Q645 567 643 575T634 597T609 619T560 635Q553 636 480 637Q463 637 445 637T416 636T404 636Q391 635 386 627Q384 621 367 550T332 412T314 344Q314 342 395 342H407H430Q542 342 590 392Q617 419 631 471T645 554Z"></path></g><g data-mml-node="mo" transform="translate(12368.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(12757.3, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(13102.3, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(13769.1, 0)"><path data-c="3A" d="M78 370Q78 394 95 412T138 430Q162 430 180 414T199 371Q199 346 182 328T139 310T96 327T78 370ZM78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mi" transform="translate(14324.9, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(15005.9, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(15394.9, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(15739.9, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(16406.7, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mi" transform="translate(17462.4, 0)"><path data-c="1D43F" d="M228 637Q194 637 192 641Q191 643 191 649Q191 673 202 682Q204 683 217 683Q271 680 344 680Q485 680 506 683H518Q524 677 524 674T522 656Q517 641 513 637H475Q406 636 394 628Q387 624 380 600T313 336Q297 271 279 198T252 88L243 52Q243 48 252 48T311 46H328Q360 46 379 47T428 54T478 72T522 106T564 161Q580 191 594 228T611 270Q616 273 628 273H641Q647 264 647 262T627 203T583 83T557 9Q555 4 553 3T537 0T494 -1Q483 -1 418 -1T294 0H116Q32 0 32 10Q32 17 34 24Q39 43 44 45Q48 46 59 46H65Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Q285 635 228 637Z"></path></g><g data-mml-node="mo" transform="translate(18143.4, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(18532.4, 0)"><path data-c="1D45D" d="M23 287Q24 290 25 295T30 317T40 348T55 381T75 411T101 433T134 442Q209 442 230 378L240 387Q302 442 358 442Q423 442 460 395T497 281Q497 173 421 82T249 -10Q227 -10 210 -4Q199 1 187 11T168 28L161 36Q160 35 139 -51T118 -138Q118 -144 126 -145T163 -148H188Q194 -155 194 -157T191 -175Q188 -187 185 -190T172 -194Q170 -194 161 -194T127 -193T65 -192Q-5 -192 -24 -194H-32Q-39 -187 -39 -183Q-37 -156 -26 -148H-6Q28 -147 33 -136Q36 -130 94 103T155 350Q156 355 156 364Q156 405 131 405Q109 405 94 377T71 316T59 280Q57 278 43 278H29Q23 284 23 287ZM178 102Q200 26 252 26Q282 26 310 49T356 107Q374 141 392 215T411 325V331Q411 405 350 405Q339 405 328 402T306 393T286 380T269 365T254 350T243 336T235 326L232 322Q232 321 229 308T218 264T204 212Q178 106 178 102Z"></path></g><g data-mml-node="mo" transform="translate(19035.4, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(19646.7, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(20646.9, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container></li></ul></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OzvLpE.png" alt="OzvLpE"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/vTuETi.png" alt="vTuETi"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/KIVbcG.png" alt="KIVbcG"></li></ol><h3 id="Chord"><a href="#Chord" class="headerlink" title="Chord"></a>Chord</h3><p>chord characteristics 和弦特征</p><ol><li>简单，可证明的正确性和性能</li><li>源自一致性哈希算法，用于解决负载平衡load balance、分散化decentralization、可扩展性scalability、availability、灵活的命名flexible naming</li><li>一致性哈希需要满足的4个条件：<ol><li>均衡性balance：哈希的结果能够尽可能分布到所有的缓冲中</li><li>单调性monotonicity：缓冲区大小变化的时候，应尽量保护已经分配的内容不会被重新映射到新缓冲区</li><li>分散性低：避免由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致</li><li>负载低：对于一个特定的缓冲区，避免被不同用户映射为不同的内容</li></ol></li><li>整个哈希值空间组成一个虚拟的圆环，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间的数据，其他数据不受影响</li><li>哈希的key被分配到恰好的环中位置，或者是接下来的位置，节点被称为successor node of key k, successor(k)</li><li>改进：finger table，每个节点维护一个fingertable，最多m行，第i行包含了succeeds n最少 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0" xmlns="http://www.w3.org/2000/svg" width="3.841ex" height="1.887ex" role="img" focusable="false" viewBox="0 -833.9 1697.6 833.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mn"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="TeXAtom" transform="translate(500, 363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(1123, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></g></g></svg></mjx-container>距离的第一个节点的标识符<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5eja12.png" alt="5eja12"></li><li>节点加入：<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/we0tPc.png" alt="we0tPc"></li><li>节点离开：<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/6V72yh.png" alt="6V72yh"></li></ol></li></ol><h3 id="Raft"><a href="#Raft" class="headerlink" title="Raft"></a>Raft</h3><ul><li>目标<ul><li>分布式共识：即使某些计算机出现故障，也可以使计算机集合作为一个连贯的组工作，提供连续的服务</li><li>可理解性：直觉，易于解释</li></ul></li><li>技术<ul><li>问题分解：领导者选举，日志复制（正常操作），安全性（保持日志顺序一致性）</li><li>最小化状态空间<ul><li>通过单一机制处理多个问题</li><li>消除特殊情况</li><li>最大化连贯性</li><li>最小化不确定性</li></ul></li></ul></li><li>Raft基础：服务器状态和RPCs</li><li>看PPT期末考点那个</li></ul><h3 id="Edge-chasing-algorithm"><a href="#Edge-chasing-algorithm" class="headerlink" title="Edge-chasing algorithm"></a>Edge-chasing algorithm</h3><ul><li>看PPT期末考点那个</li></ul>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;UCAS并行与分布式计算期末复习&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="UCAS" scheme="https://hanielxx.com/tags/UCAS/"/>
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
    <category term="Review" scheme="https://hanielxx.com/tags/Review/"/>
    
    <category term="ParallelComputing" scheme="https://hanielxx.com/tags/ParallelComputing/"/>
    
    <category term="DistributedSystem" scheme="https://hanielxx.com/tags/DistributedSystem/"/>
    
  </entry>
  
  <entry>
    <title>UCAS算法设计与分析</title>
    <link href="https://hanielxx.com/Notes/2020-12-24-ucas-algorithm"/>
    <id>https://hanielxx.com/Notes/2020-12-24-ucas-algorithm</id>
    <published>2020-12-24T03:42:24.000Z</published>
    <updated>2021-05-17T15:34:23.084Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>简单知识点大纲总结。</p><p>新增：期末考试题总结。</p></div><a id="more"></a><h2 id="期末考试题"><a href="#期末考试题" class="headerlink" title="期末考试题"></a>期末考试题</h2><p>这考试，复习和不复习的结果是一样的，绝了，说好是作业题的变形，和作业几乎没关系。。保佑及格。。</p><ul><li>第一题分治卡了快40分钟，<a href="https://leetcode-cn.com/problems/median-of-two-sorted-arrays/" target="_blank" rel="external nofollow noopener noreferrer">LeetCode第四题</a></li><li>第二题动态规划，给个数组，算相邻两个数的差，要使得相邻的两个差值异号，并且要求是连续的，如果是0也不算。简单的dp，难得有道正常的题。</li><li>第三题贪心，<a href="https://codeforces.ml/problemset/problem/1426/E" target="_blank" rel="external nofollow noopener noreferrer">codeforce 1426E</a>，剪刀石头布，两个人，固定各自的出手次数为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="17.011ex" height="2.009ex" role="img" focusable="false" viewBox="0 -694 7518.7 888"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(932.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1377.2, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2309.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2754.4, 0)"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(529, -150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g><g data-mml-node="mo" transform="translate(3687, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4131.7, 0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(429, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(4964.2, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(5408.9, 0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(429, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(6241.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(6686.1, 0)"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"></path></g><g data-mml-node="mn" transform="translate(429, -150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></svg></mjx-container>，对应剪刀石头布三个<ul><li>第一问，求第一个人的最大能赢的次数，只要贪赢就行。</li><li>第二问，求第一个人的最少能赢的次数，这个比较难想明白。。。一直从正面想，也卡了好久。</li><li>排除输的和平局，反面思路，<img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/j2GFK1.png" alt="j2GFK1"></li><li><a href="https://www.cnblogs.com/airisa/p/13766028.html" target="_blank" rel="external nofollow noopener noreferrer">正面思路</a></li></ul></li><li>第四问LP，设置停车场位置，固定n栋楼位置为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex" xmlns="http://www.w3.org/2000/svg" width="11.812ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 5220.7 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(975.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1420.2, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2395.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(2840.4, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(3285.1, 0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path></g><g data-mml-node="mo" transform="translate(3729.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4174.4, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g></g></g></svg></mjx-container>，在每个楼距离<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex" xmlns="http://www.w3.org/2000/svg" width="1.02ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 451 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g></g></g></svg></mjx-container>范围内设置一个停车场，每个楼都要设置一个，然后要使得最大的任意两个停车场之间的距离最小化。包含去绝对值，还有去max。max不知道怎么去，写了个min max。。。<ul><li><a href="https://zhuanlan.zhihu.com/p/69397833" target="_blank" rel="external nofollow noopener noreferrer">优化 | 线性规划和整数规划的若干建模技巧</a></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1wN6CI.png" alt="1wN6CI"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/BacZDj.png" alt="BacZDj"></li></ul></li><li>第五题是树状DP，题目都理解反了，把neighbor理解成了兄弟节点，没想到是节点编号的neighbor。。。也是两小问，第二问好像是贪心。</li><li>第六题是括号匹配，只有第三小问比较难，但是没啥时间写了。。听说是卡特兰数。</li></ul><h2 id="Lec1-Introduction-and-some-representative-problems"><a href="#Lec1-Introduction-and-some-representative-problems" class="headerlink" title="Lec1-Introduction-and-some-representative-problems"></a>Lec1-Introduction-and-some-representative-problems</h2><ol><li>TSP问题<ol><li>没有环，分治<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/uQQylV.png" alt="uQQylV"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Mn9REW.png" alt="Mn9REW"></li><li>改进策略，2-opt，只允许两条边不同<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/o5Vm8l.png" alt="o5Vm8l"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/dYMtdy.png" alt="dYMtdy"></li><li>聪明的枚举策略，就是先按照每步的顺序，构造多步决策树，然后剪枝<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/2WhZ9j.png" alt="2WhZ9j"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/g4uP48.png" alt="g4uP48"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XlXxK6.png" alt="XlXxK6"></li><li>回溯策略，考虑第一步之后，枚举所有可能的情况，不用先把每一步排好序，上面的智能枚举是枚举每一步走不走，是二叉树，这里不是。这里解的形式是<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="14.086ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 6226 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"></path></g><g data-mml-node="msub" transform="translate(278, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(1253.6, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1698.2, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mn" transform="translate(572, -150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(2673.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mo" transform="translate(3118.4, 0)"><path data-c="2026" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60ZM525 60Q525 84 542 102T585 120Q609 120 627 104T646 61Q646 36 629 18T586 0T543 17T525 60ZM972 60Q972 84 989 102T1032 120Q1056 120 1074 104T1093 61Q1093 36 1076 18T1033 0T990 17T972 60Z"></path></g><g data-mml-node="mo" transform="translate(4457.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(4901.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(5948, 0)"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"></path></g></g></g></svg></mjx-container>的形式<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hGZrt4.png" alt="hGZrt4"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kYdsmb.png" alt="kYdsmb"><br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rfE54X.png" alt="rfE54X"></li></ol></li></ol><h2 id="Lec5-Divide-And-Conquer"><a href="#Lec5-Divide-And-Conquer" class="headerlink" title="Lec5-Divide-And-Conquer"></a>Lec5-Divide-And-Conquer</h2><ol><li>merge sort<ol><li>分治的划分，如果是数组就是看是按照值分还是按照下标分，要选择好的划分点，不能线性规模下降，要指数规模下降</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gYEk0N.png" alt="gYEk0N"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/erKmMR.png" alt="erKmMR"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/EKQn9K.png" alt="EKQn9K"></li></ol></li><li>逆序数<ol><li>划分成两部分，分别计算逆序数</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ZnUNL4.png" alt="ZnUNL4"></li><li>合并的时候有两种策略<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/15oiuY.png" alt="15oiuY"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zOHSb0.png" alt="zOHSb0"></li></ol></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/m3twg4.png" alt="m3twg4"></li></ol></li><li>快排<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/bMQS0k.png" alt="bMQS0k"></li><li>时间复杂度<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3lnboF.png" alt="3lnboF"></li><li>快排的优化<ol><li>pivot位置，在 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.781ex" xmlns="http://www.w3.org/2000/svg" width="7.476ex" height="2.736ex" role="img" focusable="false" viewBox="0 -864.2 3304.5 1209.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mfrac"><g data-mml-node="mi" transform="translate(220, 394) scale(0.707)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mn" transform="translate(255.4, -345) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><rect width="624.3" height="60" x="120" y="220"></rect></g><g data-mml-node="mo" transform="translate(1086.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mfrac" transform="translate(2086.7, 0)"><g data-mml-node="mrow" transform="translate(220, 394) scale(0.707)"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(500, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mn" transform="translate(432.1, -345) scale(0.707)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><rect width="977.8" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>足够好<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9EjVF8.png" alt="9EjVF8"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/noPZWg.png" alt="noPZWg"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YypPpb.png" alt="YypPpb"></li><li>就地算法</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/gFN0OD.png" alt="gFN0OD"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/rHxjYo.png" alt="rHxjYo"></li></ol></li></ol></li><li>第k大数<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ouEAYN.png" alt="ouEAYN"></li><li>和快排差不多，只要看返回的pivot是哪个位置，如果整好是k，就返回这个位置，否则只要再递归找其中一边的就好了</li><li>优化</li><li>对于pivot位置进行限制，选接近中心点的位置作为pivot，如何获取这个pivot，有三种方法<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HQP3Ph.png" alt="HQP3Ph"></li><li>分组，然后取中点的中点来近似全局的median</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/yZ78ME.png" alt="yZ78ME"></li><li>inplace的方法：BFPRT算法</li><li>随机选择pivot</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8iwQyx.png" alt="8iwQyx"></li><li>随机采样的方法选pivot：Lazy Select算法</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/jhgoxo.png" alt="jhgoxo"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/MskR14.png" alt="MskR14"></li></ol></li></ol></li><li>乘法和矩阵乘法</li><li>找最邻近点对<ol><li>划分成两个大小相等的子集</li><li>合并过程是在每个子集找最近点对<ol><li>左边的，右边的，还有横跨中间的</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/akaqzc.png" alt="akaqzc"></li><li>如何加快combine？</li><li>条带法去冗余</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8aFF7E.png" alt="8aFF7E"></li><li>每个点不需要探索所有的地方</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/jhu9Lf.png" alt="jhu9Lf"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/4AaTqF.png" alt="4AaTqF"></li></ol></li></ol></li><li>FFT, DFT一系列，不考</li></ol><h2 id="Lec6-Dynamic-Programming"><a href="#Lec6-Dynamic-Programming" class="headerlink" title="Lec6-Dynamic-Programming"></a>Lec6-Dynamic-Programming</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/cifljM.png" alt="cifljM"></p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/HTalBV.png" alt="HTalBV"></p><ol><li>导弹分配问题<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/K8ag9u.png" alt="K8ag9u"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/e6adxN.png" alt="e6adxN"></li></ol></li><li>矩阵链乘<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hikk3v.png" alt="hikk3v"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mYefSH.png" alt="mYefSH"></li><li>DP方法</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/N9xJdb.png" alt="N9xJdb"></li><li>直接递归DP，指数级别</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/F4xLWq.png" alt="F4xLWq"></li><li>去冗余，记忆化OPT表，多项式级别</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/R1LtUq.png" alt="R1LtUq"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WcGjoj.png" alt="WcGjoj"></li><li>自底向上unroll recursion tree</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/hrwxY0.png" alt="hrwxY0"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0FPsED.png" alt="0FPsED"></li></ol></li><li>背包问题<ol><li>0-1背包<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3kX04r.png" alt="3kX04r"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/yLhTPi.png" alt="yLhTPi"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/joEQWM.png" alt="joEQWM"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/w5sWO2.png" alt="w5sWO2"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Sn7pQE.png" alt="Sn7pQE"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7VIou9.png" alt="7VIou9"></li><li>空间可以用滚动数组优化到<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex" xmlns="http://www.w3.org/2000/svg" width="4.844ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 2141 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D442" d="M740 435Q740 320 676 213T511 42T304 -22Q207 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435ZM637 476Q637 565 591 615T476 665Q396 665 322 605Q242 542 200 428T157 216Q157 126 200 73T314 19Q404 19 485 98T608 313Q637 408 637 476Z"></path></g><g data-mml-node="mo" transform="translate(763, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1152, 0)"><path data-c="1D45B" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(1752, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></li></ol></li><li>完全背包<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Dl6bo1.png" alt="Dl6bo1"></li></ol></li></ol></li><li>顶点覆盖问题<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/FOSUJU.png" alt="FOSUJU"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/cuG5st.png" alt="cuG5st"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/l6vTKY.png" alt="l6vTKY"></li></ol></li><li>字符串序列对齐问题<ol><li>方法和最长公共子序列差不多，回溯的方法也是，只不过LCS的权重为1</li><li>当前字符i和j，匹配、错配、不匹配且删除i、不匹配且删除j</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/XABQ4B.png" alt="XABQ4B"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/jjop5h.png" alt="jjop5h"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/zdpaLj.png" alt="zdpaLj"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/r2EuCA.png" alt="r2EuCA"></li><li>回溯</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/pVAjQp.png" alt="pVAjQp"></li></ol></li><li>高级动态规划Advanced DP<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/vubpu6.png" alt="vubpu6"></li><li>缩减空间<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/5kI0RY.png" alt="5kI0RY"></li><li>后缀对齐<br><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mIjgJr.png" alt="mIjgJr"></li><li>对于只用两列数组的DP问题，回溯方法<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/ch0N7m.png" alt="ch0N7m"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/08cIl9.png" alt="08cIl9"></li></ol></li><li>进一步的优化略过</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/2LU133.png" alt="2LU133"></li></ol></li><li>banded dp</li><li>LCS问题<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/3036uS.png" alt="3036uS"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DdSkOu.png" alt="DdSkOu"></li><li>Sparse DP</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QGNdMw.png" alt="QGNdMw"></li></ol></li><li>单源最短路径<ol><li>如果是有向无环图，相当于拓扑排序<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tKgtJM.png" alt="tKgtJM"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/6jp2DH.png" alt="6jp2DH"></li></ol></li><li>如果有环存在，那上面的就无法处理，要把子问题划分的更小<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9cW0dq.png" alt="9cW0dq"></li><li>限制步数，Bellman-Ford算法</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UhEdWi.png" alt="UhEdWi"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/8tHLqW.png" alt="8tHLqW"></li></ol></li><li>对于负圈的问题<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LsaTcC.png" alt="LsaTcC"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/wIGHkF.png" alt="wIGHkF"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/iqmMP9.png" alt="iqmMP9"></li></ol></li></ol></li></ol><h2 id="线性规划LP"><a href="#线性规划LP" class="headerlink" title="线性规划LP"></a>线性规划LP</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/YejHvp.png" alt="YejHvp"></p><ol><li>Diet问题</li><li>最大流</li><li>最小化费流</li><li>多物品流</li><li>SAT</li><li>基因重排序距离问题</li><li>ILP整数线性规划</li><li><strong>单纯形算法</strong>，这个比较难</li><li>对偶问题</li></ol><h2 id="LP和拉格朗日对偶"><a href="#LP和拉格朗日对偶" class="headerlink" title="LP和拉格朗日对偶"></a>LP和拉格朗日对偶</h2><p>其他的不管了，不考，直接记一下公式和对偶怎么用拉格朗日推出来</p><ol><li>对偶公式<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/z0CwhR.png" alt="z0CwhR"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9Yx4Lr.png" alt="9Yx4Lr"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/OMSSfr.png" alt="OMSSfr"></li></ol></li><li>拉格朗日对偶<ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tkRLPR.png" alt="tkRLPR"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/nyQPih.png" alt="nyQPih"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/DJX00X.png" alt="DJX00X"></li></ol></li></ol><h2 id="证明方法"><a href="#证明方法" class="headerlink" title="证明方法"></a>证明方法</h2><ol><li>循环不变量<ol><li>分为循环不变量和递归不变量</li><li>分治里有用到，可以用来证明包含循环或者递归的算法</li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eunmgn.png" alt="eunmgn"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/brshyj.png" alt="brshyj"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Fnihhm.png" alt="Fnihhm"></li><li>分为三步骤<ol><li>初始化</li><li>维持</li><li>终止</li></ol></li></ol></li><li>数学归纳法</li><li>交换论证法</li><li>反证法</li></ol><h2 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h2><p>对于递归问题</p><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/IiwZoZ.png" alt="IiwZoZ"></p><ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/UTVKGR.png" alt="UTVKGR"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/MEsm6O.png" alt="MEsm6O"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/eE3E2I.png" alt="eE3E2I"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/D1O3eK.png" alt="D1O3eK"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lUK62z.png" alt="lUK62z"></li></ol><h2 id="Take-Home-Message"><a href="#Take-Home-Message" class="headerlink" title="Take Home Message"></a>Take Home Message</h2><ol><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/qO1Q4S.png" alt="qO1Q4S"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/kowSU1.png" alt="kowSU1"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/WKn8V3.png" alt="WKn8V3"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/RAUwtC.png" alt="RAUwtC"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/9jf8s1.png" alt="9jf8s1"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Pqwd6Q.png" alt="Pqwd6Q"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/1rndvv.png" alt="1rndvv"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/SRxNMi.png" alt="SRxNMi"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/mNljoP.png" alt="mNljoP"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/Jt2RTM.png" alt="Jt2RTM"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/tk0TMv.png" alt="tk0TMv"></li><li><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/QZ6zIV.png" alt="QZ6zIV"></li></ol>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;简单知识点大纲总结。&lt;/p&gt;&lt;p&gt;新增：期末考试题总结。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="Algorithm" scheme="https://hanielxx.com/tags/Algorithm/"/>
    
    <category term="Math" scheme="https://hanielxx.com/tags/Math/"/>
    
    <category term="UCAS" scheme="https://hanielxx.com/tags/UCAS/"/>
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>UCAS算法OJ题目和代码</title>
    <link href="https://hanielxx.com/Notes/2020-12-22-ucas-algorithm-oj-code"/>
    <id>https://hanielxx.com/Notes/2020-12-22-ucas-algorithm-oj-code</id>
    <published>2020-12-22T03:42:24.000Z</published>
    <updated>2021-05-17T15:34:23.084Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><p>直接导出pdf太不方便看代码，直接放这里看了。</p></div><a id="more"></a><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/lmYVa4.png" alt="lmYVa4"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXN = <span class="number">5000001</span>;</span><br><span class="line"><span class="keyword">int</span> vec[MAXN];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> pivot = a[i];</span><br><span class="line">    <span class="keyword">while</span> (i &lt; j)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; a[j] &lt;= pivot)</span><br><span class="line">            j--;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j)</span><br><span class="line">            a[i++] = a[j];</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; a[i] &gt;= pivot)</span><br><span class="line">            i++;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j)</span><br><span class="line">            a[j--] = a[i];</span><br><span class="line">    }</span><br><span class="line">    a[i] = pivot;</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">quick_sort</span><span class="params">(<span class="keyword">int</span> a[], <span class="keyword">int</span> l, <span class="keyword">int</span> r)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> pivot_pos;</span><br><span class="line">    <span class="keyword">if</span> (l &lt; r)</span><br><span class="line">    {</span><br><span class="line">        pivot_pos = partition(a, l, r);</span><br><span class="line">        quick_sort(a, l, pivot_pos - <span class="number">1</span>);</span><br><span class="line">        quick_sort(a, pivot_pos + <span class="number">1</span>, r);</span><br><span class="line">    }</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">const</span> <span class="keyword">char</span> **argv)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n, k;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d%d"</span>, &amp;n, &amp;k);</span><br><span class="line">    <span class="keyword">if</span> (n == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;vec[i]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方法1，快排，时间是1914ms，ac</span></span><br><span class="line">    <span class="comment">// quick_sort(vec, 0, n - 1);</span></span><br><span class="line">    <span class="comment">// printf("%d", vec[k - 1]);</span></span><br><span class="line">    <span class="comment">// for (int i = 0; i &lt; n; i++)</span></span><br><span class="line">    <span class="comment">// {</span></span><br><span class="line">    <span class="comment">//     printf("%d ", vec[i]);</span></span><br><span class="line">    <span class="comment">// }</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方法2，快速选择，就是剪枝的快排，第k大在哪个区间就排哪个，另一个区间不排序，时间是1099ms，ac</span></span><br><span class="line">    <span class="keyword">int</span> l = <span class="number">0</span>, r = n - <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">int</span> pivot_pos = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">    {</span><br><span class="line">        pivot_pos = partition(vec, l, r);</span><br><span class="line">        <span class="keyword">if</span> (pivot_pos == k - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (pivot_pos &gt; k - <span class="number">1</span>)</span><br><span class="line">            r = pivot_pos - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            l = pivot_pos + <span class="number">1</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d"</span>, vec[pivot_pos]);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 方法3，STL的sort，时间1801ms，ac</span></span><br><span class="line">    <span class="comment">// sort(vec, vec + n);</span></span><br><span class="line">    <span class="comment">// printf("%d", vec[n-k]);</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><h2 id="Divide-and-Conquer"><a href="#Divide-and-Conquer" class="headerlink" title="Divide and Conquer"></a>Divide and Conquer</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/W4CxsA.png" alt="W4CxsA"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 普通公式法 TLE</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">normalPower</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> base, <span class="keyword">long</span> <span class="keyword">long</span> power)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> result = <span class="number">1</span>;</span><br><span class="line">    base = <span class="built_in">abs</span>(base) % <span class="number">1000</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= power; i++)</span><br><span class="line">    {</span><br><span class="line">        result = result * base;</span><br><span class="line">        result = result % <span class="number">1000</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> result % <span class="number">1000</span>;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment">// 快速幂 29ms</span></span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">fastPower</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> base, <span class="keyword">long</span> <span class="keyword">long</span> power)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> result = <span class="number">1</span>;</span><br><span class="line">    base = <span class="built_in">abs</span>(base) % <span class="number">1000</span>;</span><br><span class="line">    <span class="keyword">while</span> (power &gt; <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (power &amp; <span class="number">1</span>)</span><br><span class="line">            <span class="comment">//等价于if(power%2==1)</span></span><br><span class="line">            result = result * base % <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">        power &gt;&gt;= <span class="number">1</span>; <span class="comment">//等价于power=power/2</span></span><br><span class="line">        base = (base * base) % <span class="number">1000</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> base, power;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%lld %lld"</span>, &amp;base, &amp;power);</span><br><span class="line">    <span class="keyword">if</span> (base == <span class="number">0</span> &amp;&amp; power == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (power == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d"</span>, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lld"</span>, fastPower(base, power));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/PKEk0p.png" alt="PKEk0p"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> ull;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> MAXN = <span class="number">5000001</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">p</span></span></span><br><span class="line"><span class="class">{</span></span><br><span class="line">    ll x;</span><br><span class="line">    ll y;</span><br><span class="line">    ull dis;</span><br><span class="line">};</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">p</span> <span class="title">points</span>[<span class="title">MAXN</span>];</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(p a[], <span class="keyword">int</span> i, <span class="keyword">int</span> j)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    p pivot = a[i];</span><br><span class="line">    <span class="keyword">while</span> (i &lt; j)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; a[j].dis &gt;= pivot.dis)</span><br><span class="line">            j--;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j)</span><br><span class="line">            a[i++] = a[j];</span><br><span class="line">        <span class="keyword">while</span> (i &lt; j &amp;&amp; a[i].dis &lt;= pivot.dis)</span><br><span class="line">            i++;</span><br><span class="line">        <span class="keyword">if</span> (i &lt; j)</span><br><span class="line">            a[j--] = a[i];</span><br><span class="line">    }</span><br><span class="line">    a[i] = pivot;</span><br><span class="line">    <span class="keyword">return</span> i;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(p a, p b)</span></span>{</span><br><span class="line">    <span class="keyword">return</span> a.dis &lt; b.dis;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// 读数据</span></span><br><span class="line">    <span class="keyword">int</span> N, K;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d %d"</span>, &amp;N, &amp;K);</span><br><span class="line">    <span class="built_in">memset</span>(points, <span class="number">0</span>, <span class="keyword">sizeof</span>(struct p) * MAXN);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%lld %lld"</span>, &amp;points[i].x, &amp;points[i].y);</span><br><span class="line">        points[i].dis = points[i].x * points[i].x + points[i].y * points[i].y;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 快速选择，就是剪枝的快排，第k大在哪个区间就排哪个，另一个区间不排序</span></span><br><span class="line">    <span class="comment">// int l = 0, r = N - 1;</span></span><br><span class="line">    <span class="comment">// int pivot_pos = 0;</span></span><br><span class="line">    <span class="comment">// while (true)</span></span><br><span class="line">    <span class="comment">// {</span></span><br><span class="line">    <span class="comment">//     pivot_pos = partition(points, l, r);</span></span><br><span class="line">    <span class="comment">//     if (pivot_pos == K - 1)</span></span><br><span class="line">    <span class="comment">//         break;</span></span><br><span class="line">    <span class="comment">//     else if (pivot_pos &gt; K - 1)</span></span><br><span class="line">    <span class="comment">//         r = pivot_pos - 1;</span></span><br><span class="line">    <span class="comment">//     else</span></span><br><span class="line">    <span class="comment">//         l = pivot_pos + 1;</span></span><br><span class="line">    <span class="comment">// }</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// printf("%lld %lld", points[pivot_pos].x, points[pivot_pos].y);</span></span><br><span class="line">    sort(points, points + N, cmp);</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%lld %lld"</span>, points[K - <span class="number">1</span>].x, points[K - <span class="number">1</span>].y);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p>LeetCode上有个一样的题，有三种解法，<a href="https://leetcode-cn.com/problems/k-closest-points-to-origin/solution/zui-jie-jin-yuan-dian-de-k-ge-dian-by-leetcode-sol/" target="_blank" rel="external nofollow noopener noreferrer">题解链接</a></p><ul><li>排序</li><li>优先队列</li><li>快速选择</li></ul><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//快速选择算法</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> {</span></span><br><span class="line">    <span class="keyword">private</span>:</span><br><span class="line">        mt19937 gen{random_device{}()};</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">dis</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; <span class="built_in">point</span>)</span></span>{</span><br><span class="line">            <span class="keyword">return</span> <span class="built_in">point</span>[<span class="number">0</span>] * <span class="built_in">point</span>[<span class="number">0</span>] + <span class="built_in">point</span>[<span class="number">1</span>] * <span class="built_in">point</span>[<span class="number">1</span>];</span><br><span class="line">        }</span><br><span class="line">        <span class="function"><span class="keyword">int</span> <span class="title">partition</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; points, <span class="keyword">int</span> left, <span class="keyword">int</span> right)</span> </span></span><br><span class="line"><span class="function">        </span>{</span><br><span class="line">            <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; pivot = points[left];</span><br><span class="line">            <span class="keyword">int</span> pivot_dis = dis(points[left]);</span><br><span class="line">            <span class="keyword">while</span> (left &lt; right)</span><br><span class="line">            {</span><br><span class="line">                <span class="keyword">while</span> (left &lt; right &amp;&amp; dis(points[right]) &gt;= pivot_dis)</span><br><span class="line">                    right--;</span><br><span class="line">                <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">                    points[left++] = points[right];</span><br><span class="line">                <span class="keyword">while</span> (left &lt; right &amp;&amp; dis(points[left]) &lt;= pivot_dis)</span><br><span class="line">                    left++;</span><br><span class="line">                <span class="keyword">if</span> (left &lt; right)</span><br><span class="line">                    points[right--] = points[left];</span><br><span class="line">            }</span><br><span class="line">            points[left] = pivot;</span><br><span class="line">            <span class="keyword">return</span> left;</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; <span class="title">kClosest</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt;&amp; points, <span class="keyword">int</span> K)</span> </span>{</span><br><span class="line">            <span class="keyword">int</span> n = points.<span class="built_in">size</span>();</span><br><span class="line">            <span class="keyword">int</span> l = <span class="number">0</span>, r = n - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">int</span> pivot_pos;</span><br><span class="line">            <span class="keyword">while</span> (<span class="literal">true</span>)</span><br><span class="line">            {</span><br><span class="line">                pivot_pos = partition(points, l, r);</span><br><span class="line">                <span class="keyword">if</span> (pivot_pos == K - <span class="number">1</span>)</span><br><span class="line">                    <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">else</span> <span class="keyword">if</span> (pivot_pos &gt; K - <span class="number">1</span>)</span><br><span class="line">                    r = pivot_pos - <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span></span><br><span class="line">                    l = pivot_pos + <span class="number">1</span>;</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">return</span> {points.<span class="built_in">begin</span>(), points.<span class="built_in">begin</span>() + K};</span><br><span class="line">        }</span><br><span class="line">};</span><br></pre></td></tr></table></figure><h2 id="DP"><a href="#DP" class="headerlink" title="DP"></a>DP</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0g5KhH.png" alt="0g5KhH"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">10001</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">climb_stair</span><span class="params">(<span class="keyword">int</span> *stones, <span class="keyword">int</span> <span class="built_in">size</span>)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">// 如果最后dp[n]非空就可以</span></span><br><span class="line">    <span class="comment">// 可以自己用opt表举例子</span></span><br><span class="line">    <span class="keyword">int</span> n = stones[<span class="built_in">size</span> - <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">if</span> ((<span class="number">1</span> + <span class="built_in">size</span>) * <span class="built_in">size</span> / <span class="number">2</span> &lt; n)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    <span class="comment">// dp vector保存每个石头的jump size，就是到这个位置的k值</span></span><br><span class="line">    <span class="function"><span class="built_in">vector</span>&lt;<span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; &gt; <span class="title">dp</span><span class="params">(n + <span class="number">1</span>)</span></span>;</span><br><span class="line">    dp[<span class="number">1</span>].insert(<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; <span class="built_in">size</span>; i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (<span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;::iterator iter = dp[stones[i]].<span class="built_in">begin</span>(); iter != dp[stones[i]].<span class="built_in">end</span>(); ++iter)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> k = *iter;</span><br><span class="line">            <span class="keyword">if</span> (stones[i] + k - <span class="number">1</span> &lt;= n)</span><br><span class="line">                dp[stones[i] + k - <span class="number">1</span>].insert(k - <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span> (stones[i] + k &lt;= n)</span><br><span class="line">                dp[stones[i] + k].insert(k);</span><br><span class="line">            <span class="keyword">if</span> (stones[i] + k + <span class="number">1</span> &lt;= n)</span><br><span class="line">                dp[stones[i] + k + <span class="number">1</span>].insert(k + <span class="number">1</span>);</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> !dp[n].empty();</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="keyword">int</span> w[N];</span><br><span class="line">    <span class="built_in">memset</span>(w, <span class="number">0</span>, <span class="keyword">sizeof</span>(<span class="keyword">int</span>) * N);</span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n) == <span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">int</span> a = <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;w[i]);</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">bool</span> res = climb_stair(w, n);</span><br><span class="line">        <span class="keyword">if</span> (res == <span class="literal">true</span>)</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"true"</span>);</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">"false"</span>);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/C4z9Uw.png" alt="C4z9Uw"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> ll long long</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="comment">//这里会卡long long下界，设置成longlong最小值</span></span><br><span class="line">    ll w=<span class="number">0</span>, del_sum,no_del_sum;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%lld"</span>, &amp;w);</span><br><span class="line">    ll max_sum = w, max_remove_one = w, remove_one_sum = w;</span><br><span class="line">    ll max_data = <span class="number">-9223372036854775808</span>;</span><br><span class="line">    max_data = <span class="built_in">max</span>(max_data, w);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">scanf</span>(<span class="string">"%lld"</span>, &amp;w) != EOF)</span><br><span class="line">    {</span><br><span class="line">        <span class="comment">// max_data保存数组最大值，如果数组全负，则返回最大值</span></span><br><span class="line">        max_data = <span class="built_in">max</span>(max_data, w);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 最多删除一个元素的dp</span></span><br><span class="line">        no_del_sum = remove_one_sum + w;</span><br><span class="line">        del_sum = max_sum;</span><br><span class="line"></span><br><span class="line">        remove_one_sum = <span class="built_in">max</span>(del_sum, no_del_sum);</span><br><span class="line">        max_remove_one = <span class="built_in">max</span>(remove_one_sum, max_remove_one);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 不删除元素的dp</span></span><br><span class="line">        max_sum = <span class="built_in">max</span>(max_sum + w, w);</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span>(max_data&lt;<span class="number">0</span>)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%lld"</span>, max_data);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%lld"</span>, <span class="built_in">max</span>(max_remove_one, max_sum));</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><h2 id="Greedy"><a href="#Greedy" class="headerlink" title="Greedy"></a>Greedy</h2><hr><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/0BeIYo.png" alt="0BeIYo"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cmath&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">200001</span>;</span><br><span class="line"><span class="keyword">char</span> s[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// ac</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%s"</span>, s);</span><br><span class="line">    <span class="keyword">int</span> cnta = <span class="number">0</span>, cntb = <span class="number">0</span>,i=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (s[i] != <span class="string">'\0'</span>)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (s[i]==<span class="string">'A'</span>)</span><br><span class="line">        {</span><br><span class="line">            cnta++;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(cnta&gt;<span class="number">0</span>)</span><br><span class="line">                cnta--;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                cntb++;</span><br><span class="line">        }</span><br><span class="line">        i++;</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d"</span>, cnta+cntb%<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/i6lDBv.png" alt="i6lDBv"></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> N = <span class="number">100000</span>;</span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">long</span> <span class="keyword">long</span> ll;</span><br><span class="line"></span><br><span class="line">ll s[N];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="built_in">scanf</span>(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="built_in">scanf</span>(<span class="string">"%lld"</span>, &amp;s[i]);</span><br><span class="line">    }</span><br><span class="line">    sort(s, s + n);</span><br><span class="line">    <span class="keyword">int</span> flag = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; n; i++)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span> (s[i] &lt; s[i - <span class="number">1</span>] + s[i - <span class="number">2</span>])</span><br><span class="line">        {</span><br><span class="line">            flag = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">    <span class="keyword">if</span> (flag)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"YES"</span>);</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"NO"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;p&gt;直接导出pdf太不方便看代码，直接放这里看了。&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="Algorithm" scheme="https://hanielxx.com/tags/Algorithm/"/>
    
    <category term="Math" scheme="https://hanielxx.com/tags/Math/"/>
    
    <category term="UCAS" scheme="https://hanielxx.com/tags/UCAS/"/>
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
  </entry>
  
  <entry>
    <title>TSP问题整数线性规划的几种解法</title>
    <link href="https://hanielxx.com/Notes/2020-12-17-tsp-ilp"/>
    <id>https://hanielxx.com/Notes/2020-12-17-tsp-ilp</id>
    <published>2020-12-17T06:38:35.000Z</published>
    <updated>2021-05-17T15:34:23.079Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer"><div class="note info"><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本来是作业里的一道题，但是有好几种解法，mark一下。</p><p>不得不说，线性规划建模是真的让人头大，一个LP作业搞了一下午加一晚上…</p></div><a id="more"></a><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>TSP问题，就是旅行商问题，太经典了，就不多说了。简单概括就是N个城市，找到访问每一座城市仅一次并回到起始城市的最短回路。</p><p>这里只说线性规划解法。</p><h2 id="Dantzig-Fulkerson-Johnson-formulation（DFJ）"><a href="#Dantzig-Fulkerson-Johnson-formulation（DFJ）" class="headerlink" title="Dantzig-Fulkerson-Johnson formulation（DFJ）"></a>Dantzig-Fulkerson-Johnson formulation（DFJ）</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/p7n8tX.png" alt="p7n8tX"></p><ul><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.304ex" height="1.666ex" role="img" focusable="false" viewBox="0 -442 1018.3 736.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="TeXAtom" transform="translate(433, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container>表示每个路径的距离</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="2.618ex" height="1.666ex" role="img" focusable="false" viewBox="0 -442 1157.3 736.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></g></g></svg></mjx-container>表示是否访问这个路径</li><li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex" xmlns="http://www.w3.org/2000/svg" width="1.459ex" height="1.645ex" role="img" focusable="false" viewBox="0 -705 645 727"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D446" d="M308 24Q367 24 416 76T466 197Q466 260 414 284Q308 311 278 321T236 341Q176 383 176 462Q176 523 208 573T273 648Q302 673 343 688T407 704H418H425Q521 704 564 640Q565 640 577 653T603 682T623 704Q624 704 627 704T632 705Q645 705 645 698T617 577T585 459T569 456Q549 456 549 465Q549 471 550 475Q550 478 551 494T553 520Q553 554 544 579T526 616T501 641Q465 662 419 662Q362 662 313 616T263 510Q263 480 278 458T319 427Q323 425 389 408T456 390Q490 379 522 342T554 242Q554 216 546 186Q541 164 528 137T492 78T426 18T332 -20Q320 -22 298 -22Q199 -22 144 33L134 44L106 13Q83 -14 78 -18T65 -22Q52 -22 52 -14Q52 -11 110 221Q112 227 130 227H143Q149 221 149 216Q149 214 148 207T144 186T142 153Q144 114 160 87T203 47T255 29T308 24Z"></path></g></g></g></svg></mjx-container>是路径数量</li></ul><p>缺点：约束规模过大，无法求解大规模算例</p><h2 id="Miller-Tucker-Zemlin-formulation（MTZ）"><a href="#Miller-Tucker-Zemlin-formulation（MTZ）" class="headerlink" title="Miller-Tucker-Zemlin formulation（MTZ）"></a>Miller-Tucker-Zemlin formulation（MTZ）</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/LWTjcQ.png" alt="LWTjcQ"></p><ul><li>前两个约束都是为了保证每城市只访问一次，每行每列的标志和都为1。</li><li>第三个约束条件 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="47.118ex" height="2.363ex" role="img" focusable="false" viewBox="0 -750 20826.1 1044.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1088.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2088.4, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(3279.5, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(4335.3, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(4613.3, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(5382.3, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mo" transform="translate(5660.3, 0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mn" transform="translate(6049.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(6771.5, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(7771.7, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(8929, 0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(9540.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(10540.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(11040.4, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(11485.1, 0)"><path data-c="2200" d="M0 673Q0 684 7 689T20 694Q32 694 38 680T82 567L126 451H430L473 566Q483 593 494 622T512 668T519 685Q524 694 538 694Q556 692 556 674Q556 670 426 329T293 -15Q288 -22 278 -22T263 -15Q260 -11 131 328T0 673ZM414 410Q414 411 278 411T142 410L278 55L414 410Z"></path></g><g data-mml-node="mi" transform="translate(12041.1, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(12386.1, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(12830.8, 0)"><path data-c="2200" d="M0 673Q0 684 7 689T20 694Q32 694 38 680T82 567L126 451H430L473 566Q483 593 494 622T512 668T519 685Q524 694 538 694Q556 692 556 674Q556 670 426 329T293 -15Q288 -22 278 -22T263 -15Q260 -11 131 328T0 673ZM414 410Q414 411 278 411T142 410L278 55L414 410Z"></path></g><g data-mml-node="mi" transform="translate(13386.8, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(13798.8, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mn" transform="translate(14243.4, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g><g data-mml-node="mo" transform="translate(15021.2, 0)"><path data-c="3C" d="M694 -11T694 -19T688 -33T678 -40Q671 -40 524 29T234 166L90 235Q83 240 83 250Q83 261 91 266Q664 540 678 540Q681 540 687 534T694 519T687 505Q686 504 417 376L151 250L417 124Q686 -4 687 -5Q694 -11 694 -19Z"></path></g><g data-mml-node="mi" transform="translate(16077, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(16699.8, 0)"><path data-c="2260" d="M166 -215T159 -215T147 -212T141 -204T139 -197Q139 -190 144 -183L306 133H70Q56 140 56 153Q56 168 72 173H327L406 327H72Q56 332 56 347Q56 360 70 367H426Q597 702 602 707Q605 716 618 716Q625 716 630 712T636 703T638 696Q638 692 471 367H707Q722 359 722 347Q722 336 708 328L451 327L371 173H708Q722 163 722 153Q722 140 707 133H351Q175 -210 170 -212Q166 -215 159 -215Z"></path></g><g data-mml-node="mi" transform="translate(17755.6, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mo" transform="translate(18445.3, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(19501.1, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g><g data-mml-node="mi" transform="translate(19779.1, 0)"><path data-c="1D449" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></g><g data-mml-node="mo" transform="translate(20548.1, 0)"><path data-c="7C" d="M139 -249H137Q125 -249 119 -235V251L120 737Q130 750 139 750Q152 750 159 735V-235Q151 -249 141 -249H139Z"></path></g></g></g></svg></mjx-container> 是为了保证图里面没有孤立的子圈。假设存在子圈，那么<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="12.402ex" height="2.173ex" role="img" focusable="false" viewBox="0 -666 5481.7 960.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(345, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g><g data-mml-node="mo" transform="translate(1435.1, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(2490.8, 0)"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="TeXAtom" transform="translate(572, -150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g><g data-mml-node="mi" transform="translate(412, 0)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g></g><g data-mml-node="mo" transform="translate(3925.9, 0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(4981.7, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>，则有<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="12.7ex" height="2.173ex" role="img" focusable="false" viewBox="0 -666 5613.3 960.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1088.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2088.4, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(3279.5, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(4335.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(5113.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>且<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="12.7ex" height="2.173ex" role="img" focusable="false" viewBox="0 -666 5613.3 960.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(1135.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2135.8, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(3279.5, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(4335.3, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(5113.3, 0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>，则<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.666ex" xmlns="http://www.w3.org/2000/svg" width="22.257ex" height="2.173ex" role="img" focusable="false" viewBox="0 -666 9837.4 960.2"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1088.2, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(2088.4, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(3223.9, 0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(4224.2, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g><g data-mml-node="mo" transform="translate(5359.7, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(6359.9, 0)"><g data-mml-node="mi"><path data-c="1D462" d="M21 287Q21 295 30 318T55 370T99 420T158 442Q204 442 227 417T250 358Q250 340 216 246T182 105Q182 62 196 45T238 27T291 44T328 78L339 95Q341 99 377 247Q407 367 413 387T427 416Q444 431 463 431Q480 431 488 421T496 402L420 84Q419 79 419 68Q419 43 426 35T447 26Q469 29 482 57T512 145Q514 153 532 153Q551 153 551 144Q550 139 549 130T540 98T523 55T498 17T462 -8Q454 -10 438 -10Q372 -10 347 46Q345 45 336 36T318 21T296 6T267 -6T233 -11Q189 -11 155 7Q103 38 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(572, -150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(7503.7, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(8559.4, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(9337.4, 0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></svg></mjx-container>，即<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.312ex" xmlns="http://www.w3.org/2000/svg" width="7.04ex" height="1.819ex" role="img" focusable="false" viewBox="0 -666 3111.6 804"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g><g data-mml-node="mo" transform="translate(777.8, 0)"><path data-c="2264" d="M674 636Q682 636 688 630T694 615T687 601Q686 600 417 472L151 346L399 228Q687 92 691 87Q694 81 694 76Q694 58 676 56H670L382 192Q92 329 90 331Q83 336 83 348Q84 359 96 365Q104 369 382 500T665 634Q669 636 674 636ZM84 -118Q84 -108 99 -98H678Q694 -104 694 -118Q694 -130 679 -138H98Q84 -131 84 -118Z"></path></g><g data-mml-node="mo" transform="translate(1833.6, 0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(2611.6, 0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g></g></svg></mjx-container>，很显然矛盾。所以这样能够确保不存在互相孤立的子圈。同时。<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.462ex" xmlns="http://www.w3.org/2000/svg" width="3.284ex" height="1.957ex" role="img" focusable="false" viewBox="0 -661 1451.7 865"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(345, 0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mtext" transform="translate(789.7, 0)"><path data-c="A0" d=""></path></g><g data-mml-node="mi" transform="translate(1039.7, 0)"><path data-c="1D457" d="M297 596Q297 627 318 644T361 661Q378 661 389 651T403 623Q403 595 384 576T340 557Q322 557 310 567T297 596ZM288 376Q288 405 262 405Q240 405 220 393T185 362T161 325T144 293L137 279Q135 278 121 278H107Q101 284 101 286T105 299Q126 348 164 391T252 441Q253 441 260 441T272 442Q296 441 316 432Q341 418 354 401T367 348V332L318 133Q267 -67 264 -75Q246 -125 194 -164T75 -204Q25 -204 7 -183T-12 -137Q-12 -110 7 -91T53 -71Q70 -71 82 -81T95 -112Q95 -148 63 -167Q69 -168 77 -168Q111 -168 139 -140T182 -74L193 -32Q204 11 219 72T251 197T278 308T289 365Q289 372 288 376Z"></path></g></g></g></svg></mjx-container>不取1，是因为经过所有点的“子圈”即是我们要求解的较优圈，应剔除出限制条件。</li><li>第四个和第五个约束条件就都是对变量范围进行约束了</li></ul><h2 id="Gavish-Graves-formulation（GG）"><a href="#Gavish-Graves-formulation（GG）" class="headerlink" title="Gavish-Graves formulation（GG）"></a>Gavish-Graves formulation（GG）</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/7DT4EU.png" alt="7DT4EU"></p><ul><li>分析：通过增加一组变量，确定每两点间的弧的前序弧数，来消除子回路</li></ul><h2 id="Gouveia-Pires-L3RMTZ-formulation（GP）"><a href="#Gouveia-Pires-L3RMTZ-formulation（GP）" class="headerlink" title="Gouveia-Pires L3RMTZ formulation（GP）"></a>Gouveia-Pires L3RMTZ formulation（GP）</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/efOsq6.png" alt="efOsq6"></p><ol><li>L3RMTZ 模型要强于 Gouveia 和 Pires 提出的另外几种模型，但该模型推导的原理较为复杂</li><li>该模型理论上更好，但实际求解时间更长。</li></ol><h2 id="效率对比"><a href="#效率对比" class="headerlink" title="效率对比"></a>效率对比</h2><p><img src="https://cdn.jsdelivr.net/gh/HanielF/ImageRepo@main/blog/xpLK48.png" alt="xpLK48"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li>Dantzig, G., Fulkerson, R., &amp; Johnson, S. (1954). Solution of a large-scale traveling-salesman problem. Journal of the operations research society of America, 2(4), 393-410.</li><li>Miller, C. E., Tucker, A. W., &amp; Zemlin, R. A. (1960). Integer programming formulation of traveling salesman problems. Journal of the ACM (JACM), 7(4), 326-329.</li><li>Gavish, B., &amp; Graves, S. C. (1978). The travelling salesman problem and related problems.</li><li>Gouveia L, Pires J (1999) The asymmetric travelling salesman problem and a reformulation of the Miller–Tucker–Zemlin constraints. Eur J Oper Res 112:134–146.</li><li>Roberti, R., &amp; Toth, P. (2012). Models and algorithms for the asymmetric traveling salesman problem: An experimental comparison. EURO Journal on Transportation and Logistics, 1(1-2), 113-133.</li></ul>]]></content>
    
    
    <summary type="html">&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;&gt;&lt;div class=&quot;note info&quot;&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;本来是作业里的一道题，但是有好几种解法，mark一下。&lt;/p&gt;&lt;p&gt;不得不说，线性规划建模是真的让人头大，一个LP作业搞了一下午加一晚上…&lt;/p&gt;&lt;/div&gt;</summary>
    
    
    
    <category term="Notes" scheme="https://hanielxx.com/categories/Notes/"/>
    
    
    <category term="Notes" scheme="https://hanielxx.com/tags/Notes/"/>
    
    <category term="TSP" scheme="https://hanielxx.com/tags/TSP/"/>
    
    <category term="LP" scheme="https://hanielxx.com/tags/LP/"/>
    
    <category term="ILP" scheme="https://hanielxx.com/tags/ILP/"/>
    
  </entry>
  
</feed>
